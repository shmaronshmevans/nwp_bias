{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import statistics\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import xarray as xr\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    \"\"\"Format a DataFrame with counts of values into a DataFrame with individual values\n",
    "\n",
    "    Args:\n",
    "    - df: A pandas DataFrame with columns \"COUNT\" and \"VALUE\"\n",
    "\n",
    "    Returns:\n",
    "    - A new pandas DataFrame with a single column \"VALUE\" containing individual values\n",
    "\n",
    "    Example:\n",
    "    input DataFrame:\n",
    "\n",
    "    | COUNT | VALUE |\n",
    "    |-------|-------|\n",
    "    |   2   |  10   |\n",
    "    |   3   |  20   |\n",
    "    |   1   |  30   |\n",
    "\n",
    "    output DataFrame:\n",
    "\n",
    "    | VALUE |\n",
    "    |-------|\n",
    "    |  10   |\n",
    "    |  10   |\n",
    "    |  20   |\n",
    "    |  20   |\n",
    "    |  20   |\n",
    "    |  30   |\n",
    "    \"\"\"\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    value_list = []\n",
    "\n",
    "    # iterate over the rows of the input DataFrame\n",
    "    for x, _ in df.iterrows():\n",
    "        # extract the count and value from each row\n",
    "        count = int(df.iloc[x][\"COUNT\"])\n",
    "        value = df.iloc[x][\"VALUE\"]\n",
    "\n",
    "        # repeat the value the specified number of times\n",
    "        for n in np.arange(count):\n",
    "            val = value\n",
    "            value_list.append(val)\n",
    "\n",
    "    # create a new DataFrame with the individual values\n",
    "    new_df[\"VALUE\"] = value_list\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def stat_anal(directory, state_df, station_list, lonlist, latlist):\n",
    "    \"\"\"\n",
    "    Performs statistical analysis on a set of elevation data for a given state.\n",
    "\n",
    "    Args:\n",
    "    directory (str): File directory where the elevation data is stored.\n",
    "    state_df (pandas DataFrame): DataFrame containing the elevation data for each station in the state.\n",
    "    station_list (list): List of station IDs.\n",
    "    lonlist (list): List of longitude coordinates for each station.\n",
    "    latlist (list): List of latitude coordinates for each station.\n",
    "\n",
    "    Returns:\n",
    "    final_df (pandas DataFrame): DataFrame containing the results of the statistical analysis.\n",
    "    Columns include: \"station\", \"elev\", \"std\", \"variance\", \"skew\", \"med_dist\", \"lon\", and \"lat\".\n",
    "    \"\"\"\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    std_list = []\n",
    "    variance_list = []\n",
    "    skew_list = []\n",
    "    distance_list = []\n",
    "    stations = []\n",
    "    elevs = []\n",
    "    x = 0\n",
    "    for i in np.arange(1, 127):\n",
    "        # read in csv\n",
    "        df2 = pd.DataFrame()\n",
    "        elev_df = pd.read_csv(f\"{directory}/gfs/{i}_csv.csv\")\n",
    "        dfv1 = format_df(elev_df)  # apply format_df to the elevation data\n",
    "        std = statistics.stdev(dfv1[\"VALUE\"])  # calculate the standard deviation\n",
    "        variance = statistics.pvariance(dfv1[\"VALUE\"])  # calculate the variance\n",
    "        my_skew = skew(dfv1[\"VALUE\"])  # calculate the skewness\n",
    "        elevation = state_df[\"elev\"].iloc[\n",
    "            x\n",
    "        ]  # get the elevation for the current station\n",
    "        station = station_list[x]  # get the station ID for the current station\n",
    "        split_diff = (\n",
    "            dfv1[\"VALUE\"] - state_df[\"elev\"].iloc[x]\n",
    "        )  # calculate the difference between elevation and state_df\n",
    "        diff_list = split_diff.to_list()  # convert the difference to a list\n",
    "        df2[\"diff_elev\"] = diff_list  # add the difference to the DataFrame\n",
    "        describe = df2[\n",
    "            \"diff_elev\"\n",
    "        ].describe()  # calculate the descriptive statistics for the difference\n",
    "        fifty = describe[5]  # get the median of the difference\n",
    "        distance = state_df[\"elev\"].iloc[x] - fifty  # calculate the median distance\n",
    "        # add data to lists\n",
    "        stations.append(station)\n",
    "        elevs.append(elevation)\n",
    "        distance_list.append(distance)\n",
    "        skew_list.append(my_skew)\n",
    "        variance_list.append(variance)\n",
    "        std_list.append(std)\n",
    "        x += 1\n",
    "\n",
    "    final_df[\"station\"] = stations\n",
    "    final_df[\"elev\"] = elevs\n",
    "    final_df[\"std\"] = std_list\n",
    "    final_df[\"variance\"] = variance_list\n",
    "    final_df[\"skew\"] = skew_list\n",
    "    final_df[\"med_dist\"] = distance_list\n",
    "    final_df[\"lon\"] = lonlist\n",
    "    final_df[\"lat\"] = latlist\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "station_list_ny = ny_df[\"stid\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0]\n",
      "[400.0, 440.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0, 780.0, 800.0]\n",
      "[200.0, 220.0, 240.0, 260.0, 280.0, 300.0]\n",
      "[0.0, 20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 240.0, 280.0, 300.0, 340.0, 360.0, 400.0, 440.0]\n",
      "[300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0]\n",
      "[100.0, 120.0, 140.0, 160.0, 180.0]\n",
      "[400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0]\n",
      "[340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 520.0]\n",
      "[280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0]\n",
      "[0.0, 20.0, 40.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0, 280.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0]\n",
      "[0.0, 20.0, 40.0, 60.0, 80.0, 100.0, 120.0]\n",
      "[360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0]\n",
      "[80.0, 100.0, 120.0, 140.0, 160.0]\n",
      "[160.0, 180.0, 200.0, 220.0]\n",
      "[320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0]\n",
      "[80.0, 100.0]\n",
      "[140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0]\n",
      "[80.0, 100.0]\n",
      "[40.0, 60.0, 80.0, 100.0, 120.0]\n",
      "[240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0]\n",
      "[340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0]\n",
      "[580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0, 780.0, 800.0, 820.0, 840.0, 860.0, 880.0, 900.0, 920.0, 940.0, 960.0, 1000.0, 1080.0]\n",
      "[160.0, 180.0]\n",
      "[440.0, 460.0, 480.0, 500.0, 520.0, 540.0]\n",
      "[280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0]\n",
      "[400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0]\n",
      "[300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0]\n",
      "[220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0]\n",
      "[400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0]\n",
      "[120.0, 140.0, 160.0, 180.0]\n",
      "[480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0]\n",
      "[300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0]\n",
      "[220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0]\n",
      "[260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]\n",
      "[240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 500.0, 540.0, 560.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0]\n",
      "[260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0]\n",
      "[240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0]\n",
      "[280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0]\n",
      "[20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 240.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0]\n",
      "[480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 620.0]\n",
      "[60.0, 80.0, 100.0, 120.0, 140.0]\n",
      "[240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 520.0, 540.0]\n",
      "[300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0]\n",
      "[420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0]\n",
      "[80.0, 100.0, 120.0]\n",
      "[440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0]\n",
      "[400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0]\n",
      "[440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0]\n",
      "[100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0]\n",
      "[40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 280.0, 300.0, 320.0, 360.0]\n",
      "[460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0]\n",
      "[100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0, 220.0]\n",
      "[0.0, 20.0, 40.0, 60.0, 80.0, 100.0, 120.0]\n",
      "[340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0]\n",
      "[60.0, 80.0, 100.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0]\n",
      "[0.0, 20.0, 40.0, 60.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0]\n",
      "[240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0]\n",
      "[320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0]\n",
      "[300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0]\n",
      "[480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0]\n",
      "[280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 640.0, 660.0, 680.0, 700.0, 720.0, 760.0, 860.0, 920.0]\n",
      "[520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0]\n",
      "[440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0]\n",
      "[80.0, 100.0, 120.0, 140.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0]\n",
      "[300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]\n",
      "[80.0, 100.0, 120.0]\n",
      "[160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0]\n",
      "[280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]\n",
      "[140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0]\n",
      "[120.0, 140.0, 160.0]\n",
      "[500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0, 780.0, 800.0, 820.0, 860.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0]\n",
      "[0.0, 20.0, 40.0]\n",
      "[400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0]\n",
      "[540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0]\n",
      "[320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0]\n",
      "[0.0, 20.0, 40.0, 60.0, 80.0, 100.0, 120.0]\n",
      "[360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0, 780.0, 800.0, 820.0, 840.0, 860.0, 880.0, 960.0]\n",
      "[180.0, 200.0, 220.0]\n",
      "[160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 560.0, 620.0, 640.0]\n",
      "[200.0, 220.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 640.0]\n",
      "[20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0]\n",
      "[20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0]\n",
      "[40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0]\n",
      "[220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0]\n",
      "[340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0]\n",
      "[80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0]\n",
      "[0.0, 20.0]\n",
      "[100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0]\n",
      "[360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0]\n",
      "[0.0, 20.0, 40.0, 60.0, 80.0, 100.0]\n",
      "[260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0]\n",
      "[0.0, 20.0, 40.0, 60.0]\n",
      "[100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 260.0, 280.0, 300.0, 320.0, 340.0]\n",
      "[380.0, 420.0, 440.0, 480.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0, 780.0, 800.0, 820.0, 840.0, 860.0, 880.0, 900.0, 940.0, 1000.0, 1060.0]\n",
      "[20.0, 40.0, 60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 300.0, 320.0, 340.0, 360.0, 400.0, 420.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0]\n",
      "[460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 600.0, 620.0, 680.0, 700.0]\n",
      "[340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0]\n",
      "[80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 300.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0]\n",
      "[60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0]\n",
      "[420.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 620.0, 640.0]\n",
      "[0.0, 20.0]\n",
      "[380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0]\n",
      "[140.0, 160.0, 180.0, 200.0, 220.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0]\n",
      "[120.0, 140.0, 160.0, 180.0, 200.0]\n",
      "[320.0, 340.0, 360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0]\n",
      "[80.0, 100.0]\n",
      "[160.0, 180.0, 200.0, 220.0, 240.0, 280.0, 320.0]\n",
      "[280.0, 300.0, 320.0, 340.0, 360.0, 400.0, 420.0, 440.0, 480.0, 500.0, 520.0, 540.0, 560.0, 580.0, 600.0, 640.0, 660.0, 680.0, 700.0, 720.0, 740.0, 760.0, 780.0, 800.0, 820.0, 840.0, 860.0, 900.0, 920.0, 940.0, 980.0, 1000.0, 1100.0, 1120.0, 1160.0, 1220.0]\n",
      "[360.0, 380.0, 400.0, 420.0, 440.0, 460.0, 480.0, 500.0, 520.0]\n",
      "[60.0, 80.0, 100.0, 120.0, 140.0, 160.0, 180.0, 200.0, 240.0, 260.0, 280.0, 300.0, 320.0, 340.0, 360.0, 380.0, 400.0, 480.0]\n",
      "[80.0, 100.0, 120.0, 140.0]\n",
      "[180.0, 200.0, 220.0, 240.0, 260.0, 280.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>VALUE</th>\n",
       "      <th>340.0</th>\n",
       "      <th>360.0</th>\n",
       "      <th>380.0</th>\n",
       "      <th>400.0</th>\n",
       "      <th>420.0</th>\n",
       "      <th>440.0</th>\n",
       "      <th>460.0</th>\n",
       "      <th>480.0</th>\n",
       "      <th>500.0</th>\n",
       "      <th>520.0</th>\n",
       "      <th>...</th>\n",
       "      <th>960.0</th>\n",
       "      <th>1000.0</th>\n",
       "      <th>1080.0</th>\n",
       "      <th>1060.0</th>\n",
       "      <th>980.0</th>\n",
       "      <th>1100.0</th>\n",
       "      <th>1120.0</th>\n",
       "      <th>1160.0</th>\n",
       "      <th>1220.0</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.539823</td>\n",
       "      <td>1.769912</td>\n",
       "      <td>1.769912</td>\n",
       "      <td>5.309735</td>\n",
       "      <td>4.424779</td>\n",
       "      <td>15.929204</td>\n",
       "      <td>12.389381</td>\n",
       "      <td>21.238938</td>\n",
       "      <td>22.123894</td>\n",
       "      <td>9.734513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ADDI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.678571</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>5.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ANDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>BATA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>BEAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.769912</td>\n",
       "      <td>5.309735</td>\n",
       "      <td>7.079646</td>\n",
       "      <td>8.849558</td>\n",
       "      <td>8.849558</td>\n",
       "      <td>24.778761</td>\n",
       "      <td>15.044248</td>\n",
       "      <td>15.929204</td>\n",
       "      <td>7.079646</td>\n",
       "      <td>3.539823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>BELD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2.678571</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.928571</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>2.678571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.678571</td>\n",
       "      <td>3.571429</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.678571</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>2.678571</td>\n",
       "      <td>1.785714</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>WFMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>3.478261</td>\n",
       "      <td>15.652174</td>\n",
       "      <td>27.826087</td>\n",
       "      <td>17.391304</td>\n",
       "      <td>15.652174</td>\n",
       "      <td>9.565217</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>1.739130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>WGAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.900901</td>\n",
       "      <td>4.504505</td>\n",
       "      <td>1.801802</td>\n",
       "      <td>2.702703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>WHIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>WOLC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>YORK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "VALUE     340.0     360.0     380.0      400.0      420.0      440.0  \\\n",
       "site                                                                   \n",
       "1      3.539823  1.769912  1.769912   5.309735   4.424779  15.929204   \n",
       "2      0.000000  0.000000  0.000000   0.892857   0.000000   1.785714   \n",
       "3      0.000000  0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "4      0.892857  0.892857  0.000000   0.892857   0.000000   0.892857   \n",
       "5      1.769912  5.309735  7.079646   8.849558   8.849558  24.778761   \n",
       "...         ...       ...       ...        ...        ...        ...   \n",
       "122    2.678571  3.571429  0.000000   8.928571   0.892857   2.678571   \n",
       "123    0.000000  4.347826  3.478261  15.652174  27.826087  17.391304   \n",
       "124    0.900901  4.504505  1.801802   2.702703   0.000000   0.000000   \n",
       "125    0.000000  0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "126    0.000000  0.000000  0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "VALUE      460.0      480.0      500.0     520.0  ...  960.0    1000.0  \\\n",
       "site                                              ...                    \n",
       "1      12.389381  21.238938  22.123894  9.734513  ...    0.0  0.000000   \n",
       "2       0.000000   2.678571   1.785714  5.357143  ...    0.0  0.000000   \n",
       "3       0.000000   0.000000   0.000000  0.000000  ...    0.0  0.000000   \n",
       "4       0.000000   0.000000   0.000000  0.000000  ...    0.0  0.000000   \n",
       "5      15.044248  15.929204   7.079646  3.539823  ...    0.0  0.000000   \n",
       "...          ...        ...        ...       ...  ...    ...       ...   \n",
       "122     0.000000   2.678571   3.571429  0.892857  ...    0.0  1.785714   \n",
       "123    15.652174   9.565217   4.347826  1.739130  ...    0.0  0.000000   \n",
       "124     0.000000   0.900901   0.000000  0.000000  ...    0.0  0.000000   \n",
       "125     0.000000   0.000000   0.000000  0.000000  ...    0.0  0.000000   \n",
       "126     0.000000   0.000000   0.000000  0.000000  ...    0.0  0.000000   \n",
       "\n",
       "VALUE  1080.0  1060.0     980.0    1100.0    1120.0    1160.0    1220.0  \\\n",
       "site                                                                      \n",
       "1         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5         0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...       ...     ...       ...       ...       ...       ...       ...   \n",
       "122       0.0     0.0  2.678571  0.892857  2.678571  1.785714  0.892857   \n",
       "123       0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "124       0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "125       0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "126       0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "VALUE  station  \n",
       "site            \n",
       "1         ADDI  \n",
       "2         ANDE  \n",
       "3         BATA  \n",
       "4         BEAC  \n",
       "5         BELD  \n",
       "...        ...  \n",
       "122       WFMB  \n",
       "123       WGAT  \n",
       "124       WHIT  \n",
       "125       WOLC  \n",
       "126       YORK  \n",
       "\n",
       "[126 rows x 58 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store the combined data.\n",
    "df_complete = pd.DataFrame()\n",
    "\n",
    "# Loop over the CSV files and combine them into a single DataFrame.\n",
    "for i in range(1, len(station_list_ny) + 1):\n",
    "    # Read the CSV file and calculate the percentage of each aspect.\n",
    "    elev_df = pd.read_csv(\n",
    "        f\"/home/aevans/nwp_bias/src/landtype/elevation/data/CSVs_elevation_ny_nam/aspect_csv_{i}.csv\"\n",
    "    )\n",
    "    # format raw elev df\n",
    "    elev_df = format_df(elev_df)\n",
    "\n",
    "    # bucket elev df\n",
    "    elev_df[\"rounded\"] = round(elev_df[\"VALUE\"] / 20) * 20\n",
    "\n",
    "    the_df = pd.DataFrame()\n",
    "    buckets = elev_df[\"rounded\"].unique().tolist()\n",
    "    print(buckets)\n",
    "    the_df[\"VALUE\"] = buckets\n",
    "\n",
    "    counts_ls = []\n",
    "    for value in buckets:\n",
    "        counts = elev_df[\"rounded\"].value_counts()[value]\n",
    "        counts_ls.append(counts)\n",
    "\n",
    "    the_df[\"COUNT\"] = counts_ls\n",
    "\n",
    "    # get percentage of bucket\n",
    "    the_df = the_df.assign(\n",
    "        Percentage=lambda x: (x[\"COUNT\"] / sum(the_df[\"COUNT\"]) * 100)\n",
    "    )\n",
    "\n",
    "    # Add the site number and pivot the DataFrame to have aspects as columns.\n",
    "    the_df[\"site\"] = i\n",
    "    the_df = the_df.pivot(index=\"site\", columns=\"VALUE\", values=\"Percentage\")\n",
    "\n",
    "    # Concatenate the current DataFrame with the combined DataFrame.\n",
    "    df_complete = pd.concat([df_complete, the_df])\n",
    "\n",
    "# Add the station names as a column in the DataFrame and fill missing values with 0.\n",
    "df_complete[\"station\"] = station_list_ny\n",
    "df_complete = df_complete.fillna(0)\n",
    "df_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.to_csv(\n",
    "    \"/home/aevans/nwp_bias/src/landtype/data/nysm_representativeness_analysis/clean/all_sites_csv.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.listdir(\n",
    "    \"/home/aevans/nwp_bias/src/landtype/data/nysm_representativeness_analysis/elev\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in directory:\n",
    "    # import raw elevation dataframe\n",
    "    elev_df = pd.read_csv(\n",
    "        f\"/home/aevans/nwp_bias/src/landtype/data/nysm_representativeness_analysis/elev/{file}\"\n",
    "    )\n",
    "\n",
    "    # format raw elev df\n",
    "    elev_df = format_df(elev_df)\n",
    "\n",
    "    # bucket elev df\n",
    "    elev_df[\"rounded\"] = round(elev_df[\"Value\"] / 20) * 20\n",
    "\n",
    "    the_df = pd.DataFrame()\n",
    "    buckets = elev_df[\"rounded\"].unique().tolist()\n",
    "    the_df[\"Value\"] = buckets\n",
    "\n",
    "    counts_ls = []\n",
    "    for value in buckets:\n",
    "        counts = elev_df[\"rounded\"].value_counts()[value]\n",
    "        counts_ls.append(counts)\n",
    "\n",
    "    the_df[\"Count\"] = counts_ls\n",
    "    the_df\n",
    "\n",
    "    # get percentage of bucket\n",
    "    the_df = the_df.assign(\n",
    "        Percentage=lambda x: (x[\"Count\"] / sum(the_df[\"Count\"]) * 100)\n",
    "    )\n",
    "    the_df\n",
    "    # format and save for climdiv\n",
    "\n",
    "    the_df.to_csv(\n",
    "        f\"/home/aevans/nwp_bias/src/landtype/data/nysm_representativeness_analysis/clean/elev/{file}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_time_mesonet_df(mesonet_data_path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This will return a dataframe that contains data from the mesonet sites\n",
    "\n",
    "    Args:\n",
    "        Mesonet Data Path (f string)\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Mesonet Data Frame\n",
    "    \"\"\"\n",
    "\n",
    "    # most recent year\n",
    "    dir_Year = os.listdir(f\"{mesonet_data_path}\")\n",
    "    sort_dir_Year = sorted(dir_Year)\n",
    "    data_point_Year = sort_dir_Year[-1]\n",
    "\n",
    "    # find most recent month\n",
    "    dir_Month = os.listdir(f\"{mesonet_data_path}/{data_point_Year}\")\n",
    "    sort_dir_Month = sorted(dir_Month)\n",
    "    data_point_Month = sort_dir_Month[-1]\n",
    "\n",
    "    # this is your directory for most recent year and month\n",
    "    most_recent = os.listdir(\n",
    "        f\"{mesonet_data_path}/{data_point_Year}/{data_point_Month}\"\n",
    "    )\n",
    "\n",
    "    # most recent datapoint\n",
    "    sort_most_recent = sorted(most_recent)\n",
    "    data_point = sort_most_recent[-1]\n",
    "\n",
    "    # this will return the year of the most recent data point\n",
    "    new_year = data_point[0:4]\n",
    "\n",
    "    # this will return the month of the most recent datapoint\n",
    "    new_month = data_point[4:6]\n",
    "\n",
    "    # this will return the day of the most recent datapoint\n",
    "    new_day = data_point[6:8]\n",
    "\n",
    "    # create Mesonet DataFrame\n",
    "\n",
    "    # year\n",
    "    year = new_year\n",
    "\n",
    "    # month\n",
    "    month = new_month\n",
    "\n",
    "    # day\n",
    "    day = new_day\n",
    "\n",
    "    # file path\n",
    "    file = year + month + day + \".nc\"\n",
    "\n",
    "    mesonet_df = (\n",
    "        xr.open_dataset(f\"{mesonet_data_path}/{year}/{month}/{file}\")\n",
    "        .to_dataframe()\n",
    "        .reset_index()\n",
    "    )\n",
    "    return mesonet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_recent_time(df: pd.DataFrame, mesonet_data_path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This will return a dataframe that contains only the timestamps with filled data from the mesonet sites\n",
    "\n",
    "    Args:\n",
    "    Mesonet Data Path (f string)\n",
    "\n",
    "    Returns:\n",
    "    df (pd.DataFrame): Mesonet Data Frame\n",
    "    \"\"\"\n",
    "\n",
    "    # most recent year\n",
    "    dir_Year = os.listdir(f\"{mesonet_data_path}\")\n",
    "    sort_dir_Year = sorted(dir_Year)\n",
    "    data_point_Year = sort_dir_Year[-1]\n",
    "\n",
    "    # find most recent month\n",
    "    dir_Month = os.listdir(f\"{mesonet_data_path}/{data_point_Year}\")\n",
    "    sort_dir_Month = sorted(dir_Month)\n",
    "    data_point_Month = sort_dir_Month[-1]\n",
    "\n",
    "    # this is your directory for most recent year and month\n",
    "    most_recent = os.listdir(\n",
    "        f\"{mesonet_data_path}/{data_point_Year}/{data_point_Month}\"\n",
    "    )\n",
    "\n",
    "    # most recent datapoint\n",
    "    sort_most_recent = sorted(most_recent)\n",
    "    data_point = sort_most_recent[-1]\n",
    "\n",
    "    # this will return the year of the most recent data point\n",
    "    new_year = data_point[0:4]\n",
    "\n",
    "    # this will return the month of the most recent datapoint\n",
    "    new_month = data_point[4:6]\n",
    "\n",
    "    # this will return the day of the most recent datapoint\n",
    "    new_day = data_point[6:8]\n",
    "\n",
    "    # create Mesonet DataFrame\n",
    "\n",
    "    # year\n",
    "    year = new_year\n",
    "\n",
    "    # month\n",
    "    month = new_month\n",
    "\n",
    "    # day\n",
    "    day = new_day\n",
    "\n",
    "    current_time_df = df.dropna(subset=[\"tair\"])\n",
    "\n",
    "    last_value = current_time_df[\"time_5M\"].iat[-1]\n",
    "    hour = last_value.hour\n",
    "    minute = last_value.minute\n",
    "    second = last_value.second\n",
    "\n",
    "    string_hour = str(hour)\n",
    "    string_minute = str(minute)\n",
    "    string_sec = str(second)\n",
    "\n",
    "    # time\n",
    "    time = string_hour + \":\" + string_minute + \":\" + string_sec\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # creating a new dataframe that is centered on the location in the dataframe\n",
    "    mesonet_single_datetime_df = df.loc[df[\"time_5M\"] == f\"{year}-{month}-{day} {time}\"]\n",
    "    return mesonet_single_datetime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the most recent data avail on mesonet\n",
    "# this is my file path\n",
    "ny_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/notebooks/nysm_coords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will return the most recent data avail on mesonet\n",
    "# this is my file path\n",
    "ny_mesonet_data_path = \"/home/aevans/nysm/archive/nysm/netcdf/proc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aevans/nysm/archive/nysm/netcdf/proc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nysm_df1 \u001b[38;5;241m=\u001b[39m current_time_mesonet_df(ny_mesonet_data_path)\n\u001b[1;32m      2\u001b[0m nysm_df \u001b[38;5;241m=\u001b[39m most_recent_time(nysm_df1, ny_mesonet_data_path)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mcurrent_time_mesonet_df\u001b[0;34m(mesonet_data_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis will return a dataframe that contains data from the mesonet sites\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    df (pd.DataFrame): Mesonet Data Frame\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# most recent year\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m dir_Year \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmesonet_data_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m sort_dir_Year \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(dir_Year)\n\u001b[1;32m     15\u001b[0m data_point_Year \u001b[38;5;241m=\u001b[39m sort_dir_Year[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aevans/nysm/archive/nysm/netcdf/proc'"
     ]
    }
   ],
   "source": [
    "nysm_df1 = current_time_mesonet_df(ny_mesonet_data_path)\n",
    "nysm_df = most_recent_time(nysm_df1, ny_mesonet_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df[\"elev\"] = nysm_df[\"elev\"].to_list()\n",
    "ny_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.listdir(f\"/home/aevans/nwp_bias/src/landtype/elevation/data/NY/elev/nam\")\n",
    "sorted_direct = sorted(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADDI_elev.csv',\n",
       " 'ANDE_elev.csv',\n",
       " 'BATA_elev.csv',\n",
       " 'BEAC_elev.csv',\n",
       " 'BELD_elev.csv',\n",
       " 'BELL_elev.csv',\n",
       " 'BELM_elev.csv',\n",
       " 'BERK_elev.csv',\n",
       " 'BING_elev.csv',\n",
       " 'BKLN_elev.csv',\n",
       " 'BRAN_elev.csv',\n",
       " 'BREW_elev.csv',\n",
       " 'BROC_elev.csv',\n",
       " 'BRON_elev.csv',\n",
       " 'BROO_elev.csv',\n",
       " 'BSPA_elev.csv',\n",
       " 'BUFF_elev.csv',\n",
       " 'BURD_elev.csv',\n",
       " 'BURT_elev.csv',\n",
       " 'CAMD_elev.csv',\n",
       " 'CAPE_elev.csv',\n",
       " 'CHAZ_elev.csv',\n",
       " 'CHES_elev.csv',\n",
       " 'CINC_elev.csv',\n",
       " 'CLAR_elev.csv',\n",
       " 'CLIF_elev.csv',\n",
       " 'CLYM_elev.csv',\n",
       " 'COBL_elev.csv',\n",
       " 'COHO_elev.csv',\n",
       " 'COLD_elev.csv',\n",
       " 'COPA_elev.csv',\n",
       " 'COPE_elev.csv',\n",
       " 'CROG_elev.csv',\n",
       " 'CSQR_elev.csv',\n",
       " 'DELE_elev.csv',\n",
       " 'DEPO_elev.csv',\n",
       " 'DOVE_elev.csv',\n",
       " 'DUAN_elev.csv',\n",
       " 'EAUR_elev.csv',\n",
       " 'EDIN_elev.csv',\n",
       " 'EDWA_elev.csv',\n",
       " 'ELDR_elev.csv',\n",
       " 'ELLE_elev.csv',\n",
       " 'ELMI_elev.csv',\n",
       " 'ESSX_elev.csv',\n",
       " 'FAYE_elev.csv',\n",
       " 'FRED_elev.csv',\n",
       " 'GABR_elev.csv',\n",
       " 'GFAL_elev.csv',\n",
       " 'GFLD_elev.csv',\n",
       " 'GROT_elev.csv',\n",
       " 'GROV_elev.csv',\n",
       " 'HAMM_elev.csv',\n",
       " 'HARP_elev.csv',\n",
       " 'HARR_elev.csv',\n",
       " 'HART_elev.csv',\n",
       " 'HERK_elev.csv',\n",
       " 'HFAL_elev.csv',\n",
       " 'ILAK_elev.csv',\n",
       " 'JOHN_elev.csv',\n",
       " 'JORD_elev.csv',\n",
       " 'KIND_elev.csv',\n",
       " 'LAUR_elev.csv',\n",
       " 'LOUI_elev.csv',\n",
       " 'MALO_elev.csv',\n",
       " 'MANH_elev.csv',\n",
       " 'MEDI_elev.csv',\n",
       " 'MEDU_elev.csv',\n",
       " 'MORR_elev.csv',\n",
       " 'NBRA_elev.csv',\n",
       " 'NEWC_elev.csv',\n",
       " 'NHUD_elev.csv',\n",
       " 'OLDF_elev.csv',\n",
       " 'OLEA_elev.csv',\n",
       " 'ONTA_elev.csv',\n",
       " 'OPPE_elev.csv',\n",
       " 'OSCE_elev.csv',\n",
       " 'OSWE_elev.csv',\n",
       " 'OTIS_elev.csv',\n",
       " 'OWEG_elev.csv',\n",
       " 'PENN_elev.csv',\n",
       " 'PHIL_elev.csv',\n",
       " 'PISE_elev.csv',\n",
       " 'POTS_elev.csv',\n",
       " 'QUEE_elev.csv',\n",
       " 'RAND_elev.csv',\n",
       " 'RAQU_elev.csv',\n",
       " 'REDF_elev.csv',\n",
       " 'REDH_elev.csv',\n",
       " 'ROXB_elev.csv',\n",
       " 'RUSH_elev.csv',\n",
       " 'SARA_elev.csv',\n",
       " 'SBRI_elev.csv',\n",
       " 'SCHA_elev.csv',\n",
       " 'SCHO_elev.csv',\n",
       " 'SCHU_elev.csv',\n",
       " 'SCIP_elev.csv',\n",
       " 'SHER_elev.csv',\n",
       " 'SOME_elev.csv',\n",
       " 'SOUT_elev.csv',\n",
       " 'SPRA_elev.csv',\n",
       " 'SPRI_elev.csv',\n",
       " 'STAT_elev.csv',\n",
       " 'STEP_elev.csv',\n",
       " 'STON_elev.csv',\n",
       " 'SUFF_elev.csv',\n",
       " 'TANN_elev.csv',\n",
       " 'TICO_elev.csv',\n",
       " 'TULL_elev.csv',\n",
       " 'TUPP_elev.csv',\n",
       " 'TYRO_elev.csv',\n",
       " 'VOOR_elev.csv',\n",
       " 'WALL_elev.csv',\n",
       " 'WALT_elev.csv',\n",
       " 'WANT_elev.csv',\n",
       " 'WARS_elev.csv',\n",
       " 'WARW_elev.csv',\n",
       " 'WATE_elev.csv',\n",
       " 'WBOU_elev.csv',\n",
       " 'WELL_elev.csv',\n",
       " 'WEST_elev.csv',\n",
       " 'WFMB_elev.csv',\n",
       " 'WGAT_elev.csv',\n",
       " 'WHIT_elev.csv',\n",
       " 'WOLC_elev.csv',\n",
       " 'YORK_elev.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to data\n",
    "path_ny = f\"/home/aevans/nwp_bias/src/landtype/elevation/data/CSVs_elevation_ny_30km\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_list_ny = ny_df[\"station\"].to_list()\n",
    "ny_df_lons = ny_df[\"longitude\"].to_list()\n",
    "ny_df_lats = ny_df[\"latitude\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 0\n",
    "# for i in range(1, 127):\n",
    "#     df = pd.read_csv(\n",
    "#         f\"/home/aevans/nwp_bias/src/landtype/elevation/data/CSVs_slope_ny_gfs/aspect_csv_{i}.csv\"\n",
    "#     )\n",
    "#     df.to_csv(\n",
    "#         f\"/home/aevans/nwp_bias/src/landtype/elevation/data/NY/elev/gfs/{station_list_ny[x]}_elev.csv\"\n",
    "#     )\n",
    "#     x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 36] File name too long: \"['ADDI_elev.csv', 'ANDE_elev.csv', 'BATA_elev.csv', 'BEAC_elev.csv', 'BELD_elev.csv', 'BELL_elev.csv', 'BELM_elev.csv', 'BERK_elev.csv', 'BING_elev.csv', 'BKLN_elev.csv', 'BRAN_elev.csv', 'BREW_elev.csv', 'BROC_elev.csv', 'BRON_elev.csv', 'BROO_elev.csv', 'BSPA_elev.csv', 'BUFF_elev.csv', 'BURD_elev.csv', 'BURT_elev.csv', 'CAMD_elev.csv', 'CAPE_elev.csv', 'CHAZ_elev.csv', 'CHES_elev.csv', 'CINC_elev.csv', 'CLAR_elev.csv', 'CLIF_elev.csv', 'CLYM_elev.csv', 'COBL_elev.csv', 'COHO_elev.csv', 'COLD_elev.csv', 'COPA_elev.csv', 'COPE_elev.csv', 'CROG_elev.csv', 'CSQR_elev.csv', 'DELE_elev.csv', 'DEPO_elev.csv', 'DOVE_elev.csv', 'DUAN_elev.csv', 'EAUR_elev.csv', 'EDIN_elev.csv', 'EDWA_elev.csv', 'ELDR_elev.csv', 'ELLE_elev.csv', 'ELMI_elev.csv', 'ESSX_elev.csv', 'FAYE_elev.csv', 'FRED_elev.csv', 'GABR_elev.csv', 'GFAL_elev.csv', 'GFLD_elev.csv', 'GROT_elev.csv', 'GROV_elev.csv', 'HAMM_elev.csv', 'HARP_elev.csv', 'HARR_elev.csv', 'HART_elev.csv', 'HERK_elev.csv', 'HFAL_elev.csv', 'ILAK_elev.csv', 'JOHN_elev.csv', 'JORD_elev.csv', 'KIND_elev.csv', 'LAUR_elev.csv', 'LOUI_elev.csv', 'MALO_elev.csv', 'MANH_elev.csv', 'MEDI_elev.csv', 'MEDU_elev.csv', 'MORR_elev.csv', 'NBRA_elev.csv', 'NEWC_elev.csv', 'NHUD_elev.csv', 'OLDF_elev.csv', 'OLEA_elev.csv', 'ONTA_elev.csv', 'OPPE_elev.csv', 'OSCE_elev.csv', 'OSWE_elev.csv', 'OTIS_elev.csv', 'OWEG_elev.csv', 'PENN_elev.csv', 'PHIL_elev.csv', 'PISE_elev.csv', 'POTS_elev.csv', 'QUEE_elev.csv', 'RAND_elev.csv', 'RAQU_elev.csv', 'REDF_elev.csv', 'REDH_elev.csv', 'ROXB_elev.csv', 'RUSH_elev.csv', 'SARA_elev.csv', 'SBRI_elev.csv', 'SCHA_elev.csv', 'SCHO_elev.csv', 'SCHU_elev.csv', 'SCIP_elev.csv', 'SHER_elev.csv', 'SOME_elev.csv', 'SOUT_elev.csv', 'SPRA_elev.csv', 'SPRI_elev.csv', 'STAT_elev.csv', 'STEP_elev.csv', 'STON_elev.csv', 'SUFF_elev.csv', 'TANN_elev.csv', 'TICO_elev.csv', 'TULL_elev.csv', 'TUPP_elev.csv', 'TYRO_elev.csv', 'VOOR_elev.csv', 'WALL_elev.csv', 'WALT_elev.csv', 'WANT_elev.csv', 'WARS_elev.csv', 'WARW_elev.csv', 'WATE_elev.csv', 'WBOU_elev.csv', 'WELL_elev.csv', 'WEST_elev.csv', 'WFMB_elev.csv', 'WGAT_elev.csv', 'WHIT_elev.csv', 'WOLC_elev.csv', 'YORK_elev.csv']/gfs/1_csv.csv\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m slope_df \u001b[38;5;241m=\u001b[39m stat_anal(sorted_direct, ny_df, station_list_ny, ny_df_lons, ny_df_lats)\n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mstat_anal\u001b[0;34m(directory, state_df, station_list, lonlist, latlist)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m127\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# read in csv\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m---> 78\u001b[0m     elev_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirectory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/gfs/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_csv.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     dfv1 \u001b[38;5;241m=\u001b[39m format_df(elev_df)  \u001b[38;5;66;03m# apply format_df to the elevation data\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     std \u001b[38;5;241m=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mstdev(dfv1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# calculate the standard deviation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 36] File name too long: \"['ADDI_elev.csv', 'ANDE_elev.csv', 'BATA_elev.csv', 'BEAC_elev.csv', 'BELD_elev.csv', 'BELL_elev.csv', 'BELM_elev.csv', 'BERK_elev.csv', 'BING_elev.csv', 'BKLN_elev.csv', 'BRAN_elev.csv', 'BREW_elev.csv', 'BROC_elev.csv', 'BRON_elev.csv', 'BROO_elev.csv', 'BSPA_elev.csv', 'BUFF_elev.csv', 'BURD_elev.csv', 'BURT_elev.csv', 'CAMD_elev.csv', 'CAPE_elev.csv', 'CHAZ_elev.csv', 'CHES_elev.csv', 'CINC_elev.csv', 'CLAR_elev.csv', 'CLIF_elev.csv', 'CLYM_elev.csv', 'COBL_elev.csv', 'COHO_elev.csv', 'COLD_elev.csv', 'COPA_elev.csv', 'COPE_elev.csv', 'CROG_elev.csv', 'CSQR_elev.csv', 'DELE_elev.csv', 'DEPO_elev.csv', 'DOVE_elev.csv', 'DUAN_elev.csv', 'EAUR_elev.csv', 'EDIN_elev.csv', 'EDWA_elev.csv', 'ELDR_elev.csv', 'ELLE_elev.csv', 'ELMI_elev.csv', 'ESSX_elev.csv', 'FAYE_elev.csv', 'FRED_elev.csv', 'GABR_elev.csv', 'GFAL_elev.csv', 'GFLD_elev.csv', 'GROT_elev.csv', 'GROV_elev.csv', 'HAMM_elev.csv', 'HARP_elev.csv', 'HARR_elev.csv', 'HART_elev.csv', 'HERK_elev.csv', 'HFAL_elev.csv', 'ILAK_elev.csv', 'JOHN_elev.csv', 'JORD_elev.csv', 'KIND_elev.csv', 'LAUR_elev.csv', 'LOUI_elev.csv', 'MALO_elev.csv', 'MANH_elev.csv', 'MEDI_elev.csv', 'MEDU_elev.csv', 'MORR_elev.csv', 'NBRA_elev.csv', 'NEWC_elev.csv', 'NHUD_elev.csv', 'OLDF_elev.csv', 'OLEA_elev.csv', 'ONTA_elev.csv', 'OPPE_elev.csv', 'OSCE_elev.csv', 'OSWE_elev.csv', 'OTIS_elev.csv', 'OWEG_elev.csv', 'PENN_elev.csv', 'PHIL_elev.csv', 'PISE_elev.csv', 'POTS_elev.csv', 'QUEE_elev.csv', 'RAND_elev.csv', 'RAQU_elev.csv', 'REDF_elev.csv', 'REDH_elev.csv', 'ROXB_elev.csv', 'RUSH_elev.csv', 'SARA_elev.csv', 'SBRI_elev.csv', 'SCHA_elev.csv', 'SCHO_elev.csv', 'SCHU_elev.csv', 'SCIP_elev.csv', 'SHER_elev.csv', 'SOME_elev.csv', 'SOUT_elev.csv', 'SPRA_elev.csv', 'SPRI_elev.csv', 'STAT_elev.csv', 'STEP_elev.csv', 'STON_elev.csv', 'SUFF_elev.csv', 'TANN_elev.csv', 'TICO_elev.csv', 'TULL_elev.csv', 'TUPP_elev.csv', 'TYRO_elev.csv', 'VOOR_elev.csv', 'WALL_elev.csv', 'WALT_elev.csv', 'WANT_elev.csv', 'WARS_elev.csv', 'WARW_elev.csv', 'WATE_elev.csv', 'WBOU_elev.csv', 'WELL_elev.csv', 'WEST_elev.csv', 'WFMB_elev.csv', 'WGAT_elev.csv', 'WHIT_elev.csv', 'WOLC_elev.csv', 'YORK_elev.csv']/gfs/1_csv.csv\""
     ]
    }
   ],
   "source": [
    "slope_df = stat_anal(sorted_direct, ny_df, station_list_ny, ny_df_lons, ny_df_lats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_df.to_csv(\"/home/aevans/nwp_bias/src/correlation/data/elev_gfs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
