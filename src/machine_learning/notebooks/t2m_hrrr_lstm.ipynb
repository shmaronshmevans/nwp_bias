{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.processing import col_drop\n",
    "from src.processing import get_flag\n",
    "from src.processing import encode\n",
    "from src.processing import normalize\n",
    "from src.processing import get_error\n",
    "from src import fsdp\n",
    "\n",
    "from src.data import hrrr_data\n",
    "from src.data import nysm_data\n",
    "\n",
    "from src.visuals import loss_curves\n",
    "\n",
    "from src.evaluate import eval_lstm\n",
    "\n",
    "from comet_ml import Experiment, Artifact\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "from comet_ml import Optimizer\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import emd\n",
    "import statistics as st\n",
    "from dateutil.parser import parse\n",
    "import warnings\n",
    "import os\n",
    "import xarray as xr\n",
    "import glob\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
    "    CPUOffload,\n",
    "    BackwardPrefetch,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    size_based_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "import argparse\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "    # initialize the process group\n",
    "    dist.init_process_group(\"nccl\", rank = rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_drop(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"level_0\",\n",
    "            \"index\",\n",
    "            \"lead time\",\n",
    "            \"lsm\",\n",
    "            \"index_nysm\",\n",
    "            \"station_nysm\",\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffix(df, stations):\n",
    "    cols = [\"valid_time\", \"time\"]\n",
    "    df = df.rename(\n",
    "        columns={c: c + f\"_{stations[0]}\" for c in df.columns if c not in cols}\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements_from_batch(X, y, s):\n",
    "    cond = np.where(s)\n",
    "    return X[cond], y[cond], s[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_title(station, val_loss):\n",
    "    title = f\"{station}_loss_{val_loss}\"\n",
    "    today = datetime.now()\n",
    "    today_date = today.strftime(\"%Y%m%d\")\n",
    "    today_date_hr = today.strftime(\"%Y%m%d_%H:%M\")\n",
    "\n",
    "    return title, today_date, today_date_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer, rank, sampler):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    ddp_loss = torch.zeros(2).to(rank)\n",
    "    if sampler:\n",
    "        sampler.set_epoch(epoch)\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        for X, y, s in tepoch:\n",
    "            X, y, s = remove_elements_from_batch(X, y, s)\n",
    "            X, y, s = X.to(rank), y.to(rank)\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            ddp_loss[0] += loss.item()\n",
    "            ddp_loss[1] += len(X)\n",
    "        \n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    # loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    if rank == 0:\n",
    "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data_loader, model, loss_function, rank, world_size):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    ddp_loss = torch.zeros(3).to(rank)\n",
    "    with torch.no_grad():\n",
    "        with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "            for X, y, s in tepoch:\n",
    "                X, y, s = remove_elements_from_batch(X, y, s)\n",
    "                X, y, s = X.to(rank), y.to(rank)\n",
    "                output = model(X)\n",
    "                total_loss += loss_function(output, y).item()\n",
    "                ddp_loss[0] += F.nll_loss(output, y, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n",
    "                ddp_loss[2] += len(X)\n",
    "\n",
    "                    # loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    if rank == 0:\n",
    "        test_loss = ddp_loss[0] / ddp_loss[2]\n",
    "        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n",
    "            100. * ddp_loss[1] / ddp_loss[2]))\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM Model\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length, device):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[target].values).float().to(device)\n",
    "        self.X = torch.tensor(dataframe[features].values).float().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        keep_sample = self.dataframe.iloc[i][\"flag\"]\n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start : (i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0 : (i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i], keep_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, num_layers, device):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.to(device)\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_units)\n",
    "            .requires_grad_()\n",
    "            .to(device)\n",
    "        )\n",
    "        c0 = (\n",
    "            torch.zeros(self.num_layers, batch_size, self.hidden_units)\n",
    "            .requires_grad_()\n",
    "            .to(device)\n",
    "        )\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(\n",
    "            hn[0]\n",
    "        ).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsdp_main(rank, world_size, args):\n",
    "    print(\"Am I using GPUS ???\", torch.cuda.is_available())\n",
    "    print(\"Number of gpus: \", torch.cuda.device_count())\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    print(\" *********\")\n",
    "    print(\"::: In Main :::\")\n",
    "\n",
    "    df_train, df_test, features = create_data_for_model()\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"leAiWyR5Ck7tkdiHIT7n6QWNa\",\n",
    "        project_name=\"v4\",\n",
    "        workspace=\"shmaronshmevans\",\n",
    "    )\n",
    "    setup(rank, world_size)\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size, 'shuffle':True}\n",
    "    test_kwargs = {'batch_size': args.batch_size, 'shuffle': False}\n",
    "    cuda_kwargs = {'num_workers': 2,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': False}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "    train_dataset = SequenceDataset(\n",
    "        df_train,\n",
    "        target=target,\n",
    "        features=features,\n",
    "        sequence_length=sequence_length,\n",
    "        device=device,\n",
    "    )\n",
    "    test_dataset = SequenceDataset(\n",
    "        df_test,\n",
    "        target=target,\n",
    "        features=features,\n",
    "        sequence_length=sequence_length,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "    sampler2 = DistributedSampler(test_dataset, rank=rank, num_replicas=world_size)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)\n",
    "    my_auto_wrap_policy = functools.partial(\n",
    "        size_based_auto_wrap_policy, min_num_params=100\n",
    "    )\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "    init_start_event = torch.cuda.Event(enable_timing=True)\n",
    "    init_end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    model = ShallowRegressionLSTM(num_sensors=int(len(features)),\n",
    "        hidden_units=int(len(features)),\n",
    "        num_layers=args.num_layers,\n",
    "        device=device).to(rank)\n",
    "    model = FSDP(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    loss_function = nn.MSELoss()\n",
    "\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1)\n",
    "    init_start_event.record()\n",
    "    train_loss_ls = []\n",
    "    test_loss_ls = []\n",
    "    for ix_epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train_model(train_loader, model, loss_function, optimizer, rank, sampler1)\n",
    "        test_loss = test_model(test_loader, model, loss_function, rank, world_size)\n",
    "        scheduler.step()\n",
    "        print()\n",
    "        experiment.set_epoch(ix_epoch)\n",
    "        train_loss_ls.append(train_loss)\n",
    "        test_loss_ls.append(val_loss)\n",
    "\n",
    "    init_end_event.record()\n",
    "    if rank == 0:\n",
    "        print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n",
    "        print(f\"{model}\")\n",
    "\n",
    "    if args.save_model:\n",
    "        # use a barrier to make sure training is done on all ranks\n",
    "        dist.barrier()\n",
    "        states = model.state_dict()\n",
    "        if rank == 0:\n",
    "            torch.save(states, \"mnist_cnn.pt\")\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "\n",
    "        # Report multiple hyperparameters using a dictionary:\n",
    "    hyper_params = {\n",
    "        \"num_layers\": num_layers,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"sequence_length\": sequence_length,\n",
    "        \"num_hidden_units\": num_hidden_units,\n",
    "        \"forecast_lead\": forecast_lead,\n",
    "    }\n",
    "\n",
    "    title, today_date, today_date_hr = get_time_title(station, val_loss)\n",
    "\n",
    "    # evaluate model\n",
    "    eval_lstm.eval_model(\n",
    "        train_dataset,\n",
    "        df_train,\n",
    "        df_test,\n",
    "        test_loader,\n",
    "        model,\n",
    "        batch_size,\n",
    "        title,\n",
    "        target,\n",
    "        new_df,\n",
    "        features,\n",
    "        today_date,\n",
    "        today_date_hr,\n",
    "        experiment,\n",
    "    )\n",
    "    loss_curves.loss_curves(train_loss_ls, test_loss_ls, today_date, title, today_date_hr)\n",
    "\n",
    "    print(\"Successful Experiment\")\n",
    "    # Seamlessly log your Pytorch model\n",
    "    log_model(experiment, model, model_name=\"v4\")\n",
    "    experiment.log_metrics(hyper_params, epoch=ix_epoch)\n",
    "    experiment.end()\n",
    "    cleanup()\n",
    "    print(\"... completed ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am I using GPUS ??? True\n",
      "Number of gpus:  1\n",
      "cuda\n",
      " *********\n",
      "::: In Main :::\n",
      "-- loading data from nysm --\n",
      "-- loading data from hrrr --\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[260], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     16\u001b[0m WORLD_SIZE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m---> 17\u001b[0m mp\u001b[38;5;241m.\u001b[39mspawn(fn \u001b[38;5;241m=\u001b[39m fsdp\u001b[38;5;241m.\u001b[39mfsdp_main,\n\u001b[1;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39m(WORLD_SIZE, args),\n\u001b[1;32m     19\u001b[0m     nprocs\u001b[38;5;241m=\u001b[39mWORLD_SIZE,\n\u001b[1;32m     20\u001b[0m     join\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:239\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    235\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    236\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    238\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 239\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:197\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    196\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    198\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:109\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ready \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinels\u001b[39m.\u001b[39;49mkeys(),\n\u001b[1;32m    111\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m error_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m sentinel \u001b[39min\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--sequence_length', type=int, default=250, help='input sequence length')\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-3, help='learning rate')\n",
    "    parser.add_argument('--num_layers', type=int, default=3, help='number of layers')\n",
    "    parser.add_argument('--epochs', type=int, default=500, help='number of epochs to train (default: 500)')\n",
    "    parser.add_argument('--seed', type=int, default=101, help='random seed (default: 101)')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0, help='weight decay')\n",
    "    parser.add_argument('--save-model', action='store_true', default=False, help='For Saving the current Model')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    WORLD_SIZE = torch.cuda.device_count()\n",
    "    mp.spawn(fn = fsdp.fsdp_main,\n",
    "        args=(WORLD_SIZE, args),\n",
    "        nprocs=WORLD_SIZE,\n",
    "        join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--weight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight decay\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--save-model\u001b[39m\u001b[38;5;124m'\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_true\u001b[39m\u001b[38;5;124m'\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFor Saving the current Model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[1;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m     16\u001b[0m WORLD_SIZE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/argparse.py:1828\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[39mif\u001b[39;00m argv:\n\u001b[1;32m   1827\u001b[0m     msg \u001b[39m=\u001b[39m _(\u001b[39m'\u001b[39m\u001b[39munrecognized arguments: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror(msg \u001b[39m%\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(argv))\n\u001b[1;32m   1829\u001b[0m \u001b[39mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/argparse.py:2582\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_usage(_sys\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m   2581\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mprog\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprog, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m: message}\n\u001b[0;32m-> 2582\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexit(\u001b[39m2\u001b[39;49m, _(\u001b[39m'\u001b[39;49m\u001b[39m%(prog)s\u001b[39;49;00m\u001b[39m: error: \u001b[39;49m\u001b[39m%(message)s\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m) \u001b[39m%\u001b[39;49m args)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/argparse.py:2569\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[39mif\u001b[39;00m message:\n\u001b[1;32m   2568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_print_message(message, _sys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m-> 2569\u001b[0m _sys\u001b[39m.\u001b[39;49mexit(status)\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
