{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn related imports\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# pytorch relates imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    DeepLift,\n",
    "    GradientShap,\n",
    "    NoiseTunnel,\n",
    "    FeatureAblation,\n",
    ")\n",
    "\n",
    "\" https://captum.ai/tutorials/House_Prices_Regression_Interpret \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hrrr_data():\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\"]\n",
    "    savedir = \"/home/aevans/nwp_bias/src/machine_learning/data/hrrr_data/ny/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    hrrr_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        for month in np.arange(1, 13):\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}HRRR_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                hrrr_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}HRRR_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    hrrr_fcast_and_error_df = pd.concat(hrrr_fcast_and_error)\n",
    "    hrrr_fcast_and_error_df = hrrr_fcast_and_error_df.reset_index().dropna()\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return hrrr_fcast_and_error_df\n",
    "\n",
    "\n",
    "def columns_drop(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"level_0\",\n",
    "            \"index\",\n",
    "            \"lead time\",\n",
    "            \"lsm\",\n",
    "            \"index_nysm\",\n",
    "            \"station_nysm\",\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_suffix(df, stations):\n",
    "    cols = [\"valid_time\", \"time\"]\n",
    "    df = df.rename(\n",
    "        columns={c: c + f\"_{stations[0]}\" for c in df.columns if c not in cols}\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_nysm_data():\n",
    "    # these parquet files are created by running \"get_resampled_nysm_data.ipynb\"\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    nysm_1H = []\n",
    "    for year in np.arange(2018, 2023):\n",
    "        df = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "        df.reset_index(inplace=True)\n",
    "        nysm_1H.append(df)\n",
    "    nysm_1H_obs = pd.concat(nysm_1H)\n",
    "    nysm_1H_obs[\"snow_depth\"] = nysm_1H_obs[\"snow_depth\"].fillna(-999)\n",
    "    nysm_1H_obs.dropna(inplace=True)\n",
    "    return nysm_1H_obs\n",
    "\n",
    "\n",
    "def remove_elements_from_batch(X, y, s):\n",
    "    cond = np.where(s)\n",
    "    return X[cond], y[cond], s[cond]\n",
    "\n",
    "\n",
    "def nwp_error(target, station, df):\n",
    "    vars_dict = {\n",
    "        \"t2m\": \"tair\",\n",
    "        \"mslma\": \"pres\",\n",
    "    }\n",
    "    nysm_var = vars_dict.get(target)\n",
    "\n",
    "    df = df[df[target] > -999]\n",
    "\n",
    "    df[\"target_error\"] = df[f\"{target}_{station}\"] - df[f\"{nysm_var}_{station}\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(\n",
    "            self.num_layers, batch_size, self.hidden_units\n",
    "        ).requires_grad_()\n",
    "        c0 = torch.zeros(\n",
    "            self.num_layers, batch_size, self.hidden_units\n",
    "        ).requires_grad_()\n",
    "\n",
    "        # peephole architecture\n",
    "\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.linear(\n",
    "            hn[0]\n",
    "        ).flatten()  # First dim of Hn is num_layers, which is set to 1 above.\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import emd\n",
    "import statistics as st\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "def col_drop(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"flag\",\n",
    "            \"station\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"t2m\",\n",
    "            \"sh2\",\n",
    "            \"d2m\",\n",
    "            \"r2\",\n",
    "            \"u10\",\n",
    "            \"v10\",\n",
    "            \"tp\",\n",
    "            \"mslma\",\n",
    "            \"orog\",\n",
    "            \"tcc\",\n",
    "            \"asnow\",\n",
    "            \"cape\",\n",
    "            \"dswrf\",\n",
    "            \"dlwrf\",\n",
    "            \"gh\",\n",
    "            \"u_total\",\n",
    "            \"u_dir\",\n",
    "            \"new_tp\",\n",
    "            \"lat\",\n",
    "            \"lon\",\n",
    "            \"elev\",\n",
    "            \"tair\",\n",
    "            \"ta9m\",\n",
    "            \"td\",\n",
    "            \"relh\",\n",
    "            \"srad\",\n",
    "            \"pres\",\n",
    "            \"mslp\",\n",
    "            \"wspd_sonic\",\n",
    "            \"wmax_sonic\",\n",
    "            \"wdir_sonic\",\n",
    "            \"precip_total\",\n",
    "            \"snow_depth\",\n",
    "        ]\n",
    "    )\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"time\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"station\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"tair\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"ta9m\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"td\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"relh\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"srad\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"pres\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"wspd\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"wmax\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"wdir\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"precip_total\")))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex=\"snow_depth\")))]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_climate_df(data_path):\n",
    "    \"\"\"\n",
    "    Formats a climate data file located at the specified `data_path` into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The file path for the climate data file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the climate data, with the first column renamed to \"year\".\n",
    "    \"\"\"\n",
    "    raw_index = np.loadtxt(f\"{data_path}\")\n",
    "    cl_index = pd.DataFrame(raw_index)\n",
    "    cl_index = cl_index.rename(columns={0: \"year\"})\n",
    "    return cl_index\n",
    "\n",
    "\n",
    "def get_clim_indexes(df, valid_times):\n",
    "    \"\"\"\n",
    "    Fetch climate indexes data and add corresponding index values to the input DataFrame.\n",
    "\n",
    "    This function takes a DataFrame (`df`) containing weather data with a 'valid_time' column representing\n",
    "    timestamps. It reads climate indexes data from text files in the specified directory and extracts index\n",
    "    values corresponding to the month and year of each timestamp in the DataFrame. The extracted index values\n",
    "    are then added to the DataFrame with new columns named after each index.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame containing weather data with a 'valid_time' column.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with additional columns for each climate index containing their values.\n",
    "    \"\"\"\n",
    "\n",
    "    clim_df_path = \"/home/aevans/nwp_bias/src/correlation/data/indexes/\"\n",
    "    directory = os.listdir(clim_df_path)\n",
    "    df[\"valid_time\"] = valid_times\n",
    "\n",
    "    # Loop through each file in the specified directory\n",
    "    for d in directory:\n",
    "        if d.endswith(\".txt\"):\n",
    "            # Read the climate index data from the file and format it into a DataFrame\n",
    "            clim_df = format_climate_df(f\"{clim_df_path}{d}\")\n",
    "            index_name = d.split(\".\")[0]\n",
    "\n",
    "            clim_ind_ls = []\n",
    "            for t, _ in enumerate(df[\"valid_time\"]):\n",
    "                time_obj = df[\"valid_time\"].iloc[t]\n",
    "                dt_object = parse(str(time_obj))\n",
    "                year = dt_object.strftime(\"%Y\")\n",
    "                month = dt_object.strftime(\"%m\")\n",
    "                # Filter the climate DataFrame to get data for the specific year\n",
    "                df1 = clim_df.loc[clim_df[\"year\"] == int(year)]\n",
    "                df1 = df1.drop(columns=\"year\")\n",
    "                row_list = df1.values\n",
    "                keys = df1.keys()\n",
    "                key_vals = keys.tolist()\n",
    "\n",
    "                # Extract the index value corresponding to the month of the timestamp\n",
    "                the_list = []\n",
    "                for n, _ in enumerate(key_vals):\n",
    "                    val1 = key_vals[n]\n",
    "                    val2 = row_list[0, n]\n",
    "                    tup = (val1, val2)\n",
    "                    the_list.append(tup)\n",
    "                for k, r in the_list:\n",
    "                    if str(k).zfill(2) == month:\n",
    "                        clim_ind_ls.append(r)\n",
    "\n",
    "            # Add the climate index values as a new column in the DataFrame\n",
    "            df[index_name] = clim_ind_ls\n",
    "\n",
    "    df = df.drop(columns=\"valid_time\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode(data, col, max_val, valid_times):\n",
    "    data[\"valid_time\"] = valid_times\n",
    "    data = data[data.columns.drop(list(data.filter(regex=\"day\")))]\n",
    "    data[\"day_of_year\"] = data[\"valid_time\"].dt.dayofyear\n",
    "    data[col + \"_sin\"] = np.sin(2 * np.pi * data[col] / max_val).astype(float)\n",
    "    data[col + \"_cos\"] = np.cos(2 * np.pi * data[col] / max_val)\n",
    "    data = data.drop(columns=[\"valid_time\", \"day_of_year\"]).astype(float)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize_df(df, valid_times):\n",
    "    print(\"init normalizer\")\n",
    "    df = col_drop(df)\n",
    "    the_df = df.dropna()\n",
    "    for k, r in the_df.items():\n",
    "        if len(the_df[k].unique()) == 1:\n",
    "            org_str = str(k)\n",
    "            my_str = org_str[:-5]\n",
    "            vals = the_df.filter(regex=my_str)\n",
    "            vals = vals.loc[0].tolist()\n",
    "            means = st.mean(vals)\n",
    "            stdevs = st.pstdev(vals)\n",
    "            the_df[k] = (the_df[k] - means) / stdevs\n",
    "\n",
    "            the_df = the_df.fillna(0)\n",
    "        if re.search(\n",
    "            \"t2m|u10|v10\",\n",
    "            k,\n",
    "        ):\n",
    "            ind_val = the_df.columns.get_loc(k)\n",
    "            x = the_df[k]\n",
    "            imf = emd.sift.sift(x)\n",
    "            the_df = the_df.drop(columns=k)\n",
    "            for i in range(imf.shape[1]):\n",
    "                imf_ls = imf[:, i].tolist()\n",
    "                # Inserting the column at the\n",
    "                # beginning in the DataFrame\n",
    "                my_loc = ind_val + i\n",
    "                the_df.insert(loc=(my_loc), column=f\"{k}_imf_{i}\", value=imf_ls)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for k, r in the_df.items():\n",
    "        means = st.mean(the_df[k])\n",
    "        stdevs = st.pstdev(the_df[k])\n",
    "        the_df[k] = (the_df[k] - means) / stdevs\n",
    "\n",
    "    final_df = the_df.fillna(0)\n",
    "    print(\"!!! Dropping Columns !!!\")\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"latitude\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"longitude\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"u_total\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"mslp\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"orog\")))]\n",
    "\n",
    "    print(\"--- configuring data ---\")\n",
    "    final_df = encode(final_df, \"day_of_year\", 366, valid_times)\n",
    "    final_df = get_clim_indexes(final_df, valid_times)\n",
    "    og_features = list(final_df.columns.difference([\"target_error\"]))\n",
    "    new_features = og_features\n",
    "\n",
    "    print(\"---normalize successful---\")\n",
    "\n",
    "    return final_df, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"-- loading data from nysm --\")\n",
    "# # read in hrrr and nysm data\n",
    "# nysm_df = load_nysm_data()\n",
    "# nysm_df.reset_index(inplace=True)\n",
    "# print(\"-- loading data from hrrr --\")\n",
    "# hrrr_df = read_hrrr_data()\n",
    "# nysm_df = nysm_df.rename(columns={\"time_1H\": \"valid_time\"})\n",
    "# mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "# nysm_df = nysm_df[nysm_df[\"valid_time\"].isin(mytimes)]\n",
    "# nysm_df.to_csv(\"/home/aevans/nwp_bias/src/machine_learning/frankenstein/test.csv\")\n",
    "\n",
    "# # tabular data paths\n",
    "# nysm_cats_path = \"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\"\n",
    "\n",
    "# # tabular data dataframes\n",
    "# print(\"-- adding geo data --\")\n",
    "# nysm_cats_df = pd.read_csv(nysm_cats_path)\n",
    "\n",
    "# print(\"-- locating target data --\")\n",
    "# # partition out parquets by nysm climate division\n",
    "# category = \"Western Plateau\"\n",
    "# nysm_cats_df1 = nysm_cats_df[nysm_cats_df[\"climate_division_name\"] == category]\n",
    "# stations = nysm_cats_df1[\"stid\"].tolist()\n",
    "# hrrr_df1 = hrrr_df[hrrr_df[\"station\"].isin(stations)]\n",
    "# nysm_df1 = nysm_df[nysm_df[\"station\"].isin(stations)]\n",
    "# print(\"-- cleaning target data --\")\n",
    "# master_df = hrrr_df1.merge(nysm_df1, on=\"valid_time\", suffixes=(None, \"_nysm\"))\n",
    "# master_df = master_df.drop_duplicates(\n",
    "#     subset=[\"valid_time\", \"station\", \"t2m\"], keep=\"first\"\n",
    "# )\n",
    "# print(\"-- finalizing dataframe --\")\n",
    "# df = columns_drop(master_df)\n",
    "# stations = df[\"station\"].unique()\n",
    "\n",
    "# master_df = df[df[\"station\"] == stations[0]]\n",
    "# master_df = add_suffix(master_df, stations)\n",
    "\n",
    "# for station in stations:\n",
    "#     df1 = df[df[\"station\"] == station]\n",
    "#     master_df = master_df.merge(df1, on=\"valid_time\", suffixes=(None, f\"_{station}\"))\n",
    "\n",
    "# the_df = master_df.copy()\n",
    "\n",
    "# the_df.dropna(inplace=True)\n",
    "# print(\"getting flag and error\")\n",
    "# the_df = get_flag.get_flag(the_df)\n",
    "\n",
    "# the_df = nwp_error(\"t2m\", \"OLEA\", the_df)\n",
    "# new_df = the_df.copy()\n",
    "\n",
    "# print(\"Data Processed\")\n",
    "# print(\"--init model LSTM--\")\n",
    "# valid_times = new_df[\"valid_time\"].tolist()\n",
    "# # columns to reintigrate back into the df after model is done running\n",
    "# cols_to_carry = [\"valid_time\", \"flag\"]\n",
    "\n",
    "# # establish target\n",
    "# target_sensor = \"target_error\"\n",
    "# lstm_df, features = normalize_df(new_df, valid_times)\n",
    "# forecast_lead = forecast_lead\n",
    "# target = f\"{target_sensor}_lead_{forecast_lead}\"\n",
    "# lstm_df[target] = lstm_df[target_sensor].shift(-forecast_lead)\n",
    "# lstm_df = lstm_df.iloc[:-forecast_lead]\n",
    "\n",
    "# # create train and test set\n",
    "# length = len(lstm_df)\n",
    "# test_len = int(length * 0.2)\n",
    "# df_train = lstm_df.iloc[test_len:].copy()\n",
    "# df_test = lstm_df.iloc[:test_len].copy()\n",
    "# print(\"Test Set Fraction\", len(df_test) / len(lstm_df))\n",
    "# df_train = df_train.fillna(0)\n",
    "# df_test = df_test.fillna(0)\n",
    "\n",
    "# # bring back columns\n",
    "# for c in cols_to_carry:\n",
    "#     df_train[c] = the_df[c]\n",
    "#     df_test[c] = the_df[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20231023/OLEA_loss_0.8970391166190339_20231023_15:56/OLEA_loss_0.8970391166190339.txt\",\n",
    "    dtype=str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "num_layers = 3\n",
    "model = ShallowRegressionLSTM(\n",
    "    num_sensors=len(features), hidden_units=len(features), num_layers=num_layers\n",
    ")\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20231023/OLEA_loss_0.8970391166190339_20231023_15:56/OLEA_loss_0.8970391166190339.pth\"\n",
    "    )\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "ig_nt = NoiseTunnel(ig)\n",
    "dl = DeepLift(model)\n",
    "gs = GradientShap(model)\n",
    "fa = FeatureAblation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "e = 20\n",
    "n = 10\n",
    "# prepare attributions for visualization\n",
    "\n",
    "x_axis_data = np.arange(features.shape[0])\n",
    "x_axis_data_labels = list(map(lambda idx: features[idx], x_axis_data))\n",
    "\n",
    "while e < len(x_axis_data):\n",
    "    # ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "    # ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(\n",
    "    #     ig_nt_attr_test_sum, ord=1\n",
    "    # )\n",
    "\n",
    "    lin_weight = model.linear.weight[0].detach().numpy()\n",
    "    y_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n",
    "\n",
    "    width = 0.14\n",
    "    legends = [\"Weights\"]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    ax.set_title(\n",
    "        \"Comparing input feature importances across multiple algorithms and learned weights\"\n",
    "    )\n",
    "    ax.set_ylabel(\"Attributions\")\n",
    "\n",
    "    FONT_SIZE = 16\n",
    "    plt.rc(\"font\", size=FONT_SIZE)  # fontsize of the text sizes\n",
    "    plt.rc(\"axes\", titlesize=FONT_SIZE)  # fontsize of the axes title\n",
    "    plt.rc(\"axes\", labelsize=FONT_SIZE)  # fontsize of the x and y labels\n",
    "    plt.rc(\"legend\", fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "    print(x_axis_data.shape)\n",
    "\n",
    "    # ax.bar(\n",
    "    #     x_axis_data[b:e] + width,\n",
    "    #     ig_nt_attr_test_norm_sum[b:e, n],\n",
    "    #     width,\n",
    "    #     align=\"center\",\n",
    "    #     alpha=0.7,\n",
    "    #     color=\"#A90000\",\n",
    "    # )\n",
    "    ax.bar(\n",
    "        x_axis_data[b:e] + 5 * width,\n",
    "        y_axis_lin_weight[b:e],\n",
    "        width,\n",
    "        align=\"center\",\n",
    "        alpha=1.0,\n",
    "        color=\"grey\",\n",
    "    )\n",
    "    ax.autoscale_view()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax.set_xticks(x_axis_data[b:e] + 0.5)\n",
    "    ax.set_xticklabels(x_axis_data_labels[b:e], rotation=90)\n",
    "\n",
    "    plt.legend(legends, loc=3)\n",
    "    plt.show()\n",
    "\n",
    "    b += 20\n",
    "    e += 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
