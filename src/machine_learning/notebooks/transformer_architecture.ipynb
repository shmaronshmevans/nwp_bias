{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.ops.misc import MLP, Conv2dNormActivation\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvStemConfig(NamedTuple):\n",
    "    out_channels: int\n",
    "    kernel_size: int\n",
    "    stride: int\n",
    "    norm_layer: Callable[..., nn.Module] = nn.BatchNorm2d\n",
    "    activation_layer: Callable[..., nn.Module] = nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(\n",
    "            in_dim,\n",
    "            [mlp_dim, in_dim],\n",
    "            activation_layer=nn.GELU,\n",
    "            inplace=None,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            hidden_dim, num_heads, dropout=attention_dropout, batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(\n",
    "            input.dim() == 3,\n",
    "            f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\",\n",
    "        )\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.empty(1, seq_length, hidden_dim).normal_(std=0.02)\n",
    "        )  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(\n",
    "            input.dim() == 3,\n",
    "            f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\",\n",
    "        )\n",
    "        input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/920232796/SETR-pytorch/tree/master\n",
    "class Decoder2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[512, 256, 128, 64]):\n",
    "        super().__init__()\n",
    "        self.decoder_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], 3, padding=1),\n",
    "            nn.BatchNorm2d(features[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "        )\n",
    "        self.decoder_2 = nn.Sequential(\n",
    "            nn.Conv2d(features[0], features[1], 3, padding=1),\n",
    "            nn.BatchNorm2d(features[1]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "        )\n",
    "        self.decoder_3 = nn.Sequential(\n",
    "            nn.Conv2d(features[1], features[2], 3, padding=1),\n",
    "            nn.BatchNorm2d(features[2]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "        )\n",
    "        self.decoder_4 = nn.Sequential(\n",
    "            nn.Conv2d(features[2], features[3], 3, padding=1),\n",
    "            nn.BatchNorm2d(features[3]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True),\n",
    "        )\n",
    "\n",
    "        self.final_out = nn.Conv2d(features[-1], out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder_1(x)\n",
    "        x = self.decoder_2(x)\n",
    "        x = self.decoder_3(x)\n",
    "        x = self.decoder_4(x)\n",
    "        x = self.final_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 768,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._assert(\n",
    "            image_size % patch_size == 0, \"Input shape indivisible by patch size!\"\n",
    "        )\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        if conv_stem_configs is not None:\n",
    "            # As per https://arxiv.org/abs/2106.14881\n",
    "            seq_proj = nn.Sequential()\n",
    "            prev_channels = 3\n",
    "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
    "                seq_proj.add_module(\n",
    "                    f\"conv_bn_relu_{i}\",\n",
    "                    Conv2dNormActivation(\n",
    "                        in_channels=prev_channels,\n",
    "                        out_channels=conv_stem_layer_config.out_channels,\n",
    "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
    "                        stride=conv_stem_layer_config.stride,\n",
    "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
    "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
    "                    ),\n",
    "                )\n",
    "                prev_channels = conv_stem_layer_config.out_channels\n",
    "            seq_proj.add_module(\n",
    "                \"conv_last\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1\n",
    "                ),\n",
    "            )\n",
    "            self.conv_proj: nn.Module = seq_proj\n",
    "        else:\n",
    "            self.conv_proj = nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=hidden_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "            )\n",
    "\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Add a class token this is just zeroes but can be something else\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        # self.err_tokens = [nn.Parameter(torch.zeros(1, 1, hidden_dim // 2)) for _ in range(18)]\n",
    "        seq_length += 1\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            norm_layer,\n",
    "        )\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = (\n",
    "                self.conv_proj.in_channels\n",
    "                * self.conv_proj.kernel_size[0]\n",
    "                * self.conv_proj.kernel_size[1]\n",
    "            )\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(\n",
    "            self.conv_proj.conv_last, nn.Conv2d\n",
    "        ):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight,\n",
    "                mean=0.0,\n",
    "                std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels),\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(\n",
    "            self.heads.pre_logits, nn.Linear\n",
    "        ):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(\n",
    "                self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in)\n",
    "            )\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(\n",
    "            h == self.image_size,\n",
    "            f\"Wrong image height! Expected {self.image_size} but got {h}!\",\n",
    "        )\n",
    "        torch._assert(\n",
    "            w == self.image_size,\n",
    "            f\"Wrong image width! Expected {self.image_size} but got {w}!\",\n",
    "        )\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" is just a register and is ignored\n",
    "        x = x[:, 1:]\n",
    "\n",
    "        x = self.heads(x)\n",
    "\n",
    "        x = torch.reshape(\n",
    "            x.permute(0, 2, 1),\n",
    "            (\n",
    "                x.shape[0],\n",
    "                -1,\n",
    "                self.image_size // self.patch_size,\n",
    "                self.image_size // self.patch_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Embedding and Positional Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
