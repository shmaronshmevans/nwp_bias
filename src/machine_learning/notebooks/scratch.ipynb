{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import hrrr_data, nam_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import statistics as st\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADDI', 'ANDE', 'BATA', 'BEAC', 'BELD', 'BELL', 'BELM', 'BERK',\n",
       "       'BING', 'BKLN', 'BRAN', 'BREW', 'BROC', 'BRON', 'BROO', 'BSPA',\n",
       "       'BUFF', 'BURD', 'BURT', 'CAMD', 'CAPE', 'CHAZ', 'CHES', 'CINC',\n",
       "       'CLAR', 'CLIF', 'CLYM', 'COBL', 'COHO', 'COLD', 'COPA', 'COPE',\n",
       "       'CROG', 'CSQR', 'DELE', 'DEPO', 'DOVE', 'DUAN', 'EAUR', 'EDIN',\n",
       "       'EDWA', 'ELDR', 'ELLE', 'ELMI', 'ESSX', 'FAYE', 'FRED', 'GABR',\n",
       "       'GFAL', 'GFLD', 'GROT', 'GROV', 'HAMM', 'HARP', 'HARR', 'HART',\n",
       "       'HERK', 'HFAL', 'ILAK', 'JOHN', 'JORD', 'KIND', 'LAUR', 'LOUI',\n",
       "       'MALO', 'MANH', 'MEDI', 'MEDU', 'MORR', 'NBRA', 'NEWC', 'NHUD',\n",
       "       'OLDF', 'OLEA', 'ONTA', 'OPPE', 'OSCE', 'OSWE', 'OTIS', 'OWEG',\n",
       "       'PENN', 'PHIL', 'PISE', 'POTS', 'QUEE', 'RAND', 'RAQU', 'REDF',\n",
       "       'REDH', 'ROXB', 'RUSH', 'SARA', 'SBRI', 'SCHA', 'SCHO', 'SCHU',\n",
       "       'SCIP', 'SHER', 'SOME', 'SOUT', 'SPRA', 'SPRI', 'STAT', 'STEP',\n",
       "       'STON', 'SUFF', 'TANN', 'TICO', 'TULL', 'TUPP', 'TYRO', 'VOOR',\n",
       "       'WALL', 'WALT', 'WANT', 'WARS', 'WARW', 'WATE', 'WBOU', 'WELL',\n",
       "       'WEST', 'WFMB', 'WGAT', 'WHIT', 'WOLC', 'YORK'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nysm_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "stations = nysm_df[\"stid\"].unique()\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_path = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250508\"\n",
    "\n",
    "end_path = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus_v2\"\n",
    ")\n",
    "\n",
    "\n",
    "files = os.listdir(q_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stations:\n",
    "    for f in files:\n",
    "        if s in f:\n",
    "            src = os.path.join(q_path, f)\n",
    "            dst = os.path.join(end_path, f)\n",
    "\n",
    "            if os.path.isdir(src):\n",
    "                # Ensure the destination directory exists\n",
    "                if not os.path.exists(dst):\n",
    "                    os.makedirs(dst)\n",
    "\n",
    "                for item in os.listdir(src):\n",
    "                    src_item = os.path.join(src, item)\n",
    "                    dst_item = os.path.join(dst, item)\n",
    "\n",
    "                    if not os.path.exists(dst_item):\n",
    "                        if os.path.isdir(src_item):\n",
    "                            shutil.copytree(src_item, dst_item)\n",
    "                        else:\n",
    "                            shutil.copy2(src_item, dst_item)\n",
    "                    else:\n",
    "                        print(f\"Skipped {dst_item} (already exists)\")\n",
    "            else:\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy2(src, dst)\n",
    "                else:\n",
    "                    print(f\"Skipped {dst} (already exists)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_path = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/oksm_hrrr\"\n",
    "\n",
    "for root, dirs, files in os.walk(end_path):\n",
    "    for name in files:\n",
    "        if \"full\" in name:\n",
    "            file_path = os.path.join(root, name)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed file: {file_path}\")\n",
    "    for name in dirs:\n",
    "        if \"full\" in name:\n",
    "            dir_path = os.path.join(root, name)\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Removed directory: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/HRRR/exclusion_buffer/\"\n",
    "\n",
    "files_ = os.listdir(directory_path)\n",
    "\n",
    "master = pd.DataFrame()\n",
    "for f in files_:\n",
    "    if \"csv\" in f:\n",
    "        df = pd.read_csv(f\"{directory_path}{f}\")\n",
    "        master = pd.concat([master, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = master[\"station\"].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for s in stations:\n",
    "    df_ = master[master[\"station\"] == s]\n",
    "    plt.plot(df_[\"buffer\"].values, df_[\"mae\"].values, label=s)\n",
    "\n",
    "plt.xlabel(\"Buffer (km)\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.title(\"MAE vs Buffer for Each Station\")\n",
    "plt.legend(\n",
    "    title=\"Station\", bbox_to_anchor=(1.05, 1), loc=\"upper left\"\n",
    ")  # Moves legend outside plot\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = \"East Central\"\n",
    "\n",
    "oksm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/oksm.csv\")\n",
    "df = oksm_clim[oksm_clim[\"Climate_division\"] == c]\n",
    "stations = df[\"stid\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oksmdf = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/data/oksm/oksm_1H_obs_2018.parquet\"\n",
    ").reset_index()\n",
    "oksmdf = oksmdf[oksmdf[\"station\"].isin(stations)]\n",
    "oksmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrrdf = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/oksm/hrrr_data/fh01/HRRR_2018_01_direct_compare_to_oksm_sites_mask_water.parquet\"\n",
    ").reset_index()\n",
    "hrrrdf = hrrrdf[hrrrdf[\"station\"].isin(stations)]\n",
    "hrrrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = station_to_climdiv.get(\"VOOR\")\n",
    "clim_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.to_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/notebooks/random_nysm_by_climdiv.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df[df[\"station\"] == \"ACME\"]\n",
    "# plt.plot(df_filt['time_1H'], df_filt['precip_total'])\n",
    "df_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your directory of parquet files\n",
    "path = \"/home/aevans/nwp_bias/src/landtype/NY_cartopy/oksm_v3/\"\n",
    "input_dir = os.listdir(path)\n",
    "\n",
    "# Load and concatenate all Parquet files\n",
    "for file in input_dir:\n",
    "    print(file)\n",
    "    df = pd.read_parquet(f\"{path}{file}\").reset_index()\n",
    "    # Ensure 'time' column is datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    # Format: 'YYYY-MM-DD HH:MM:00'\n",
    "    df[\"time\"] = df[\"time\"].apply(lambda t: f\"{t:%Y-%m-%d} {t:%H}:{t:%M}:00\")\n",
    "    df.set_index([\"station\", \"time\"]).sort_index()\n",
    "    df.to_parquet(f\"{path}{file}\")\n",
    "    print(file, \"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/home/aevans/nwp_bias/data/oksm/oksm_1H_obs_2018.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(df[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df[\"station\"].unique():\n",
    "    df_filt = df[df[\"station\"] == s]\n",
    "    for c in df_filt:\n",
    "        if c == \"station\" or c == \"time_1H\":\n",
    "            continue\n",
    "        else:\n",
    "            print(c)\n",
    "            print(\"MAX\", df_filt[c].max())\n",
    "            print(\"MIN\", df_filt[c].min())\n",
    "            print(\"MEAN\", st.mean(df_filt[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/aevans/nwp_bias/src/landtype/NY_cartopy/\"\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "for d in files:\n",
    "    if d.endswith(\".parquet\") or d.endswith(\".mts\"):\n",
    "        os.remove(os.path.join(directory_path, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"/home/aevans/nwp_bias/src/landtype/NY_cartopy/\"\n",
    "\n",
    "# List all .mts files\n",
    "mts_files = [f for f in os.listdir(directory) if f.endswith(\".mts\")]\n",
    "\n",
    "for mts_file in mts_files:\n",
    "    mts_path = os.path.join(directory, mts_file)\n",
    "    try:\n",
    "        # Step 1: Read the .mts file — customize if needed\n",
    "        with open(mts_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Optional: Parse lines into a list of lists\n",
    "        data = [line.strip().split() for line in lines]\n",
    "\n",
    "        # Optional: Create column names or infer them\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = [f\"col{i}\" for i in range(df.shape[1])]\n",
    "\n",
    "        # Step 2: Save to .parquet\n",
    "        parquet_path = os.path.join(directory, mts_file.replace(\".mts\", \".parquet\"))\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "        print(f\"✅ Converted {mts_file} → {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to convert {mts_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okdf = pd.read_parquet(\"/home/aevans/nwp_bias/data/oksm/oksm_1H_obs_2018.parquet\")\n",
    "okdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_files(base_path):\n",
    "    required_files = [\n",
    "        \"fh18_u_total_HRRR_ml_output_linear\",\n",
    "        \"fh18_t2m_HRRR_ml_output_linear\",\n",
    "        \"fh18_tp_HRRR_ml_output_linear\",\n",
    "    ]\n",
    "    missing_info = {}\n",
    "\n",
    "    for dir_name in os.listdir(base_path):\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        if os.path.isdir(dir_path):  # Ensure it's a directory\n",
    "            files_in_dir = set(os.listdir(dir_path))\n",
    "            missing_files = [\n",
    "                f for f in required_files if not any(f in file for file in files_in_dir)\n",
    "            ]\n",
    "\n",
    "            if missing_files:\n",
    "                missing_info[dir_name] = missing_files\n",
    "\n",
    "    if missing_info:\n",
    "        print(\"Directories with missing files:\")\n",
    "        for directory, files in missing_info.items():\n",
    "            print(f\"{directory}: Missing {', '.join(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_files(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysms = os.listdir(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")\n",
    "len(nysms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "stations = nysm_clim[\"stid\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim[nysm_clim['climate_division_name']=='Northern Plateau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stations:\n",
    "    if not s in nysms:\n",
    "        print(\"Station not yet formulated... \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_div_filter(c):\n",
    "    nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "    df = nysm_clim[nysm_clim[\"climate_division_name\"] == c]\n",
    "    stations = df[\"stid\"].unique()\n",
    "    stations = [\"VOOR\", \"BUFF\"]\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hudson_v = clim_div_filter('Hudson Valley')\n",
    "# coast = clim_div_filter('Coastal')\n",
    "# st_ = clim_div_filter('St. Lawrence Valley')\n",
    "# greats = clim_div_filter('Great Lakes')\n",
    "# west = clim_div_filter('Western Plateau')\n",
    "# north = clim_div_filter(\"Northern Plateau\")\n",
    "# champ = clim_div_filter('Champlain Valley')\n",
    "# mohawk = clim_div_filter('Mohawk Valley')\n",
    "# central = clim_div_filter(\"Central Lakes\")\n",
    "# east = clim_div_filter('Eastern Plateau')\n",
    "\n",
    "\n",
    "# base_dir = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/2025035\"\n",
    "# dirs = os.listdir(base_dir)\n",
    "\n",
    "\n",
    "# for d in dirs:\n",
    "#     dir_path = os.path.join(base_dir, d)\n",
    "#     if d in st_ or d in greats:\n",
    "#         continue\n",
    "#     else:\n",
    "#         if os.path.isdir(dir_path):  # Ensure it's a directory before removing\n",
    "#             shutil.rmtree(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_directories(base_dir, merge_dir, clim_div):\n",
    "    \"\"\"\n",
    "    Moves files from directories in `base_dir` to corresponding directories in `merge_dir`\n",
    "    if the directory name is in `hudson_v` or `coast`.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the directory containing the source data.\n",
    "        merge_dir (str): Path to the directory where data should be moved.\n",
    "        hudson_v (list): List of valid Hudson Valley directories.\n",
    "        coast (list): List of valid Coastal directories.\n",
    "    \"\"\"\n",
    "    dirs = os.listdir(base_dir)\n",
    "\n",
    "    for d in dirs:\n",
    "        if d in clim_div:\n",
    "            src_path = os.path.join(base_dir, d)\n",
    "            dest_path = os.path.join(merge_dir, d)\n",
    "\n",
    "            if os.path.exists(src_path):\n",
    "                os.makedirs(\n",
    "                    dest_path, exist_ok=True\n",
    "                )  # Ensure destination directory exists\n",
    "\n",
    "                for file in os.listdir(src_path):\n",
    "                    src_file = os.path.join(src_path, file)\n",
    "                    dest_file = os.path.join(dest_path, file)\n",
    "\n",
    "                    if os.path.isfile(src_file):\n",
    "                        shutil.move(src_file, dest_file)  # Move the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/nam_prospectus/\"\n",
    ")\n",
    "\n",
    "bases = os.listdir(base_dir)\n",
    "\n",
    "for d in bases:\n",
    "    dir_path = os.path.join(base_dir, d)\n",
    "    if os.path.isdir(dir_path):  # Ensure it's a directory\n",
    "        files = os.listdir(dir_path)\n",
    "        for f in files:\n",
    "            if \"full\" in f and f.endswith(\n",
    "                \".parquet\"\n",
    "            ):  # Check for 'full' and '.parquet'\n",
    "                file_path = os.path.join(dir_path, f)\n",
    "                os.remove(file_path)  # Delete the matching parquet file\n",
    "                print(f\"Deleted: {file_path}\")  # Print confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250413\"\n",
    "target = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/nam_prospectus/alpha3\"\n",
    "# move_directories(base, target, champ)\n",
    "# move_directories(base, target, north)\n",
    "# move_directories(base, target, champ)\n",
    "# move_directories(base, target, mohawk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/error_visuals/Coastal/Coastal_t2m_error_metrics_master.parquet\"\n",
    ")\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oksm = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/oksm/hrrr_data/fh01/HRRR_2018_01_direct_compare_to_oksm_sites_mask_water.parquet\"\n",
    ")\n",
    "oksm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometer_df = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/radiometer_network.csv\"\n",
    ")\n",
    "radiometer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"/home/aevans/nwp_bias/src/machine_learning/data/profiler_images/2018/PROF_ALBA/PROF_ALBA_2018_010100.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.load(img).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh009/GFS_2018_08_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250107/VOOR/01_07_2025_14:58:37_full_VOOR.parquet\"\n",
    ")\n",
    "for k in df.columns:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diff\"] = df[f\"tp_VOOR\"] - df[f\"precip_total_VOOR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_filter(ldf, time1, time2):\n",
    "    ldf = ldf[ldf[\"valid_time\"] > time1]\n",
    "    ldf = ldf[ldf[\"valid_time\"] < time2]\n",
    "\n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime(2024, 1, 1, 0, 0, 0)\n",
    "time2 = datetime(2024, 1, 31, 23, 0, 0)\n",
    "\n",
    "df = date_filter(df, time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_output(df, station, fh):\n",
    "    fig, ax = plt.subplots(figsize=(24, 6))\n",
    "    x = df[\"valid_time\"]\n",
    "\n",
    "    # Convert datetime values to numerical values\n",
    "    x_numeric = mdates.date2num(x)\n",
    "\n",
    "    # Assuming your timestamps are in a datetime64 format\n",
    "    day_mask = (x.dt.hour >= 6) & (\n",
    "        x.dt.hour < 18\n",
    "    )  # Adjust the hours based on your day/night definition\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"u_total_{station}\"]),\n",
    "        c=\"mediumseagreen\",\n",
    "        linewidth=3,\n",
    "        label=\"NAM Prediction\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"wspd_sonic_mean_{station}\"]),\n",
    "        c=\"black\",\n",
    "        linewidth=1,\n",
    "        alpha=0.9,\n",
    "        label=\"NYSM Observation\",\n",
    "    )\n",
    "\n",
    "    # Fill daytime hours with white color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=day_mask, color=\"white\", alpha=0.5, label=\"Daytime\"\n",
    "    )\n",
    "\n",
    "    # Fill nighttime hours with grey color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=~day_mask, color=\"grey\", alpha=0.2, label=\"Nighttime\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"NAM Prediction v NYSM Observation: {station}: FH{fh}\", fontsize=28)\n",
    "    # plt.ylim(-5, 5.)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_output(df, \"VOOR\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "clim_div = nysm_clim[\"climate_division_name\"].unique()\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim = nysm_clim[nysm_clim[\"climate_division_name\"] == \"Hudson Valley\"]\n",
    "# nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/HRRR/s2s/Central Lakes_u_total_HRRR_lookup_quad.csv\"\n",
    ")\n",
    "# df = df[df[\"station\"] == \"ADDI\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim[nysm_clim[\"climate_division_name\"] == \"Eastern Plateau\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_fh(fh, station, var, times):\n",
    "    hrrr_df_0 = hrrr_data.read_hrrr_data(str(fh + 2).zfill(2))\n",
    "    hrrr_df_1 = hrrr_data.read_hrrr_data(str(fh + 4).zfill(2))\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[hrrr_df_0[\"station\"] == station]\n",
    "    hrrr_df_1 = hrrr_df_1[hrrr_df_1[\"station\"] == station]\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[[\"valid_time\", var]]\n",
    "    hrrr_df_1 = hrrr_df_1[[\"valid_time\", var]]\n",
    "\n",
    "    # Create a DataFrame for valid times\n",
    "    df = pd.DataFrame({\"valid_time\": times})\n",
    "    df = df.merge(hrrr_df_0, on=\"valid_time\", suffixes=(None, f\"_{station}_+2\"))\n",
    "    df = df.merge(hrrr_df_1, on=\"valid_time\", suffixes=(None, f\"_{station}_+4\"))\n",
    "    df = df.rename(columns={\"t2m\": f\"{var}_{station}_+2\"})\n",
    "    # df.fillna(-999, inplace=True)\n",
    "\n",
    "    fh2 = df[f\"{var}_{station}_+2\"].values\n",
    "    fh4 = df[f\"{var}_{station}_+4\"].values\n",
    "\n",
    "    print(len(fh2))\n",
    "    print(len(fh4))\n",
    "\n",
    "    return fh2, fh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nam_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2022\", \"2023\", \"2024\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    nam_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        for month in np.arange(1, 13):\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                print(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                nam_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "            gc.collect()\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    nam_fcast_and_error_df = pd.concat(nam_fcast_and_error)\n",
    "    nam_fcast_and_error_df = nam_fcast_and_error_df.dropna()\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return nam_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gfs_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    gfs_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        print(\"compiling\", year)\n",
    "        for month in np.arange(1, 13):\n",
    "            print(month)\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                gfs_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    gfs_fcast_and_error_df = pd.concat(gfs_fcast_and_error)\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return gfs_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nysm_data():\n",
    "    \"\"\"\n",
    "    Load and concatenate NYSM (New York State Mesonet) data from parquet files.\n",
    "\n",
    "    NYSM data is resampled at 1-hour intervals and stored in separate parquet files\n",
    "    for each year from 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        nysm_1H_obs (pd.DataFrame): A DataFrame containing concatenated NYSM data with\n",
    "        missing values filled for the 'snow_depth' column.\n",
    "\n",
    "    This function reads NYSM data from parquet files, resamples it to a 1-hour interval,\n",
    "    and concatenates the data from multiple years. Missing values in the 'snow_depth'\n",
    "    column are filled with -999, and any rows with missing values are dropped before\n",
    "    returning the resulting DataFrame.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    nysm_data = load_nysm_data()\n",
    "    print(nysm_data.head())\n",
    "    ```\n",
    "\n",
    "    Note: Ensure that the parquet files are located in the specified path before using this function.\n",
    "    \"\"\"\n",
    "    # Define the path where NYSM parquet files are stored.\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    # Initialize an empty list to store data for each year.\n",
    "    nysm_1H = []\n",
    "\n",
    "    # Loop through the years from 2018 to 2022 and read the corresponding parquet files.\n",
    "    for year in np.arange(2018, 2025):\n",
    "        df = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "        df.reset_index(inplace=True)\n",
    "        nysm_1H.append(df)\n",
    "\n",
    "    # Concatenate data from different years into a single DataFrame.\n",
    "    nysm_1H_obs = pd.concat(nysm_1H)\n",
    "\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"snow_depth\"].fillna(-999, inplace=True)\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"ta9m\"].fillna(-999, inplace=True)\n",
    "\n",
    "    # if nysm_1H_obs['ta9m'].isna().mean() > 0.8:\n",
    "    #     nysm_1H_obs.drop('ta9m', axis=1, inplace=True)\n",
    "\n",
    "    # nysm_1H_obs.dropna(inplace=True)\n",
    "\n",
    "    return nysm_1H_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_nysm_data()\n",
    "\n",
    "# df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "\n",
    "# stations_ls = ['MANH', 'VOOR', 'HERK', 'ANDE', 'BUFF', 'SCIP', 'GROV', 'LOUI', 'ESSX', 'GABR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"station\"] == \"TUPP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = read_gfs_data(\"009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/aevans/nwp_bias/src/landtype/data/first_paper_stations_coords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# gfs_df = read_gfs_data(\"006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gfs_df[\"station\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 6\n",
    "station = \"SOUT\"\n",
    "var = \"t2m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh001/NAM_2022_04_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_df = read_nam_data(str(fh).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrrr_df = hrrr_data.read_hrrr_data(str(fh).zfill(2))\n",
    "\n",
    "# # Filter NYSM data to match valid times from HRRR data\n",
    "# mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "# fh2_, fh4_ = get_more_fh(fh, station, var, mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a100_mae = [\n",
    "    0.07,\n",
    "    0.17,\n",
    "]\n",
    "a100_mse = [\n",
    "    0.07,\n",
    "    0.22,\n",
    "]\n",
    "a100_batch = [\n",
    "    1000,\n",
    "    5000,\n",
    "]\n",
    "a100_gpu = [8, 30]\n",
    "a100_runtime = [\n",
    "    timedelta(seconds=24, minutes=16, hours=0),\n",
    "    timedelta(seconds=5, minutes=16, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh200_mae = [0.06, 0.06]\n",
    "gh200_mse = [0.06, 0.07]\n",
    "gh200_batch = [1000, 10000]\n",
    "gh200_gpu = [8, 64]\n",
    "gh200_runtime = [\n",
    "    timedelta(seconds=22, minutes=6, hours=0),\n",
    "    timedelta(seconds=51, minutes=6, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def plot_runtime_bar_chart(a100_batch, a100_run_time):\n",
    "    # Convert timedelta objects to total minutes\n",
    "    run_time_minutes = [rt.total_seconds() / 60 for rt in a100_run_time]\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the bar chart\n",
    "    ax.bar(a100_batch, run_time_minutes, 1000, color=\"orange\", label=\"Run Time\")\n",
    "\n",
    "    # Adding scatter points with large X markers on top of bars\n",
    "    # ax.scatter(a100_batch, run_time_minutes, color='red', marker='x', s=100, label='Run Time Points')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"Run Time (minutes)\")\n",
    "    ax.set_title(\"Run Time by Batch Size gh200\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_metrics_bar(a100_mae, a100_mse, a100_batch):\n",
    "    # Number of bars\n",
    "    n = len(a100_mae)\n",
    "\n",
    "    # Create an array for the positions of the bars\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(n)\n",
    "\n",
    "    # Plotting the bars\n",
    "    fig, ax = plt.subplots()\n",
    "    bar1 = ax.bar(\n",
    "        index,\n",
    "        a100_mae,\n",
    "        bar_width,\n",
    "    )\n",
    "    # bar2 = ax.bar(index + bar_width, a100_mse, bar_width, label='MSE')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"GPU Memory\")\n",
    "    ax.set_ylim(0, 90)\n",
    "    ax.set_title(\"GPU Memory by Batch Size for a100\")\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(a100_batch)\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(a100_batch, a100_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(gh200_batch, gh200_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(a100_gpu, a100_mse, a100_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(gh200_gpu, gh200_mse, gh200_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "color_dict = {\n",
    "    0: \"cyan\",\n",
    "    1: \"blue\",\n",
    "    2: \"yellow\",\n",
    "    3: \"green\",\n",
    "    # 4: 'red',\n",
    "    # 5: 'orange',\n",
    "    # 6: 'purple',\n",
    "    # 7: 'black',\n",
    "    # 8: 'white'\n",
    "}\n",
    "\n",
    "\n",
    "def plurality_plot(df, geovar):\n",
    "    projPC = crs.PlateCarree()\n",
    "    latN = df[\"lat\"].max() + 1\n",
    "    latS = df[\"lat\"].min() - 1\n",
    "    lonW = df[\"lon\"].max() + 1\n",
    "    lonE = df[\"lon\"].min() - 1\n",
    "    cLat = (latN + latS) / 2\n",
    "    cLon = (lonW + lonE) / 2\n",
    "    projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6), subplot_kw={\"projection\": crs.PlateCarree()}, dpi=400\n",
    "    )\n",
    "    ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    ax.add_feature(cfeature.STATES)\n",
    "    ax.xticklabels_top = False\n",
    "    ax.ylabels_right = False\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        x=df[\"lon\"],\n",
    "        y=df[\"lat\"],\n",
    "        c=df[\"color\"],\n",
    "        s=40,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "    )\n",
    "    ax.set_title(f\"Mesonet Site {geovar} Clusters\", size=16)\n",
    "    ax.set_xlabel(\"Longitude\", size=14)\n",
    "    ax.set_ylabel(\"Latitude\", size=14)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.grid()\n",
    "\n",
    "    # Create legend patches\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=color, label=f\"Category {key}\")\n",
    "        for key, color in color_dict.items()\n",
    "    ]\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    ax.legend(\n",
    "        handles=legend_patches,\n",
    "        loc=\"upper left\",  # Use 'upper left' to anchor the legend in the figure\n",
    "        bbox_to_anchor=(1.1, 1),  # Move the legend outside the plot to the right\n",
    "        borderaxespad=0,  # Adjust the padding between the legend and the axes\n",
    "        title=\"Categories\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/lstm_clusters.csv\")\n",
    "# cluster_df[\"lon\"] = lons\n",
    "# cluster_df[\"lat\"] = lats\n",
    "# cluster_df[\"color\"] = cluster_df[\"elev_cat\"].map(color_dict)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(shapefile_path):\n",
    "    # Define a list of colors for each shape file\n",
    "    # colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # projPC = crs.PlateCarree()\n",
    "    # latN = df[\"nysm_lat\"].max() + 0.5\n",
    "    # latS = df[\"nysm_lat\"].min() - 0.5\n",
    "    # lonW = df[\"nysm_lon\"].max() + 0.5\n",
    "    # lonE = df[\"nysm_lon\"].min() - 0.5\n",
    "    # cLat = (latN + latS) / 2\n",
    "    # cLon = (lonW + lonE) / 2\n",
    "    # projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(9, 15), subplot_kw={\"projection\": crs.PlateCarree()}\n",
    "    )\n",
    "    # ax.legend()\n",
    "    # # ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    # ax.add_feature(cfeature.LAND)\n",
    "    # ax.add_feature(cfeature.COASTLINE)\n",
    "    # ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    # ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    # ax.add_feature(cfeature.STATES)\n",
    "    # ax.xticklabels_top = False\n",
    "    # ax.ylabels_right = False\n",
    "    # ax.gridlines(\n",
    "    #     crs=crs.PlateCarree(),\n",
    "    #     draw_labels=True,\n",
    "    #     linewidth=2,\n",
    "    #     color=\"black\",\n",
    "    #     alpha=0.5,\n",
    "    #     linestyle=\"--\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"],\n",
    "    #     df[\"nysm_lat\"],\n",
    "    #     c=\"blue\",\n",
    "    #     s=70,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"NYSM Sites\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"].iloc[0],\n",
    "    #     df[\"nysm_lat\"].iloc[0],\n",
    "    #     c=\"green\",\n",
    "    #     marker=\"*\",\n",
    "    #     s=400,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"Southold\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"hrrr_lon\"],\n",
    "    #     df[\"hrrr_lat\"],\n",
    "    #     c='orange',\n",
    "    #     s = 70,\n",
    "    #     edgecolors='black',\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label='HRRR'\n",
    "    # )\n",
    "\n",
    "    # # Annotate each point in NYSM\n",
    "    # for i, txt in enumerate(df[\"station\"]):\n",
    "    #     plt.annotate(\n",
    "    #         txt,\n",
    "    #         (df[\"nysm_lon\"].iloc[i], df[\"nysm_lat\"].iloc[i]),\n",
    "    #         textcoords=\"offset points\",\n",
    "    #         xytext=(5, 10),\n",
    "    #         ha=\"center\",\n",
    "    #         fontsize=18,\n",
    "    #     )\n",
    "\n",
    "    # Load the shape file using geopandas\n",
    "    climate_divisions = gpd.read_file(shapefile_path)\n",
    "    # Plot climate divisions from the shape file\n",
    "    climate_divisions.plot(\n",
    "        ax=ax,\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=\"none\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1), loc=\"upper left\", borderaxespad=0, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okdf = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/oksm.csv\")\n",
    "okdf = okdf[okdf[\"datd\"] == 20991231]\n",
    "okdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "gdf = gpd.read_file(path)\n",
    "gdf_filtered = pd.concat([gdf.iloc[191:198], gdf.iloc[172:175]])\n",
    "gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "# Create Point geometries\n",
    "geometry = [Point(xy) for xy in zip(okdf[\"elon\"], okdf[\"nlat\"])]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "points_gdf = gpd.GeoDataFrame(okdf, geometry=geometry, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"Northeast\",\n",
    "    \"West Central\",\n",
    "    \"Central\",\n",
    "    \"East Central\",\n",
    "    \"Southwest\",\n",
    "    \"South Central\",\n",
    "    \"Southeast\",\n",
    "    \"Panhandle\",\n",
    "    \"North Central\",\n",
    "    \"Nan\",\n",
    "    \"Nan1\",\n",
    "    \"Nan2\",\n",
    "    \"Nan3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "ax = fig.add_subplot(\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    projection=crs.LambertConformal(\n",
    "        central_longitude=-98.0, standard_parallels=(30, 40)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "division_patches = [\n",
    "    mpatches.Patch(\n",
    "        color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "    )\n",
    "    for i in range(len(gdf_filtered) - 1)\n",
    "]\n",
    "\n",
    "# Add the climate divisions legend\n",
    "legend1 = ax.legend(\n",
    "    handles=division_patches,\n",
    "    loc=\"upper right\",\n",
    "    title=\"Climate Divisions\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "# Set extent for the plot\n",
    "ax.set_extent([-103.0, -94.0, 33.0, 38.0], crs=crs.PlateCarree())\n",
    "# Add features\n",
    "ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "ax.gridlines(\n",
    "    crs=crs.PlateCarree(),\n",
    "    draw_labels=True,\n",
    "    linewidth=2,\n",
    "    color=\"black\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "gdf_filtered.plot(\n",
    "    ax=ax,\n",
    "    transform=crs.PlateCarree(),\n",
    "    column=\"category\",\n",
    "    cmap=\"tab10\",\n",
    "    alpha=0.3,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Annotate scatter points with station IDs\n",
    "for i, row in okdf.iterrows():\n",
    "    ax.annotate(\n",
    "        row[\"stid\"],\n",
    "        (row[\"elon\"], row[\"nlat\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 7),\n",
    "        ha=\"center\",\n",
    "        fontsize=12,\n",
    "        color=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "# Plot scatter points\n",
    "ax.scatter(\n",
    "    okdf[\"elon\"],\n",
    "    okdf[\"nlat\"],\n",
    "    c=\"black\",\n",
    "    s=250,\n",
    "    edgecolors=\"black\",\n",
    "    transform=crs.PlateCarree(),\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "# Add plot title\n",
    "plt.title(\n",
    "    f\"NCEI Oklahoma State Climate Divisions\",\n",
    "    fontsize=24,\n",
    ")\n",
    "# Load and add the logo to the lower left\n",
    "logo_img = mpimg.imread(logo)\n",
    "ax.figure.figimage(\n",
    "    logo_img, 50, 50, zorder=20, alpha=0.5\n",
    ")  # Adjust (x, y) position as needed\n",
    "\n",
    "plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/OK_state_clim_div.png\")\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"St. Lawrence Valley\",\n",
    "    \"Great Lakes\",\n",
    "    \"Northern Plateau\",\n",
    "    \"Champlain Valley\",\n",
    "    \"Hudson Valley\",\n",
    "    \"Mohawk Valley\",\n",
    "    \"Western Plateau\",\n",
    "    \"Eastern Pleateau\",\n",
    "    \"Coastal\",\n",
    "    \"Central Lakes\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clim_div = sorted(clim_div)\n",
    "image = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xCITE_gif(nysm_clim, fh, clim_div, logo):\n",
    "    # Create your dataframe df_\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define colors dictionary and randomly assign colors\n",
    "    nwp_dict = {0: \"green\", 1: \"red\", 2: \"blue\"}  # NAM  # HRRR  # GFS\n",
    "    nwps_all = [0, 1, 2]\n",
    "\n",
    "    # Randomly assign values from nwps_all to the 'lister'\n",
    "    lister = [random.choice(nwps_all) for _ in df_[\"stid\"]]\n",
    "    df_[\"color\"] = [nwp_dict[value] for value in lister]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    gdf_filtered = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in range(len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"upper right\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.5], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=\"black\",\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # # Create custom legend for NWP models\n",
    "    # nam_patch = mpatches.Patch(color=\"green\", label=\"NAM\")\n",
    "    # hrrr_patch = mpatches.Patch(color=\"red\", label=\"HRRR\")\n",
    "    # gfs_patch = mpatches.Patch(color=\"blue\", label=\"GFS\")\n",
    "\n",
    "    # # Add second legend to the plot\n",
    "    # ax.legend(\n",
    "    #     handles=[nam_patch, hrrr_patch, gfs_patch],\n",
    "    #     loc=\"upper left\",\n",
    "    #     fontsize=12,\n",
    "    #     title=\"NWP Models\",\n",
    "    # )\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"NCEI New York State Climate Divisions\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "    # Load and add the logo to the lower left\n",
    "    logo_img = mpimg.imread(logo)\n",
    "    ax.figure.figimage(\n",
    "        logo_img, 50, 50, zorder=20, alpha=0.5\n",
    "    )  # Adjust (x, y) position as needed\n",
    "\n",
    "    plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/NY_state_clim_div.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xCITE_gif(nysm_clim, 1, clim_div, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim\n",
    "nysm_ = nysm_clim.copy()\n",
    "\n",
    "nysm_ = nysm_.rename(columns={\"lat [degrees]\": \"lat\", \"lon [degrees]\": \"lon\"})\n",
    "nysm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "def get_closest_stations(nysm_df, neighbors, target_station, nwp_model):\n",
    "    # Earth's radius in kilometers\n",
    "    EARTH_RADIUS_KM = 6378\n",
    "\n",
    "    lats = nysm_df[\"lat\"].unique()\n",
    "    lons = nysm_df[\"lon\"].unique()\n",
    "\n",
    "    locations_a = pd.DataFrame()\n",
    "    locations_a[\"lat\"] = lats\n",
    "    locations_a[\"lon\"] = lons\n",
    "\n",
    "    for column in locations_a[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    locations_b = locations_a\n",
    "\n",
    "    ball = BallTree(locations_a[[\"lat_rad\", \"lon_rad\"]].values, metric=\"haversine\")\n",
    "\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = neighbors\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "\n",
    "    # Convert distances from radians to kilometers\n",
    "    distances_km = distances * EARTH_RADIUS_KM\n",
    "\n",
    "    # source info to creare a dictionary\n",
    "    indices_list = [indices[x][0:k] for x in range(len(indices))]\n",
    "    distances_list = [distances_km[x][0:k] for x in range(len(distances_km))]\n",
    "    stations = nysm_df[\"stid\"].unique()\n",
    "\n",
    "    # create dictionary\n",
    "    station_dict = {}\n",
    "    for k, _ in enumerate(stations):\n",
    "        station_dict[stations[k]] = (indices_list[k], distances_list[k])\n",
    "\n",
    "    utilize_ls = []\n",
    "    vals, dists = station_dict.get(target_station)\n",
    "\n",
    "    if nwp_model == \"GFS\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 30 and len(utilize_ls) < 5:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"NAM\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 12 and len(utilize_ls) < 4:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"HRRR\":\n",
    "        for v, d in zip(vals, dists):\n",
    "            x = stations[v]\n",
    "            utilize_ls.append(x)\n",
    "\n",
    "    return utilize_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = nysm_clim[\"stid\"].unique()\n",
    "\n",
    "elev_delta = []\n",
    "\n",
    "for s in stations:\n",
    "    print(s)\n",
    "    utilize_ls = get_closest_stations(nysm_, 15, s, \"GFS\")\n",
    "    selection = nysm_[nysm_[\"stid\"].isin(utilize_ls)]\n",
    "    # Find the maximum value in 'col1'\n",
    "    max_value = selection[\"elevation [m]\"].max()\n",
    "    min_value = selection[\"elevation [m]\"].min()\n",
    "    delta = max_value - min_value\n",
    "    elev_delta.append(delta)\n",
    "elev_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners(nysm_clim, clim_div, learners):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - fh: Forecast hour.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # # Save the figure\n",
    "    # plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/xCITE_gif/mockup_fh{fh}.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    - elev_delta: List containing the elevation delta values to determine scatter point size.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Ensure elev_delta is the same length as df_\n",
    "    if len(elev_delta) != len(df_):\n",
    "        raise ValueError(\n",
    "            \"Length of elev_delta must match the number of stations in the DataFrame\"\n",
    "        )\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points with sizes based on 'elev_delta'\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=elev_delta,  # Size of scatter points based on elev_delta\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
