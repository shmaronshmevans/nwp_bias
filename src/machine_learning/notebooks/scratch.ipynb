{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import hrrr_data, nam_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import statistics as st\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>forecast_hour</th>\n",
       "      <th>alpha</th>\n",
       "      <th>diff</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME</td>\n",
       "      <td>1</td>\n",
       "      <td>0.147350</td>\n",
       "      <td>-0.266966</td>\n",
       "      <td>0.056396</td>\n",
       "      <td>0.005049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRIS</td>\n",
       "      <td>1</td>\n",
       "      <td>0.421667</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.047532</td>\n",
       "      <td>0.003641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CARL</td>\n",
       "      <td>1</td>\n",
       "      <td>6.615451</td>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.320462</td>\n",
       "      <td>6.358754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.662573</td>\n",
       "      <td>0.156197</td>\n",
       "      <td>1.114510</td>\n",
       "      <td>2.136093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHIC</td>\n",
       "      <td>1</td>\n",
       "      <td>3.558656</td>\n",
       "      <td>0.202273</td>\n",
       "      <td>0.895991</td>\n",
       "      <td>3.418353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>MINC</td>\n",
       "      <td>5</td>\n",
       "      <td>1.844041</td>\n",
       "      <td>0.418033</td>\n",
       "      <td>1.095011</td>\n",
       "      <td>1.983605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>MRSH</td>\n",
       "      <td>5</td>\n",
       "      <td>3.904233</td>\n",
       "      <td>0.143396</td>\n",
       "      <td>0.699431</td>\n",
       "      <td>0.819332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>OILT</td>\n",
       "      <td>5</td>\n",
       "      <td>2.628160</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.089338</td>\n",
       "      <td>0.013165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>OKEM</td>\n",
       "      <td>5</td>\n",
       "      <td>2.663788</td>\n",
       "      <td>-0.002641</td>\n",
       "      <td>0.570679</td>\n",
       "      <td>3.989586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>PERK</td>\n",
       "      <td>5</td>\n",
       "      <td>2.120996</td>\n",
       "      <td>-0.008106</td>\n",
       "      <td>0.856135</td>\n",
       "      <td>1.277906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   station  forecast_hour     alpha      diff       mae       mse\n",
       "0     ACME              1  0.147350 -0.266966  0.056396  0.005049\n",
       "1     BRIS              1  0.421667  0.000989  0.047532  0.003641\n",
       "2     CARL              1  6.615451  0.001755  0.320462  6.358754\n",
       "3     CHAN              1  2.662573  0.156197  1.114510  2.136093\n",
       "4     CHIC              1  3.558656  0.202273  0.895991  3.418353\n",
       "..     ...            ...       ...       ...       ...       ...\n",
       "68    MINC              5  1.844041  0.418033  1.095011  1.983605\n",
       "69    MRSH              5  3.904233  0.143396  0.699431  0.819332\n",
       "70    OILT              5  2.628160  0.019613  0.089338  0.013165\n",
       "71    OKEM              5  2.663788 -0.002641  0.570679  3.989586\n",
       "72    PERK              5  2.120996 -0.008106  0.856135  1.277906\n",
       "\n",
       "[73 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/HRRR/s2s/Central_t2m_HRRR_lookup_linear.csv\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stid</th>\n",
       "      <th>number</th>\n",
       "      <th>name</th>\n",
       "      <th>lat [degrees]</th>\n",
       "      <th>lon [degrees]</th>\n",
       "      <th>elevation [m]</th>\n",
       "      <th>county</th>\n",
       "      <th>nearest_city</th>\n",
       "      <th>state</th>\n",
       "      <th>distance_from_town [km]</th>\n",
       "      <th>direction_from_town [degrees]</th>\n",
       "      <th>climate_division</th>\n",
       "      <th>climate_division_name</th>\n",
       "      <th>wfo</th>\n",
       "      <th>commissioned</th>\n",
       "      <th>decommissioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADDI</td>\n",
       "      <td>107</td>\n",
       "      <td>Addison</td>\n",
       "      <td>42.040360</td>\n",
       "      <td>-77.237260</td>\n",
       "      <td>507.6140</td>\n",
       "      <td>Steuben</td>\n",
       "      <td>Addison</td>\n",
       "      <td>NY</td>\n",
       "      <td>6.9</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Western Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2016-08-10 18:15:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANDE</td>\n",
       "      <td>111</td>\n",
       "      <td>Andes</td>\n",
       "      <td>42.182270</td>\n",
       "      <td>-74.801390</td>\n",
       "      <td>518.2820</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>Andes</td>\n",
       "      <td>NY</td>\n",
       "      <td>1.5</td>\n",
       "      <td>WSW</td>\n",
       "      <td>2</td>\n",
       "      <td>Eastern Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2016-08-04 15:55:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BATA</td>\n",
       "      <td>24</td>\n",
       "      <td>Batavia</td>\n",
       "      <td>43.019940</td>\n",
       "      <td>-78.135660</td>\n",
       "      <td>276.1200</td>\n",
       "      <td>Genesee</td>\n",
       "      <td>Batavia</td>\n",
       "      <td>NY</td>\n",
       "      <td>4.9</td>\n",
       "      <td>ENE</td>\n",
       "      <td>9</td>\n",
       "      <td>Great Lakes</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2016-02-18 18:40:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BEAC</td>\n",
       "      <td>76</td>\n",
       "      <td>Beacon</td>\n",
       "      <td>41.528750</td>\n",
       "      <td>-73.945270</td>\n",
       "      <td>90.1598</td>\n",
       "      <td>Dutchess</td>\n",
       "      <td>Beacon</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NE</td>\n",
       "      <td>5</td>\n",
       "      <td>Hudson Valley</td>\n",
       "      <td>ALY</td>\n",
       "      <td>2016-08-22 16:45:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BELD</td>\n",
       "      <td>90</td>\n",
       "      <td>Belden</td>\n",
       "      <td>42.223220</td>\n",
       "      <td>-75.668520</td>\n",
       "      <td>470.3700</td>\n",
       "      <td>Broome</td>\n",
       "      <td>Belden</td>\n",
       "      <td>NY</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NNE</td>\n",
       "      <td>2</td>\n",
       "      <td>Eastern Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2015-11-30 20:20:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>WFMB</td>\n",
       "      <td>14</td>\n",
       "      <td>Whiteface Mountain Base</td>\n",
       "      <td>44.393236</td>\n",
       "      <td>-73.858829</td>\n",
       "      <td>614.5990</td>\n",
       "      <td>Essex</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.5</td>\n",
       "      <td>W</td>\n",
       "      <td>3</td>\n",
       "      <td>Northern Plateau</td>\n",
       "      <td>BTV</td>\n",
       "      <td>2016-01-29 20:55:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>WGAT</td>\n",
       "      <td>123</td>\n",
       "      <td>Woodgate</td>\n",
       "      <td>43.532408</td>\n",
       "      <td>-75.158597</td>\n",
       "      <td>442.9660</td>\n",
       "      <td>Oneida</td>\n",
       "      <td>Woodgate</td>\n",
       "      <td>NY</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NNW</td>\n",
       "      <td>3</td>\n",
       "      <td>Northern Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2016-08-29 18:20:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>WHIT</td>\n",
       "      <td>10</td>\n",
       "      <td>Whitehall</td>\n",
       "      <td>43.485073</td>\n",
       "      <td>-73.423071</td>\n",
       "      <td>36.5638</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Whitehall</td>\n",
       "      <td>NY</td>\n",
       "      <td>8.0</td>\n",
       "      <td>S</td>\n",
       "      <td>7</td>\n",
       "      <td>Champlain Valley</td>\n",
       "      <td>ALY</td>\n",
       "      <td>2015-08-26 20:30:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>WOLC</td>\n",
       "      <td>79</td>\n",
       "      <td>Wolcott</td>\n",
       "      <td>43.228680</td>\n",
       "      <td>-76.842610</td>\n",
       "      <td>121.2190</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>Wolcott</td>\n",
       "      <td>NY</td>\n",
       "      <td>2.4</td>\n",
       "      <td>WNW</td>\n",
       "      <td>9</td>\n",
       "      <td>Great Lakes</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2016-03-09 18:10:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>YORK</td>\n",
       "      <td>99</td>\n",
       "      <td>York</td>\n",
       "      <td>42.855040</td>\n",
       "      <td>-77.847760</td>\n",
       "      <td>177.9420</td>\n",
       "      <td>Livingston</td>\n",
       "      <td>York</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.6</td>\n",
       "      <td>ESE</td>\n",
       "      <td>10</td>\n",
       "      <td>Central Lakes</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2016-08-09 17:55:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     stid  number                     name  lat [degrees]  lon [degrees]  \\\n",
       "0    ADDI     107                  Addison      42.040360     -77.237260   \n",
       "1    ANDE     111                    Andes      42.182270     -74.801390   \n",
       "2    BATA      24                  Batavia      43.019940     -78.135660   \n",
       "3    BEAC      76                   Beacon      41.528750     -73.945270   \n",
       "4    BELD      90                   Belden      42.223220     -75.668520   \n",
       "..    ...     ...                      ...            ...            ...   \n",
       "121  WFMB      14  Whiteface Mountain Base      44.393236     -73.858829   \n",
       "122  WGAT     123                 Woodgate      43.532408     -75.158597   \n",
       "123  WHIT      10                Whitehall      43.485073     -73.423071   \n",
       "124  WOLC      79                  Wolcott      43.228680     -76.842610   \n",
       "125  YORK      99                     York      42.855040     -77.847760   \n",
       "\n",
       "     elevation [m]      county nearest_city state  distance_from_town [km]  \\\n",
       "0         507.6140     Steuben      Addison    NY                      6.9   \n",
       "1         518.2820    Delaware        Andes    NY                      1.5   \n",
       "2         276.1200     Genesee      Batavia    NY                      4.9   \n",
       "3          90.1598    Dutchess       Beacon    NY                      3.3   \n",
       "4         470.3700      Broome       Belden    NY                      2.2   \n",
       "..             ...         ...          ...   ...                      ...   \n",
       "121       614.5990       Essex   Wilmington    NY                      3.5   \n",
       "122       442.9660      Oneida     Woodgate    NY                      1.4   \n",
       "123        36.5638  Washington    Whitehall    NY                      8.0   \n",
       "124       121.2190       Wayne      Wolcott    NY                      2.4   \n",
       "125       177.9420  Livingston         York    NY                      3.6   \n",
       "\n",
       "    direction_from_town [degrees]  climate_division climate_division_name  \\\n",
       "0                               S                 1       Western Plateau   \n",
       "1                             WSW                 2       Eastern Plateau   \n",
       "2                             ENE                 9           Great Lakes   \n",
       "3                              NE                 5         Hudson Valley   \n",
       "4                             NNE                 2       Eastern Plateau   \n",
       "..                            ...               ...                   ...   \n",
       "121                             W                 3      Northern Plateau   \n",
       "122                           NNW                 3      Northern Plateau   \n",
       "123                             S                 7      Champlain Valley   \n",
       "124                           WNW                 9           Great Lakes   \n",
       "125                           ESE                10         Central Lakes   \n",
       "\n",
       "     wfo             commissioned  decommissioned  \n",
       "0    BGM  2016-08-10 18:15:00 UTC             NaN  \n",
       "1    BGM  2016-08-04 15:55:00 UTC             NaN  \n",
       "2    BUF  2016-02-18 18:40:00 UTC             NaN  \n",
       "3    ALY  2016-08-22 16:45:00 UTC             NaN  \n",
       "4    BGM  2015-11-30 20:20:00 UTC             NaN  \n",
       "..   ...                      ...             ...  \n",
       "121  BTV  2016-01-29 20:55:00 UTC             NaN  \n",
       "122  BGM  2016-08-29 18:20:00 UTC             NaN  \n",
       "123  ALY  2015-08-26 20:30:00 UTC             NaN  \n",
       "124  BUF  2016-03-09 18:10:00 UTC             NaN  \n",
       "125  BUF  2016-08-09 17:55:00 UTC             NaN  \n",
       "\n",
       "[126 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climdf = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "climdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'station_to_climdiv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clim_div \u001b[38;5;241m=\u001b[39m station_to_climdiv\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVOOR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m clim_div\n",
      "\u001b[0;31mNameError\u001b[0m: name 'station_to_climdiv' is not defined"
     ]
    }
   ],
   "source": [
    "clim_div = station_to_climdiv.get(\"VOOR\")\n",
    "clim_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.to_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/notebooks/random_nysm_by_climdiv.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>time_1H</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elev</th>\n",
       "      <th>tair</th>\n",
       "      <th>ta9m</th>\n",
       "      <th>td</th>\n",
       "      <th>relh</th>\n",
       "      <th>srad</th>\n",
       "      <th>pres</th>\n",
       "      <th>mslp</th>\n",
       "      <th>wspd_sonic_mean</th>\n",
       "      <th>wspd_sonic</th>\n",
       "      <th>wmax_sonic</th>\n",
       "      <th>wdir_sonic</th>\n",
       "      <th>precip_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACME</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>-8.9</td>\n",
       "      <td>-18.360486</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>989.61</td>\n",
       "      <td>993.752235</td>\n",
       "      <td>5.225000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACME</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>-17.952479</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990.37</td>\n",
       "      <td>994.638920</td>\n",
       "      <td>4.383333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACME</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>-17.461899</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990.86</td>\n",
       "      <td>995.174432</td>\n",
       "      <td>3.758333</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACME</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>-16.912197</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>991.58</td>\n",
       "      <td>995.923500</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACME</td>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>-16.877994</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>992.63</td>\n",
       "      <td>997.065904</td>\n",
       "      <td>3.766667</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station             time_1H       lat       lon   elev  tair  ta9m  \\\n",
       "0    ACME 2018-01-01 01:00:00  34.80833 -98.02325  397.0  -9.1  -8.9   \n",
       "1    ACME 2018-01-01 02:00:00  34.80833 -98.02325  397.0  -9.7  -9.5   \n",
       "2    ACME 2018-01-01 03:00:00  34.80833 -98.02325  397.0  -9.9  -9.7   \n",
       "3    ACME 2018-01-01 04:00:00  34.80833 -98.02325  397.0 -10.0  -9.9   \n",
       "4    ACME 2018-01-01 05:00:00  34.80833 -98.02325  397.0 -10.4 -10.2   \n",
       "\n",
       "          td  relh  srad    pres        mslp  wspd_sonic_mean  wspd_sonic  \\\n",
       "0 -18.360486  47.0   0.0  989.61  993.752235         5.225000         6.0   \n",
       "1 -17.952479  51.0   0.0  990.37  994.638920         4.383333         5.0   \n",
       "2 -17.461899  54.0   0.0  990.86  995.174432         3.758333         3.8   \n",
       "3 -16.912197  57.0   0.0  991.58  995.923500         3.333333         3.4   \n",
       "4 -16.877994  59.0   0.0  992.63  997.065904         3.766667         3.6   \n",
       "\n",
       "   wmax_sonic  wdir_sonic  precip_total  \n",
       "0         7.4         4.0           0.0  \n",
       "1         6.9         7.0           0.0  \n",
       "2         5.0         9.0           0.0  \n",
       "3         4.9         7.0           0.0  \n",
       "4         4.9        15.0           0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filt = df[df[\"station\"] == \"ACME\"]\n",
    "# plt.plot(df_filt['time_1H'], df_filt['precip_total'])\n",
    "df_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your directory of parquet files\n",
    "path = \"/home/aevans/nwp_bias/src/landtype/NY_cartopy/oksm_v3/\"\n",
    "input_dir = os.listdir(path)\n",
    "\n",
    "# Load and concatenate all Parquet files\n",
    "for file in input_dir:\n",
    "    print(file)\n",
    "    df = pd.read_parquet(f\"{path}{file}\").reset_index()\n",
    "    # Ensure 'time' column is datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    # Format: 'YYYY-MM-DD HH:MM:00'\n",
    "    df[\"time\"] = df[\"time\"].apply(lambda t: f\"{t:%Y-%m-%d} {t:%H}:{t:%M}:00\")\n",
    "    df.set_index([\"station\", \"time\"]).sort_index()\n",
    "    df.to_parquet(f\"{path}{file}\")\n",
    "    print(file, \"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elev</th>\n",
       "      <th>tair</th>\n",
       "      <th>ta9m</th>\n",
       "      <th>td</th>\n",
       "      <th>relh</th>\n",
       "      <th>srad</th>\n",
       "      <th>pres</th>\n",
       "      <th>mslp</th>\n",
       "      <th>wspd_sonic_mean</th>\n",
       "      <th>wspd_sonic</th>\n",
       "      <th>wmax_sonic</th>\n",
       "      <th>wdir_sonic</th>\n",
       "      <th>precip_total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th>time_1H</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ACME</th>\n",
       "      <th>2018-01-01 01:00:00</th>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>-8.9</td>\n",
       "      <td>-18.360486</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>989.61</td>\n",
       "      <td>993.752235</td>\n",
       "      <td>5.225000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 02:00:00</th>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>-17.952479</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990.37</td>\n",
       "      <td>994.638920</td>\n",
       "      <td>4.383333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 03:00:00</th>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>-9.7</td>\n",
       "      <td>-17.461899</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990.86</td>\n",
       "      <td>995.174432</td>\n",
       "      <td>3.758333</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 04:00:00</th>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>-16.912197</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>991.58</td>\n",
       "      <td>995.923500</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 05:00:00</th>\n",
       "      <td>34.80833</td>\n",
       "      <td>-98.02325</td>\n",
       "      <td>397.0</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>-16.877994</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>992.63</td>\n",
       "      <td>997.065904</td>\n",
       "      <td>3.766667</td>\n",
       "      <td>3.6</td>\n",
       "      <td>4.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">YUKO</th>\n",
       "      <th>2018-12-31 19:00:00</th>\n",
       "      <td>35.55671</td>\n",
       "      <td>-97.75538</td>\n",
       "      <td>407.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.168746</td>\n",
       "      <td>84.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>958.52</td>\n",
       "      <td>959.304526</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 20:00:00</th>\n",
       "      <td>35.55671</td>\n",
       "      <td>-97.75538</td>\n",
       "      <td>407.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>8.3</td>\n",
       "      <td>4.517611</td>\n",
       "      <td>73.0</td>\n",
       "      <td>553.0</td>\n",
       "      <td>959.55</td>\n",
       "      <td>960.108051</td>\n",
       "      <td>2.808333</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.3</td>\n",
       "      <td>277.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 21:00:00</th>\n",
       "      <td>35.55671</td>\n",
       "      <td>-97.75538</td>\n",
       "      <td>407.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4.251977</td>\n",
       "      <td>67.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>960.73</td>\n",
       "      <td>961.131660</td>\n",
       "      <td>3.441667</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 22:00:00</th>\n",
       "      <td>35.55671</td>\n",
       "      <td>-97.75538</td>\n",
       "      <td>407.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>3.392936</td>\n",
       "      <td>67.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>963.19</td>\n",
       "      <td>963.766384</td>\n",
       "      <td>4.575000</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>326.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:00:00</th>\n",
       "      <td>35.55671</td>\n",
       "      <td>-97.75538</td>\n",
       "      <td>407.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.246287</td>\n",
       "      <td>79.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>964.92</td>\n",
       "      <td>966.129657</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>9.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1046098 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  lat       lon   elev  tair  ta9m         td  \\\n",
       "station time_1H                                                                 \n",
       "ACME    2018-01-01 01:00:00  34.80833 -98.02325  397.0  -9.1  -8.9 -18.360486   \n",
       "        2018-01-01 02:00:00  34.80833 -98.02325  397.0  -9.7  -9.5 -17.952479   \n",
       "        2018-01-01 03:00:00  34.80833 -98.02325  397.0  -9.9  -9.7 -17.461899   \n",
       "        2018-01-01 04:00:00  34.80833 -98.02325  397.0 -10.0  -9.9 -16.912197   \n",
       "        2018-01-01 05:00:00  34.80833 -98.02325  397.0 -10.4 -10.2 -16.877994   \n",
       "...                               ...       ...    ...   ...   ...        ...   \n",
       "YUKO    2018-12-31 19:00:00  35.55671 -97.75538  407.0   7.7   7.1   5.168746   \n",
       "        2018-12-31 20:00:00  35.55671 -97.75538  407.0   9.1   8.3   4.517611   \n",
       "        2018-12-31 21:00:00  35.55671 -97.75538  407.0  10.1   9.6   4.251977   \n",
       "        2018-12-31 22:00:00  35.55671 -97.75538  407.0   9.2   9.3   3.392936   \n",
       "        2018-12-31 23:00:00  35.55671 -97.75538  407.0   5.6   5.5   2.246287   \n",
       "\n",
       "                             relh   srad    pres        mslp  wspd_sonic_mean  \\\n",
       "station time_1H                                                                 \n",
       "ACME    2018-01-01 01:00:00  47.0    0.0  989.61  993.752235         5.225000   \n",
       "        2018-01-01 02:00:00  51.0    0.0  990.37  994.638920         4.383333   \n",
       "        2018-01-01 03:00:00  54.0    0.0  990.86  995.174432         3.758333   \n",
       "        2018-01-01 04:00:00  57.0    0.0  991.58  995.923500         3.333333   \n",
       "        2018-01-01 05:00:00  59.0    0.0  992.63  997.065904         3.766667   \n",
       "...                           ...    ...     ...         ...              ...   \n",
       "YUKO    2018-12-31 19:00:00  84.0  313.0  958.52  959.304526         3.700000   \n",
       "        2018-12-31 20:00:00  73.0  553.0  959.55  960.108051         2.808333   \n",
       "        2018-12-31 21:00:00  67.0  358.0  960.73  961.131660         3.441667   \n",
       "        2018-12-31 22:00:00  67.0   48.0  963.19  963.766384         4.575000   \n",
       "        2018-12-31 23:00:00  79.0   24.0  964.92  966.129657         8.066667   \n",
       "\n",
       "                             wspd_sonic  wmax_sonic  wdir_sonic  precip_total  \n",
       "station time_1H                                                                \n",
       "ACME    2018-01-01 01:00:00         6.0         7.4         4.0           0.0  \n",
       "        2018-01-01 02:00:00         5.0         6.9         7.0           0.0  \n",
       "        2018-01-01 03:00:00         3.8         5.0         9.0           0.0  \n",
       "        2018-01-01 04:00:00         3.4         4.9         7.0           0.0  \n",
       "        2018-01-01 05:00:00         3.6         4.9        15.0           0.0  \n",
       "...                                 ...         ...         ...           ...  \n",
       "YUKO    2018-12-31 19:00:00         3.1         4.4       229.0           0.0  \n",
       "        2018-12-31 20:00:00         4.4         6.3       277.0           0.0  \n",
       "        2018-12-31 21:00:00         3.3         4.8       282.0           0.0  \n",
       "        2018-12-31 22:00:00         3.7         4.7       326.0           0.0  \n",
       "        2018-12-31 23:00:00         9.6        13.0       339.0           0.0  \n",
       "\n",
       "[1046098 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"/home/aevans/nwp_bias/data/oksm/oksm_1H_obs_2018.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    print(df[c].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df[\"station\"].unique():\n",
    "    df_filt = df[df[\"station\"] == s]\n",
    "    for c in df_filt:\n",
    "        if c == \"station\" or c == \"time_1H\":\n",
    "            continue\n",
    "        else:\n",
    "            print(c)\n",
    "            print(\"MAX\", df_filt[c].max())\n",
    "            print(\"MIN\", df_filt[c].min())\n",
    "            print(\"MEAN\", st.mean(df_filt[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/home/aevans/nwp_bias/src/landtype/NY_cartopy/\"\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "for d in files:\n",
    "    if d.endswith(\".parquet\") or d.endswith(\".mts\"):\n",
    "        os.remove(os.path.join(directory_path, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"/home/aevans/nwp_bias/src/landtype/NY_cartopy/\"\n",
    "\n",
    "# List all .mts files\n",
    "mts_files = [f for f in os.listdir(directory) if f.endswith(\".mts\")]\n",
    "\n",
    "for mts_file in mts_files:\n",
    "    mts_path = os.path.join(directory, mts_file)\n",
    "    try:\n",
    "        # Step 1: Read the .mts file — customize if needed\n",
    "        with open(mts_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Optional: Parse lines into a list of lists\n",
    "        data = [line.strip().split() for line in lines]\n",
    "\n",
    "        # Optional: Create column names or infer them\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = [f\"col{i}\" for i in range(df.shape[1])]\n",
    "\n",
    "        # Step 2: Save to .parquet\n",
    "        parquet_path = os.path.join(directory, mts_file.replace(\".mts\", \".parquet\"))\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "        print(f\"✅ Converted {mts_file} → {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to convert {mts_file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okdf = pd.read_parquet(\"/home/aevans/nwp_bias/data/oksm/oksm_1H_obs_2018.parquet\")\n",
    "okdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_files(base_path):\n",
    "    required_files = [\n",
    "        \"fh18_u_total_HRRR_ml_output_linear\",\n",
    "        \"fh18_t2m_HRRR_ml_output_linear\",\n",
    "        \"fh18_tp_HRRR_ml_output_linear\",\n",
    "    ]\n",
    "    missing_info = {}\n",
    "\n",
    "    for dir_name in os.listdir(base_path):\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        if os.path.isdir(dir_path):  # Ensure it's a directory\n",
    "            files_in_dir = set(os.listdir(dir_path))\n",
    "            missing_files = [\n",
    "                f for f in required_files if not any(f in file for file in files_in_dir)\n",
    "            ]\n",
    "\n",
    "            if missing_files:\n",
    "                missing_info[dir_name] = missing_files\n",
    "\n",
    "    if missing_info:\n",
    "        print(\"Directories with missing files:\")\n",
    "        for directory, files in missing_info.items():\n",
    "            print(f\"{directory}: Missing {', '.join(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_files(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysms = os.listdir(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")\n",
    "len(nysms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "stations = nysm_clim[\"stid\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim[nysm_clim['climate_division_name']=='Northern Plateau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stations:\n",
    "    if not s in nysms:\n",
    "        print(\"Station not yet formulated... \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_div_filter(c):\n",
    "    nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "    df = nysm_clim[nysm_clim[\"climate_division_name\"] == c]\n",
    "    stations = df[\"stid\"].unique()\n",
    "    stations = [\"VOOR\", \"BUFF\"]\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hudson_v = clim_div_filter('Hudson Valley')\n",
    "# coast = clim_div_filter('Coastal')\n",
    "# st_ = clim_div_filter('St. Lawrence Valley')\n",
    "# greats = clim_div_filter('Great Lakes')\n",
    "# west = clim_div_filter('Western Plateau')\n",
    "# north = clim_div_filter(\"Northern Plateau\")\n",
    "# champ = clim_div_filter('Champlain Valley')\n",
    "# mohawk = clim_div_filter('Mohawk Valley')\n",
    "# central = clim_div_filter(\"Central Lakes\")\n",
    "# east = clim_div_filter('Eastern Plateau')\n",
    "\n",
    "\n",
    "# base_dir = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/2025035\"\n",
    "# dirs = os.listdir(base_dir)\n",
    "\n",
    "\n",
    "# for d in dirs:\n",
    "#     dir_path = os.path.join(base_dir, d)\n",
    "#     if d in st_ or d in greats:\n",
    "#         continue\n",
    "#     else:\n",
    "#         if os.path.isdir(dir_path):  # Ensure it's a directory before removing\n",
    "#             shutil.rmtree(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_directories(base_dir, merge_dir, clim_div):\n",
    "    \"\"\"\n",
    "    Moves files from directories in `base_dir` to corresponding directories in `merge_dir`\n",
    "    if the directory name is in `hudson_v` or `coast`.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the directory containing the source data.\n",
    "        merge_dir (str): Path to the directory where data should be moved.\n",
    "        hudson_v (list): List of valid Hudson Valley directories.\n",
    "        coast (list): List of valid Coastal directories.\n",
    "    \"\"\"\n",
    "    dirs = os.listdir(base_dir)\n",
    "\n",
    "    for d in dirs:\n",
    "        if d in clim_div:\n",
    "            src_path = os.path.join(base_dir, d)\n",
    "            dest_path = os.path.join(merge_dir, d)\n",
    "\n",
    "            if os.path.exists(src_path):\n",
    "                os.makedirs(\n",
    "                    dest_path, exist_ok=True\n",
    "                )  # Ensure destination directory exists\n",
    "\n",
    "                for file in os.listdir(src_path):\n",
    "                    src_file = os.path.join(src_path, file)\n",
    "                    dest_file = os.path.join(dest_path, file)\n",
    "\n",
    "                    if os.path.isfile(src_file):\n",
    "                        shutil.move(src_file, dest_file)  # Move the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/nam_prospectus/\"\n",
    ")\n",
    "\n",
    "bases = os.listdir(base_dir)\n",
    "\n",
    "for d in bases:\n",
    "    dir_path = os.path.join(base_dir, d)\n",
    "    if os.path.isdir(dir_path):  # Ensure it's a directory\n",
    "        files = os.listdir(dir_path)\n",
    "        for f in files:\n",
    "            if \"full\" in f and f.endswith(\n",
    "                \".parquet\"\n",
    "            ):  # Check for 'full' and '.parquet'\n",
    "                file_path = os.path.join(dir_path, f)\n",
    "                os.remove(file_path)  # Delete the matching parquet file\n",
    "                print(f\"Deleted: {file_path}\")  # Print confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250413\"\n",
    "target = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/nam_prospectus/alpha3\"\n",
    "# move_directories(base, target, champ)\n",
    "# move_directories(base, target, north)\n",
    "# move_directories(base, target, champ)\n",
    "# move_directories(base, target, mohawk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/error_visuals/Coastal/Coastal_t2m_error_metrics_master.parquet\"\n",
    ")\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oksm = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/oksm/hrrr_data/fh01/HRRR_2018_01_direct_compare_to_oksm_sites_mask_water.parquet\"\n",
    ")\n",
    "oksm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometer_df = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/radiometer_network.csv\"\n",
    ")\n",
    "radiometer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"/home/aevans/nwp_bias/src/machine_learning/data/profiler_images/2018/PROF_ALBA/PROF_ALBA_2018_010100.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.load(img).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh009/GFS_2018_08_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250107/VOOR/01_07_2025_14:58:37_full_VOOR.parquet\"\n",
    ")\n",
    "for k in df.columns:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diff\"] = df[f\"tp_VOOR\"] - df[f\"precip_total_VOOR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_filter(ldf, time1, time2):\n",
    "    ldf = ldf[ldf[\"valid_time\"] > time1]\n",
    "    ldf = ldf[ldf[\"valid_time\"] < time2]\n",
    "\n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime(2024, 1, 1, 0, 0, 0)\n",
    "time2 = datetime(2024, 1, 31, 23, 0, 0)\n",
    "\n",
    "df = date_filter(df, time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_output(df, station, fh):\n",
    "    fig, ax = plt.subplots(figsize=(24, 6))\n",
    "    x = df[\"valid_time\"]\n",
    "\n",
    "    # Convert datetime values to numerical values\n",
    "    x_numeric = mdates.date2num(x)\n",
    "\n",
    "    # Assuming your timestamps are in a datetime64 format\n",
    "    day_mask = (x.dt.hour >= 6) & (\n",
    "        x.dt.hour < 18\n",
    "    )  # Adjust the hours based on your day/night definition\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"u_total_{station}\"]),\n",
    "        c=\"mediumseagreen\",\n",
    "        linewidth=3,\n",
    "        label=\"NAM Prediction\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"wspd_sonic_mean_{station}\"]),\n",
    "        c=\"black\",\n",
    "        linewidth=1,\n",
    "        alpha=0.9,\n",
    "        label=\"NYSM Observation\",\n",
    "    )\n",
    "\n",
    "    # Fill daytime hours with white color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=day_mask, color=\"white\", alpha=0.5, label=\"Daytime\"\n",
    "    )\n",
    "\n",
    "    # Fill nighttime hours with grey color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=~day_mask, color=\"grey\", alpha=0.2, label=\"Nighttime\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"NAM Prediction v NYSM Observation: {station}: FH{fh}\", fontsize=28)\n",
    "    # plt.ylim(-5, 5.)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_output(df, \"VOOR\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "clim_div = nysm_clim[\"climate_division_name\"].unique()\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim = nysm_clim[nysm_clim[\"climate_division_name\"] == \"Hudson Valley\"]\n",
    "# nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/HRRR/s2s/Central Lakes_u_total_HRRR_lookup_quad.csv\"\n",
    ")\n",
    "# df = df[df[\"station\"] == \"ADDI\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim[nysm_clim[\"climate_division_name\"] == \"Eastern Plateau\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_fh(fh, station, var, times):\n",
    "    hrrr_df_0 = hrrr_data.read_hrrr_data(str(fh + 2).zfill(2))\n",
    "    hrrr_df_1 = hrrr_data.read_hrrr_data(str(fh + 4).zfill(2))\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[hrrr_df_0[\"station\"] == station]\n",
    "    hrrr_df_1 = hrrr_df_1[hrrr_df_1[\"station\"] == station]\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[[\"valid_time\", var]]\n",
    "    hrrr_df_1 = hrrr_df_1[[\"valid_time\", var]]\n",
    "\n",
    "    # Create a DataFrame for valid times\n",
    "    df = pd.DataFrame({\"valid_time\": times})\n",
    "    df = df.merge(hrrr_df_0, on=\"valid_time\", suffixes=(None, f\"_{station}_+2\"))\n",
    "    df = df.merge(hrrr_df_1, on=\"valid_time\", suffixes=(None, f\"_{station}_+4\"))\n",
    "    df = df.rename(columns={\"t2m\": f\"{var}_{station}_+2\"})\n",
    "    # df.fillna(-999, inplace=True)\n",
    "\n",
    "    fh2 = df[f\"{var}_{station}_+2\"].values\n",
    "    fh4 = df[f\"{var}_{station}_+4\"].values\n",
    "\n",
    "    print(len(fh2))\n",
    "    print(len(fh4))\n",
    "\n",
    "    return fh2, fh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nam_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2022\", \"2023\", \"2024\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    nam_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        for month in np.arange(1, 13):\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                print(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                nam_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "            gc.collect()\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    nam_fcast_and_error_df = pd.concat(nam_fcast_and_error)\n",
    "    nam_fcast_and_error_df = nam_fcast_and_error_df.dropna()\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return nam_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gfs_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    gfs_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        print(\"compiling\", year)\n",
    "        for month in np.arange(1, 13):\n",
    "            print(month)\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                gfs_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    gfs_fcast_and_error_df = pd.concat(gfs_fcast_and_error)\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return gfs_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nysm_data():\n",
    "    \"\"\"\n",
    "    Load and concatenate NYSM (New York State Mesonet) data from parquet files.\n",
    "\n",
    "    NYSM data is resampled at 1-hour intervals and stored in separate parquet files\n",
    "    for each year from 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        nysm_1H_obs (pd.DataFrame): A DataFrame containing concatenated NYSM data with\n",
    "        missing values filled for the 'snow_depth' column.\n",
    "\n",
    "    This function reads NYSM data from parquet files, resamples it to a 1-hour interval,\n",
    "    and concatenates the data from multiple years. Missing values in the 'snow_depth'\n",
    "    column are filled with -999, and any rows with missing values are dropped before\n",
    "    returning the resulting DataFrame.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    nysm_data = load_nysm_data()\n",
    "    print(nysm_data.head())\n",
    "    ```\n",
    "\n",
    "    Note: Ensure that the parquet files are located in the specified path before using this function.\n",
    "    \"\"\"\n",
    "    # Define the path where NYSM parquet files are stored.\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    # Initialize an empty list to store data for each year.\n",
    "    nysm_1H = []\n",
    "\n",
    "    # Loop through the years from 2018 to 2022 and read the corresponding parquet files.\n",
    "    for year in np.arange(2018, 2025):\n",
    "        df = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "        df.reset_index(inplace=True)\n",
    "        nysm_1H.append(df)\n",
    "\n",
    "    # Concatenate data from different years into a single DataFrame.\n",
    "    nysm_1H_obs = pd.concat(nysm_1H)\n",
    "\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"snow_depth\"].fillna(-999, inplace=True)\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"ta9m\"].fillna(-999, inplace=True)\n",
    "\n",
    "    # if nysm_1H_obs['ta9m'].isna().mean() > 0.8:\n",
    "    #     nysm_1H_obs.drop('ta9m', axis=1, inplace=True)\n",
    "\n",
    "    # nysm_1H_obs.dropna(inplace=True)\n",
    "\n",
    "    return nysm_1H_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_nysm_data()\n",
    "\n",
    "# df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "\n",
    "# stations_ls = ['MANH', 'VOOR', 'HERK', 'ANDE', 'BUFF', 'SCIP', 'GROV', 'LOUI', 'ESSX', 'GABR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"station\"] == \"TUPP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = read_gfs_data(\"009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/aevans/nwp_bias/src/landtype/data/first_paper_stations_coords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# gfs_df = read_gfs_data(\"006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gfs_df[\"station\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 6\n",
    "station = \"SOUT\"\n",
    "var = \"t2m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh001/NAM_2022_04_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_df = read_nam_data(str(fh).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrrr_df = hrrr_data.read_hrrr_data(str(fh).zfill(2))\n",
    "\n",
    "# # Filter NYSM data to match valid times from HRRR data\n",
    "# mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "# fh2_, fh4_ = get_more_fh(fh, station, var, mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a100_mae = [\n",
    "    0.07,\n",
    "    0.17,\n",
    "]\n",
    "a100_mse = [\n",
    "    0.07,\n",
    "    0.22,\n",
    "]\n",
    "a100_batch = [\n",
    "    1000,\n",
    "    5000,\n",
    "]\n",
    "a100_gpu = [8, 30]\n",
    "a100_runtime = [\n",
    "    timedelta(seconds=24, minutes=16, hours=0),\n",
    "    timedelta(seconds=5, minutes=16, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh200_mae = [0.06, 0.06]\n",
    "gh200_mse = [0.06, 0.07]\n",
    "gh200_batch = [1000, 10000]\n",
    "gh200_gpu = [8, 64]\n",
    "gh200_runtime = [\n",
    "    timedelta(seconds=22, minutes=6, hours=0),\n",
    "    timedelta(seconds=51, minutes=6, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def plot_runtime_bar_chart(a100_batch, a100_run_time):\n",
    "    # Convert timedelta objects to total minutes\n",
    "    run_time_minutes = [rt.total_seconds() / 60 for rt in a100_run_time]\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the bar chart\n",
    "    ax.bar(a100_batch, run_time_minutes, 1000, color=\"orange\", label=\"Run Time\")\n",
    "\n",
    "    # Adding scatter points with large X markers on top of bars\n",
    "    # ax.scatter(a100_batch, run_time_minutes, color='red', marker='x', s=100, label='Run Time Points')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"Run Time (minutes)\")\n",
    "    ax.set_title(\"Run Time by Batch Size gh200\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_metrics_bar(a100_mae, a100_mse, a100_batch):\n",
    "    # Number of bars\n",
    "    n = len(a100_mae)\n",
    "\n",
    "    # Create an array for the positions of the bars\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(n)\n",
    "\n",
    "    # Plotting the bars\n",
    "    fig, ax = plt.subplots()\n",
    "    bar1 = ax.bar(\n",
    "        index,\n",
    "        a100_mae,\n",
    "        bar_width,\n",
    "    )\n",
    "    # bar2 = ax.bar(index + bar_width, a100_mse, bar_width, label='MSE')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"GPU Memory\")\n",
    "    ax.set_ylim(0, 90)\n",
    "    ax.set_title(\"GPU Memory by Batch Size for a100\")\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(a100_batch)\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(a100_batch, a100_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(gh200_batch, gh200_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(a100_gpu, a100_mse, a100_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(gh200_gpu, gh200_mse, gh200_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "color_dict = {\n",
    "    0: \"cyan\",\n",
    "    1: \"blue\",\n",
    "    2: \"yellow\",\n",
    "    3: \"green\",\n",
    "    # 4: 'red',\n",
    "    # 5: 'orange',\n",
    "    # 6: 'purple',\n",
    "    # 7: 'black',\n",
    "    # 8: 'white'\n",
    "}\n",
    "\n",
    "\n",
    "def plurality_plot(df, geovar):\n",
    "    projPC = crs.PlateCarree()\n",
    "    latN = df[\"lat\"].max() + 1\n",
    "    latS = df[\"lat\"].min() - 1\n",
    "    lonW = df[\"lon\"].max() + 1\n",
    "    lonE = df[\"lon\"].min() - 1\n",
    "    cLat = (latN + latS) / 2\n",
    "    cLon = (lonW + lonE) / 2\n",
    "    projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6), subplot_kw={\"projection\": crs.PlateCarree()}, dpi=400\n",
    "    )\n",
    "    ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    ax.add_feature(cfeature.STATES)\n",
    "    ax.xticklabels_top = False\n",
    "    ax.ylabels_right = False\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        x=df[\"lon\"],\n",
    "        y=df[\"lat\"],\n",
    "        c=df[\"color\"],\n",
    "        s=40,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "    )\n",
    "    ax.set_title(f\"Mesonet Site {geovar} Clusters\", size=16)\n",
    "    ax.set_xlabel(\"Longitude\", size=14)\n",
    "    ax.set_ylabel(\"Latitude\", size=14)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.grid()\n",
    "\n",
    "    # Create legend patches\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=color, label=f\"Category {key}\")\n",
    "        for key, color in color_dict.items()\n",
    "    ]\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    ax.legend(\n",
    "        handles=legend_patches,\n",
    "        loc=\"upper left\",  # Use 'upper left' to anchor the legend in the figure\n",
    "        bbox_to_anchor=(1.1, 1),  # Move the legend outside the plot to the right\n",
    "        borderaxespad=0,  # Adjust the padding between the legend and the axes\n",
    "        title=\"Categories\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/lstm_clusters.csv\")\n",
    "# cluster_df[\"lon\"] = lons\n",
    "# cluster_df[\"lat\"] = lats\n",
    "# cluster_df[\"color\"] = cluster_df[\"elev_cat\"].map(color_dict)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(shapefile_path):\n",
    "    # Define a list of colors for each shape file\n",
    "    # colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # projPC = crs.PlateCarree()\n",
    "    # latN = df[\"nysm_lat\"].max() + 0.5\n",
    "    # latS = df[\"nysm_lat\"].min() - 0.5\n",
    "    # lonW = df[\"nysm_lon\"].max() + 0.5\n",
    "    # lonE = df[\"nysm_lon\"].min() - 0.5\n",
    "    # cLat = (latN + latS) / 2\n",
    "    # cLon = (lonW + lonE) / 2\n",
    "    # projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(9, 15), subplot_kw={\"projection\": crs.PlateCarree()}\n",
    "    )\n",
    "    # ax.legend()\n",
    "    # # ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    # ax.add_feature(cfeature.LAND)\n",
    "    # ax.add_feature(cfeature.COASTLINE)\n",
    "    # ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    # ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    # ax.add_feature(cfeature.STATES)\n",
    "    # ax.xticklabels_top = False\n",
    "    # ax.ylabels_right = False\n",
    "    # ax.gridlines(\n",
    "    #     crs=crs.PlateCarree(),\n",
    "    #     draw_labels=True,\n",
    "    #     linewidth=2,\n",
    "    #     color=\"black\",\n",
    "    #     alpha=0.5,\n",
    "    #     linestyle=\"--\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"],\n",
    "    #     df[\"nysm_lat\"],\n",
    "    #     c=\"blue\",\n",
    "    #     s=70,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"NYSM Sites\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"].iloc[0],\n",
    "    #     df[\"nysm_lat\"].iloc[0],\n",
    "    #     c=\"green\",\n",
    "    #     marker=\"*\",\n",
    "    #     s=400,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"Southold\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"hrrr_lon\"],\n",
    "    #     df[\"hrrr_lat\"],\n",
    "    #     c='orange',\n",
    "    #     s = 70,\n",
    "    #     edgecolors='black',\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label='HRRR'\n",
    "    # )\n",
    "\n",
    "    # # Annotate each point in NYSM\n",
    "    # for i, txt in enumerate(df[\"station\"]):\n",
    "    #     plt.annotate(\n",
    "    #         txt,\n",
    "    #         (df[\"nysm_lon\"].iloc[i], df[\"nysm_lat\"].iloc[i]),\n",
    "    #         textcoords=\"offset points\",\n",
    "    #         xytext=(5, 10),\n",
    "    #         ha=\"center\",\n",
    "    #         fontsize=18,\n",
    "    #     )\n",
    "\n",
    "    # Load the shape file using geopandas\n",
    "    climate_divisions = gpd.read_file(shapefile_path)\n",
    "    # Plot climate divisions from the shape file\n",
    "    climate_divisions.plot(\n",
    "        ax=ax,\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=\"none\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1), loc=\"upper left\", borderaxespad=0, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okdf = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/oksm.csv\")\n",
    "okdf = okdf[okdf[\"datd\"] == 20991231]\n",
    "okdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "gdf = gpd.read_file(path)\n",
    "gdf_filtered = pd.concat([gdf.iloc[191:198], gdf.iloc[172:175]])\n",
    "gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "# Create Point geometries\n",
    "geometry = [Point(xy) for xy in zip(okdf[\"elon\"], okdf[\"nlat\"])]\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "points_gdf = gpd.GeoDataFrame(okdf, geometry=geometry, crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"Northeast\",\n",
    "    \"West Central\",\n",
    "    \"Central\",\n",
    "    \"East Central\",\n",
    "    \"Southwest\",\n",
    "    \"South Central\",\n",
    "    \"Southeast\",\n",
    "    \"Panhandle\",\n",
    "    \"North Central\",\n",
    "    \"Nan\",\n",
    "    \"Nan1\",\n",
    "    \"Nan2\",\n",
    "    \"Nan3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig = plt.figure(figsize=(24, 16))\n",
    "ax = fig.add_subplot(\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    projection=crs.LambertConformal(\n",
    "        central_longitude=-98.0, standard_parallels=(30, 40)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "division_patches = [\n",
    "    mpatches.Patch(\n",
    "        color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "    )\n",
    "    for i in range(len(gdf_filtered) - 1)\n",
    "]\n",
    "\n",
    "# Add the climate divisions legend\n",
    "legend1 = ax.legend(\n",
    "    handles=division_patches,\n",
    "    loc=\"upper right\",\n",
    "    title=\"Climate Divisions\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "# Set extent for the plot\n",
    "ax.set_extent([-103.0, -94.0, 33.0, 38.0], crs=crs.PlateCarree())\n",
    "# Add features\n",
    "ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "ax.gridlines(\n",
    "    crs=crs.PlateCarree(),\n",
    "    draw_labels=True,\n",
    "    linewidth=2,\n",
    "    color=\"black\",\n",
    "    alpha=0.5,\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "gdf_filtered.plot(\n",
    "    ax=ax,\n",
    "    transform=crs.PlateCarree(),\n",
    "    column=\"category\",\n",
    "    cmap=\"tab10\",\n",
    "    alpha=0.3,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "# Annotate scatter points with station IDs\n",
    "for i, row in okdf.iterrows():\n",
    "    ax.annotate(\n",
    "        row[\"stid\"],\n",
    "        (row[\"elon\"], row[\"nlat\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 7),\n",
    "        ha=\"center\",\n",
    "        fontsize=12,\n",
    "        color=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "    )\n",
    "\n",
    "# Plot scatter points\n",
    "ax.scatter(\n",
    "    okdf[\"elon\"],\n",
    "    okdf[\"nlat\"],\n",
    "    c=\"black\",\n",
    "    s=250,\n",
    "    edgecolors=\"black\",\n",
    "    transform=crs.PlateCarree(),\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "# Add plot title\n",
    "plt.title(\n",
    "    f\"NCEI Oklahoma State Climate Divisions\",\n",
    "    fontsize=24,\n",
    ")\n",
    "# Load and add the logo to the lower left\n",
    "logo_img = mpimg.imread(logo)\n",
    "ax.figure.figimage(\n",
    "    logo_img, 50, 50, zorder=20, alpha=0.5\n",
    ")  # Adjust (x, y) position as needed\n",
    "\n",
    "plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/OK_state_clim_div.png\")\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"St. Lawrence Valley\",\n",
    "    \"Great Lakes\",\n",
    "    \"Northern Plateau\",\n",
    "    \"Champlain Valley\",\n",
    "    \"Hudson Valley\",\n",
    "    \"Mohawk Valley\",\n",
    "    \"Western Plateau\",\n",
    "    \"Eastern Pleateau\",\n",
    "    \"Coastal\",\n",
    "    \"Central Lakes\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clim_div = sorted(clim_div)\n",
    "image = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xCITE_gif(nysm_clim, fh, clim_div, logo):\n",
    "    # Create your dataframe df_\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define colors dictionary and randomly assign colors\n",
    "    nwp_dict = {0: \"green\", 1: \"red\", 2: \"blue\"}  # NAM  # HRRR  # GFS\n",
    "    nwps_all = [0, 1, 2]\n",
    "\n",
    "    # Randomly assign values from nwps_all to the 'lister'\n",
    "    lister = [random.choice(nwps_all) for _ in df_[\"stid\"]]\n",
    "    df_[\"color\"] = [nwp_dict[value] for value in lister]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    gdf_filtered = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in range(len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"upper right\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.5], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=\"black\",\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # # Create custom legend for NWP models\n",
    "    # nam_patch = mpatches.Patch(color=\"green\", label=\"NAM\")\n",
    "    # hrrr_patch = mpatches.Patch(color=\"red\", label=\"HRRR\")\n",
    "    # gfs_patch = mpatches.Patch(color=\"blue\", label=\"GFS\")\n",
    "\n",
    "    # # Add second legend to the plot\n",
    "    # ax.legend(\n",
    "    #     handles=[nam_patch, hrrr_patch, gfs_patch],\n",
    "    #     loc=\"upper left\",\n",
    "    #     fontsize=12,\n",
    "    #     title=\"NWP Models\",\n",
    "    # )\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"NCEI New York State Climate Divisions\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "    # Load and add the logo to the lower left\n",
    "    logo_img = mpimg.imread(logo)\n",
    "    ax.figure.figimage(\n",
    "        logo_img, 50, 50, zorder=20, alpha=0.5\n",
    "    )  # Adjust (x, y) position as needed\n",
    "\n",
    "    plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/NY_state_clim_div.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xCITE_gif(nysm_clim, 1, clim_div, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim\n",
    "nysm_ = nysm_clim.copy()\n",
    "\n",
    "nysm_ = nysm_.rename(columns={\"lat [degrees]\": \"lat\", \"lon [degrees]\": \"lon\"})\n",
    "nysm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "def get_closest_stations(nysm_df, neighbors, target_station, nwp_model):\n",
    "    # Earth's radius in kilometers\n",
    "    EARTH_RADIUS_KM = 6378\n",
    "\n",
    "    lats = nysm_df[\"lat\"].unique()\n",
    "    lons = nysm_df[\"lon\"].unique()\n",
    "\n",
    "    locations_a = pd.DataFrame()\n",
    "    locations_a[\"lat\"] = lats\n",
    "    locations_a[\"lon\"] = lons\n",
    "\n",
    "    for column in locations_a[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    locations_b = locations_a\n",
    "\n",
    "    ball = BallTree(locations_a[[\"lat_rad\", \"lon_rad\"]].values, metric=\"haversine\")\n",
    "\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = neighbors\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "\n",
    "    # Convert distances from radians to kilometers\n",
    "    distances_km = distances * EARTH_RADIUS_KM\n",
    "\n",
    "    # source info to creare a dictionary\n",
    "    indices_list = [indices[x][0:k] for x in range(len(indices))]\n",
    "    distances_list = [distances_km[x][0:k] for x in range(len(distances_km))]\n",
    "    stations = nysm_df[\"stid\"].unique()\n",
    "\n",
    "    # create dictionary\n",
    "    station_dict = {}\n",
    "    for k, _ in enumerate(stations):\n",
    "        station_dict[stations[k]] = (indices_list[k], distances_list[k])\n",
    "\n",
    "    utilize_ls = []\n",
    "    vals, dists = station_dict.get(target_station)\n",
    "\n",
    "    if nwp_model == \"GFS\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 30 and len(utilize_ls) < 5:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"NAM\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 12 and len(utilize_ls) < 4:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"HRRR\":\n",
    "        for v, d in zip(vals, dists):\n",
    "            x = stations[v]\n",
    "            utilize_ls.append(x)\n",
    "\n",
    "    return utilize_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = nysm_clim[\"stid\"].unique()\n",
    "\n",
    "elev_delta = []\n",
    "\n",
    "for s in stations:\n",
    "    print(s)\n",
    "    utilize_ls = get_closest_stations(nysm_, 15, s, \"GFS\")\n",
    "    selection = nysm_[nysm_[\"stid\"].isin(utilize_ls)]\n",
    "    # Find the maximum value in 'col1'\n",
    "    max_value = selection[\"elevation [m]\"].max()\n",
    "    min_value = selection[\"elevation [m]\"].min()\n",
    "    delta = max_value - min_value\n",
    "    elev_delta.append(delta)\n",
    "elev_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners(nysm_clim, clim_div, learners):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - fh: Forecast hour.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # # Save the figure\n",
    "    # plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/xCITE_gif/mockup_fh{fh}.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    - elev_delta: List containing the elevation delta values to determine scatter point size.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Ensure elev_delta is the same length as df_\n",
    "    if len(elev_delta) != len(df_):\n",
    "        raise ValueError(\n",
    "            \"Length of elev_delta must match the number of stations in the DataFrame\"\n",
    "        )\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points with sizes based on 'elev_delta'\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=elev_delta,  # Size of scatter points based on elev_delta\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
