{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import hrrr_data, nam_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_error_lead_0</th>\n",
       "      <th>Model forecast</th>\n",
       "      <th>diff</th>\n",
       "      <th>valid_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.546992</td>\n",
       "      <td>-3560.535889</td>\n",
       "      <td>999.539219</td>\n",
       "      <td>2022-01-02 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.840391</td>\n",
       "      <td>-3560.535889</td>\n",
       "      <td>999.832618</td>\n",
       "      <td>2022-01-02 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.058188</td>\n",
       "      <td>-3560.535889</td>\n",
       "      <td>998.934039</td>\n",
       "      <td>2022-01-02 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.384384</td>\n",
       "      <td>-3560.535889</td>\n",
       "      <td>997.607842</td>\n",
       "      <td>2022-01-02 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.572451</td>\n",
       "      <td>-3560.535889</td>\n",
       "      <td>998.419775</td>\n",
       "      <td>2022-01-03 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>0.192047</td>\n",
       "      <td>0.619318</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>2024-11-30 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>-0.432298</td>\n",
       "      <td>0.233843</td>\n",
       "      <td>-0.464659</td>\n",
       "      <td>2024-12-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>0.083091</td>\n",
       "      <td>-0.227240</td>\n",
       "      <td>0.180093</td>\n",
       "      <td>2024-12-01 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>-0.788402</td>\n",
       "      <td>-0.741111</td>\n",
       "      <td>-0.547226</td>\n",
       "      <td>2024-12-01 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>0.717025</td>\n",
       "      <td>-1.178573</td>\n",
       "      <td>1.080938</td>\n",
       "      <td>2024-12-01 22:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3527 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target_error_lead_0  Model forecast        diff          valid_time\n",
       "0                0.546992    -3560.535889  999.539219 2022-01-02 04:00:00\n",
       "1                0.840391    -3560.535889  999.832618 2022-01-02 10:00:00\n",
       "2               -0.058188    -3560.535889  998.934039 2022-01-02 16:00:00\n",
       "3               -1.384384    -3560.535889  997.607842 2022-01-02 22:00:00\n",
       "4               -0.572451    -3560.535889  998.419775 2022-01-03 04:00:00\n",
       "...                   ...             ...         ...                 ...\n",
       "3169             0.192047        0.619318    0.051535 2024-11-30 22:00:00\n",
       "3170            -0.432298        0.233843   -0.464659 2024-12-01 04:00:00\n",
       "3171             0.083091       -0.227240    0.180093 2024-12-01 10:00:00\n",
       "3172            -0.788402       -0.741111   -0.547226 2024-12-01 16:00:00\n",
       "3173             0.717025       -1.178573    1.080938 2024-12-01 22:00:00\n",
       "\n",
       "[3527 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/AMS_2025/20241216/VOOR/VOOR_fh28_u_total_NAM_ml_output_linear.parquet\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stid</th>\n",
       "      <th>number</th>\n",
       "      <th>name</th>\n",
       "      <th>lat [degrees]</th>\n",
       "      <th>lon [degrees]</th>\n",
       "      <th>elevation [m]</th>\n",
       "      <th>county</th>\n",
       "      <th>nearest_city</th>\n",
       "      <th>state</th>\n",
       "      <th>distance_from_town [km]</th>\n",
       "      <th>direction_from_town [degrees]</th>\n",
       "      <th>climate_division</th>\n",
       "      <th>climate_division_name</th>\n",
       "      <th>wfo</th>\n",
       "      <th>commissioned</th>\n",
       "      <th>decommissioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADDI</td>\n",
       "      <td>107</td>\n",
       "      <td>Addison</td>\n",
       "      <td>42.040360</td>\n",
       "      <td>-77.237260</td>\n",
       "      <td>507.6140</td>\n",
       "      <td>Steuben</td>\n",
       "      <td>Addison</td>\n",
       "      <td>NY</td>\n",
       "      <td>6.9</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Western Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2016-08-10 18:15:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANDE</td>\n",
       "      <td>111</td>\n",
       "      <td>Andes</td>\n",
       "      <td>42.182270</td>\n",
       "      <td>-74.801390</td>\n",
       "      <td>518.2820</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>Andes</td>\n",
       "      <td>NY</td>\n",
       "      <td>1.5</td>\n",
       "      <td>WSW</td>\n",
       "      <td>2</td>\n",
       "      <td>Eastern Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2016-08-04 15:55:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BATA</td>\n",
       "      <td>24</td>\n",
       "      <td>Batavia</td>\n",
       "      <td>43.019940</td>\n",
       "      <td>-78.135660</td>\n",
       "      <td>276.1200</td>\n",
       "      <td>Genesee</td>\n",
       "      <td>Batavia</td>\n",
       "      <td>NY</td>\n",
       "      <td>4.9</td>\n",
       "      <td>ENE</td>\n",
       "      <td>9</td>\n",
       "      <td>Great Lakes</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2016-02-18 18:40:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BEAC</td>\n",
       "      <td>76</td>\n",
       "      <td>Beacon</td>\n",
       "      <td>41.528750</td>\n",
       "      <td>-73.945270</td>\n",
       "      <td>90.1598</td>\n",
       "      <td>Dutchess</td>\n",
       "      <td>Beacon</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NE</td>\n",
       "      <td>5</td>\n",
       "      <td>Hudson Valley</td>\n",
       "      <td>ALY</td>\n",
       "      <td>2016-08-22 16:45:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BELD</td>\n",
       "      <td>90</td>\n",
       "      <td>Belden</td>\n",
       "      <td>42.223220</td>\n",
       "      <td>-75.668520</td>\n",
       "      <td>470.3700</td>\n",
       "      <td>Broome</td>\n",
       "      <td>Belden</td>\n",
       "      <td>NY</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NNE</td>\n",
       "      <td>2</td>\n",
       "      <td>Eastern Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2015-11-30 20:20:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>WFMB</td>\n",
       "      <td>14</td>\n",
       "      <td>Whiteface Mountain Base</td>\n",
       "      <td>44.393236</td>\n",
       "      <td>-73.858829</td>\n",
       "      <td>614.5990</td>\n",
       "      <td>Essex</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.5</td>\n",
       "      <td>W</td>\n",
       "      <td>3</td>\n",
       "      <td>Northern Plateau</td>\n",
       "      <td>BTV</td>\n",
       "      <td>2016-01-29 20:55:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>WGAT</td>\n",
       "      <td>123</td>\n",
       "      <td>Woodgate</td>\n",
       "      <td>43.532408</td>\n",
       "      <td>-75.158597</td>\n",
       "      <td>442.9660</td>\n",
       "      <td>Oneida</td>\n",
       "      <td>Woodgate</td>\n",
       "      <td>NY</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NNW</td>\n",
       "      <td>3</td>\n",
       "      <td>Northern Plateau</td>\n",
       "      <td>BGM</td>\n",
       "      <td>2016-08-29 18:20:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>WHIT</td>\n",
       "      <td>10</td>\n",
       "      <td>Whitehall</td>\n",
       "      <td>43.485073</td>\n",
       "      <td>-73.423071</td>\n",
       "      <td>36.5638</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Whitehall</td>\n",
       "      <td>NY</td>\n",
       "      <td>8.0</td>\n",
       "      <td>S</td>\n",
       "      <td>7</td>\n",
       "      <td>Champlain Valley</td>\n",
       "      <td>ALY</td>\n",
       "      <td>2015-08-26 20:30:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>WOLC</td>\n",
       "      <td>79</td>\n",
       "      <td>Wolcott</td>\n",
       "      <td>43.228680</td>\n",
       "      <td>-76.842610</td>\n",
       "      <td>121.2190</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>Wolcott</td>\n",
       "      <td>NY</td>\n",
       "      <td>2.4</td>\n",
       "      <td>WNW</td>\n",
       "      <td>9</td>\n",
       "      <td>Great Lakes</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2016-03-09 18:10:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>YORK</td>\n",
       "      <td>99</td>\n",
       "      <td>York</td>\n",
       "      <td>42.855040</td>\n",
       "      <td>-77.847760</td>\n",
       "      <td>177.9420</td>\n",
       "      <td>Livingston</td>\n",
       "      <td>York</td>\n",
       "      <td>NY</td>\n",
       "      <td>3.6</td>\n",
       "      <td>ESE</td>\n",
       "      <td>10</td>\n",
       "      <td>Central Lakes</td>\n",
       "      <td>BUF</td>\n",
       "      <td>2016-08-09 17:55:00 UTC</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     stid  number                     name  lat [degrees]  lon [degrees]  \\\n",
       "0    ADDI     107                  Addison      42.040360     -77.237260   \n",
       "1    ANDE     111                    Andes      42.182270     -74.801390   \n",
       "2    BATA      24                  Batavia      43.019940     -78.135660   \n",
       "3    BEAC      76                   Beacon      41.528750     -73.945270   \n",
       "4    BELD      90                   Belden      42.223220     -75.668520   \n",
       "..    ...     ...                      ...            ...            ...   \n",
       "121  WFMB      14  Whiteface Mountain Base      44.393236     -73.858829   \n",
       "122  WGAT     123                 Woodgate      43.532408     -75.158597   \n",
       "123  WHIT      10                Whitehall      43.485073     -73.423071   \n",
       "124  WOLC      79                  Wolcott      43.228680     -76.842610   \n",
       "125  YORK      99                     York      42.855040     -77.847760   \n",
       "\n",
       "     elevation [m]      county nearest_city state  distance_from_town [km]  \\\n",
       "0         507.6140     Steuben      Addison    NY                      6.9   \n",
       "1         518.2820    Delaware        Andes    NY                      1.5   \n",
       "2         276.1200     Genesee      Batavia    NY                      4.9   \n",
       "3          90.1598    Dutchess       Beacon    NY                      3.3   \n",
       "4         470.3700      Broome       Belden    NY                      2.2   \n",
       "..             ...         ...          ...   ...                      ...   \n",
       "121       614.5990       Essex   Wilmington    NY                      3.5   \n",
       "122       442.9660      Oneida     Woodgate    NY                      1.4   \n",
       "123        36.5638  Washington    Whitehall    NY                      8.0   \n",
       "124       121.2190       Wayne      Wolcott    NY                      2.4   \n",
       "125       177.9420  Livingston         York    NY                      3.6   \n",
       "\n",
       "    direction_from_town [degrees]  climate_division climate_division_name  \\\n",
       "0                               S                 1       Western Plateau   \n",
       "1                             WSW                 2       Eastern Plateau   \n",
       "2                             ENE                 9           Great Lakes   \n",
       "3                              NE                 5         Hudson Valley   \n",
       "4                             NNE                 2       Eastern Plateau   \n",
       "..                            ...               ...                   ...   \n",
       "121                             W                 3      Northern Plateau   \n",
       "122                           NNW                 3      Northern Plateau   \n",
       "123                             S                 7      Champlain Valley   \n",
       "124                           WNW                 9           Great Lakes   \n",
       "125                           ESE                10         Central Lakes   \n",
       "\n",
       "     wfo             commissioned  decommissioned  \n",
       "0    BGM  2016-08-10 18:15:00 UTC             NaN  \n",
       "1    BGM  2016-08-04 15:55:00 UTC             NaN  \n",
       "2    BUF  2016-02-18 18:40:00 UTC             NaN  \n",
       "3    ALY  2016-08-22 16:45:00 UTC             NaN  \n",
       "4    BGM  2015-11-30 20:20:00 UTC             NaN  \n",
       "..   ...                      ...             ...  \n",
       "121  BTV  2016-01-29 20:55:00 UTC             NaN  \n",
       "122  BGM  2016-08-29 18:20:00 UTC             NaN  \n",
       "123  ALY  2015-08-26 20:30:00 UTC             NaN  \n",
       "124  BUF  2016-03-09 18:10:00 UTC             NaN  \n",
       "125  BUF  2016-08-09 17:55:00 UTC             NaN  \n",
       "\n",
       "[126 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "clim_div = nysm_clim[\"climate_division_name\"].unique()\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim = nysm_clim[nysm_clim[\"climate_division_name\"] == \"Hudson Valley\"]\n",
    "# nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_error_lead_0</th>\n",
       "      <th>Model forecast</th>\n",
       "      <th>diff</th>\n",
       "      <th>valid_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.071228</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1000.071228</td>\n",
       "      <td>2019-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.823466</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1000.823466</td>\n",
       "      <td>2019-01-01 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.419267</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>998.580733</td>\n",
       "      <td>2019-01-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549127</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>999.549127</td>\n",
       "      <td>2019-01-01 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.419304</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>998.580696</td>\n",
       "      <td>2019-01-02 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>0.976278</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>2022-12-10 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644</th>\n",
       "      <td>2.164532</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>2.155319</td>\n",
       "      <td>2022-12-10 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>-0.455684</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>-0.464865</td>\n",
       "      <td>2022-12-10 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>-0.445917</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>-0.454977</td>\n",
       "      <td>2022-12-10 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>1.684794</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>1.675634</td>\n",
       "      <td>2022-12-11 03:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5164 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target_error_lead_0  Model forecast         diff          valid_time\n",
       "0                1.071228     -999.000000  1000.071228 2019-01-01 03:00:00\n",
       "1                1.823466     -999.000000  1000.823466 2019-01-01 09:00:00\n",
       "2               -0.419267     -999.000000   998.580733 2019-01-01 15:00:00\n",
       "3                0.549127     -999.000000   999.549127 2019-01-01 21:00:00\n",
       "4               -0.419304     -999.000000   998.580696 2019-01-02 03:00:00\n",
       "...                   ...             ...          ...                 ...\n",
       "4643             0.976278        0.009178     0.967100 2022-12-10 03:00:00\n",
       "4644             2.164532        0.009213     2.155319 2022-12-10 09:00:00\n",
       "4645            -0.455684        0.009181    -0.464865 2022-12-10 15:00:00\n",
       "4646            -0.445917        0.009060    -0.454977 2022-12-10 21:00:00\n",
       "4647             1.684794        0.009159     1.675634 2022-12-11 03:00:00\n",
       "\n",
       "[5164 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/AMS_2025/20250102/BKLN/BKLN_fh3_t2m_GFS_ml_output_og.parquet\"\n",
    ")\n",
    "parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_error_lead_0</th>\n",
       "      <th>Model forecast</th>\n",
       "      <th>diff</th>\n",
       "      <th>valid_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.071228</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1000.071228</td>\n",
       "      <td>2019-01-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.823466</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1000.823466</td>\n",
       "      <td>2019-01-01 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.419267</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>998.580733</td>\n",
       "      <td>2019-01-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549127</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>999.549127</td>\n",
       "      <td>2019-01-01 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.419304</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>998.580696</td>\n",
       "      <td>2019-01-02 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>0.976278</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>2022-12-10 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4644</th>\n",
       "      <td>2.164532</td>\n",
       "      <td>0.009213</td>\n",
       "      <td>2.155319</td>\n",
       "      <td>2022-12-10 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4645</th>\n",
       "      <td>-0.455684</td>\n",
       "      <td>0.009181</td>\n",
       "      <td>-0.464865</td>\n",
       "      <td>2022-12-10 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>-0.445917</td>\n",
       "      <td>0.009060</td>\n",
       "      <td>-0.454977</td>\n",
       "      <td>2022-12-10 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>1.684794</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>1.675634</td>\n",
       "      <td>2022-12-11 03:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5164 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target_error_lead_0  Model forecast         diff          valid_time\n",
       "0                1.071228     -999.000000  1000.071228 2019-01-01 03:00:00\n",
       "1                1.823466     -999.000000  1000.823466 2019-01-01 09:00:00\n",
       "2               -0.419267     -999.000000   998.580733 2019-01-01 15:00:00\n",
       "3                0.549127     -999.000000   999.549127 2019-01-01 21:00:00\n",
       "4               -0.419304     -999.000000   998.580696 2019-01-02 03:00:00\n",
       "...                   ...             ...          ...                 ...\n",
       "4643             0.976278        0.009178     0.967100 2022-12-10 03:00:00\n",
       "4644             2.164532        0.009213     2.155319 2022-12-10 09:00:00\n",
       "4645            -0.455684        0.009181    -0.464865 2022-12-10 15:00:00\n",
       "4646            -0.445917        0.009060    -0.454977 2022-12-10 21:00:00\n",
       "4647             1.684794        0.009159     1.675634 2022-12-11 03:00:00\n",
       "\n",
       "[5164 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parq[\"target_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/GFS/s2s/Western Plateau_t2m_GFS_lookup_linear.csv\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/HRRR/s2s/Central Lakes_u_total_HRRR_lookup_quad.csv\"\n",
    ")\n",
    "# df = df[df[\"station\"] == \"ADDI\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'station' and set 'fh' as the index\n",
    "df_grouped = df.groupby(\"station\").apply(lambda x: x.set_index(\"forecast_hour\"))\n",
    "\n",
    "# # Optionally, reset the index to avoid multi-level indexing from `groupby().apply()`\n",
    "# df_grouped = df_grouped.reset_index(level=0, drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim[nysm_clim[\"climate_division_name\"] == \"Eastern Plateau\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_fh(fh, station, var, times):\n",
    "    hrrr_df_0 = hrrr_data.read_hrrr_data(str(fh + 2).zfill(2))\n",
    "    hrrr_df_1 = hrrr_data.read_hrrr_data(str(fh + 4).zfill(2))\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[hrrr_df_0[\"station\"] == station]\n",
    "    hrrr_df_1 = hrrr_df_1[hrrr_df_1[\"station\"] == station]\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[[\"valid_time\", var]]\n",
    "    hrrr_df_1 = hrrr_df_1[[\"valid_time\", var]]\n",
    "\n",
    "    # Create a DataFrame for valid times\n",
    "    df = pd.DataFrame({\"valid_time\": times})\n",
    "    df = df.merge(hrrr_df_0, on=\"valid_time\", suffixes=(None, f\"_{station}_+2\"))\n",
    "    df = df.merge(hrrr_df_1, on=\"valid_time\", suffixes=(None, f\"_{station}_+4\"))\n",
    "    df = df.rename(columns={\"t2m\": f\"{var}_{station}_+2\"})\n",
    "    # df.fillna(-999, inplace=True)\n",
    "\n",
    "    fh2 = df[f\"{var}_{station}_+2\"].values\n",
    "    fh4 = df[f\"{var}_{station}_+4\"].values\n",
    "\n",
    "    print(len(fh2))\n",
    "    print(len(fh4))\n",
    "\n",
    "    return fh2, fh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nam_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2022\", \"2023\", \"2024\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    nam_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        for month in np.arange(1, 13):\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                print(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                nam_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "            gc.collect()\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    nam_fcast_and_error_df = pd.concat(nam_fcast_and_error)\n",
    "    nam_fcast_and_error_df = nam_fcast_and_error_df.dropna()\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return nam_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gfs_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    gfs_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        print(\"compiling\", year)\n",
    "        for month in np.arange(1, 13):\n",
    "            print(month)\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                gfs_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    gfs_fcast_and_error_df = pd.concat(gfs_fcast_and_error)\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return gfs_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nysm_data():\n",
    "    \"\"\"\n",
    "    Load and concatenate NYSM (New York State Mesonet) data from parquet files.\n",
    "\n",
    "    NYSM data is resampled at 1-hour intervals and stored in separate parquet files\n",
    "    for each year from 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        nysm_1H_obs (pd.DataFrame): A DataFrame containing concatenated NYSM data with\n",
    "        missing values filled for the 'snow_depth' column.\n",
    "\n",
    "    This function reads NYSM data from parquet files, resamples it to a 1-hour interval,\n",
    "    and concatenates the data from multiple years. Missing values in the 'snow_depth'\n",
    "    column are filled with -999, and any rows with missing values are dropped before\n",
    "    returning the resulting DataFrame.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    nysm_data = load_nysm_data()\n",
    "    print(nysm_data.head())\n",
    "    ```\n",
    "\n",
    "    Note: Ensure that the parquet files are located in the specified path before using this function.\n",
    "    \"\"\"\n",
    "    # Define the path where NYSM parquet files are stored.\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    # Initialize an empty list to store data for each year.\n",
    "    nysm_1H = []\n",
    "\n",
    "    # Loop through the years from 2018 to 2022 and read the corresponding parquet files.\n",
    "    for year in np.arange(2024, 2025):\n",
    "        df = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "        df.reset_index(inplace=True)\n",
    "        nysm_1H.append(df)\n",
    "\n",
    "    # Concatenate data from different years into a single DataFrame.\n",
    "    nysm_1H_obs = pd.concat(nysm_1H)\n",
    "\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"snow_depth\"].fillna(-999, inplace=True)\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"ta9m\"].fillna(-999, inplace=True)\n",
    "\n",
    "    # if nysm_1H_obs['ta9m'].isna().mean() > 0.8:\n",
    "    #     nysm_1H_obs.drop('ta9m', axis=1, inplace=True)\n",
    "\n",
    "    nysm_1H_obs.dropna(inplace=True)\n",
    "\n",
    "    return nysm_1H_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/GFS_data/fh003/GFS_2018_01_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nam_df[\"station\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_nysm_data()\n",
    "\n",
    "# df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "\n",
    "# stations_ls = ['MANH', 'VOOR', 'HERK', 'ANDE', 'BUFF', 'SCIP', 'GROV', 'LOUI', 'ESSX', 'GABR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = read_gfs_data(\"009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = df[\"lon [degrees]\"].values\n",
    "lats = df[\"lat [degrees]\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"stid\"].isin(stations_ls)]\n",
    "df = df[[\"lat [degrees]\", \"lon [degrees]\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/aevans/nwp_bias/src/landtype/data/first_paper_stations_coords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# gfs_df = read_gfs_data(\"006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gfs_df[\"station\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 6\n",
    "station = \"SOUT\"\n",
    "var = \"t2m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh001/NAM_2022_04_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_df = read_nam_data(str(fh).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrrr_df = hrrr_data.read_hrrr_data(str(fh).zfill(2))\n",
    "\n",
    "# # Filter NYSM data to match valid times from HRRR data\n",
    "# mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "# fh2_, fh4_ = get_more_fh(fh, station, var, mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a100_mae = [\n",
    "    0.07,\n",
    "    0.17,\n",
    "]\n",
    "a100_mse = [\n",
    "    0.07,\n",
    "    0.22,\n",
    "]\n",
    "a100_batch = [\n",
    "    1000,\n",
    "    5000,\n",
    "]\n",
    "a100_gpu = [8, 30]\n",
    "a100_runtime = [\n",
    "    timedelta(seconds=24, minutes=16, hours=0),\n",
    "    timedelta(seconds=5, minutes=16, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh200_mae = [0.06, 0.06]\n",
    "gh200_mse = [0.06, 0.07]\n",
    "gh200_batch = [1000, 10000]\n",
    "gh200_gpu = [8, 64]\n",
    "gh200_runtime = [\n",
    "    timedelta(seconds=22, minutes=6, hours=0),\n",
    "    timedelta(seconds=51, minutes=6, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def plot_runtime_bar_chart(a100_batch, a100_run_time):\n",
    "    # Convert timedelta objects to total minutes\n",
    "    run_time_minutes = [rt.total_seconds() / 60 for rt in a100_run_time]\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the bar chart\n",
    "    ax.bar(a100_batch, run_time_minutes, 1000, color=\"orange\", label=\"Run Time\")\n",
    "\n",
    "    # Adding scatter points with large X markers on top of bars\n",
    "    # ax.scatter(a100_batch, run_time_minutes, color='red', marker='x', s=100, label='Run Time Points')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"Run Time (minutes)\")\n",
    "    ax.set_title(\"Run Time by Batch Size gh200\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_metrics_bar(a100_mae, a100_mse, a100_batch):\n",
    "    # Number of bars\n",
    "    n = len(a100_mae)\n",
    "\n",
    "    # Create an array for the positions of the bars\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(n)\n",
    "\n",
    "    # Plotting the bars\n",
    "    fig, ax = plt.subplots()\n",
    "    bar1 = ax.bar(\n",
    "        index,\n",
    "        a100_mae,\n",
    "        bar_width,\n",
    "    )\n",
    "    # bar2 = ax.bar(index + bar_width, a100_mse, bar_width, label='MSE')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"GPU Memory\")\n",
    "    ax.set_ylim(0, 90)\n",
    "    ax.set_title(\"GPU Memory by Batch Size for a100\")\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(a100_batch)\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(a100_batch, a100_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(gh200_batch, gh200_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(a100_gpu, a100_mse, a100_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(gh200_gpu, gh200_mse, gh200_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "color_dict = {\n",
    "    0: \"cyan\",\n",
    "    1: \"blue\",\n",
    "    2: \"yellow\",\n",
    "    3: \"green\",\n",
    "    # 4: 'red',\n",
    "    # 5: 'orange',\n",
    "    # 6: 'purple',\n",
    "    # 7: 'black',\n",
    "    # 8: 'white'\n",
    "}\n",
    "\n",
    "\n",
    "def plurality_plot(df, geovar):\n",
    "    projPC = crs.PlateCarree()\n",
    "    latN = df[\"lat\"].max() + 1\n",
    "    latS = df[\"lat\"].min() - 1\n",
    "    lonW = df[\"lon\"].max() + 1\n",
    "    lonE = df[\"lon\"].min() - 1\n",
    "    cLat = (latN + latS) / 2\n",
    "    cLon = (lonW + lonE) / 2\n",
    "    projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6), subplot_kw={\"projection\": crs.PlateCarree()}, dpi=400\n",
    "    )\n",
    "    ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    ax.add_feature(cfeature.STATES)\n",
    "    ax.xticklabels_top = False\n",
    "    ax.ylabels_right = False\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        x=df[\"lon\"],\n",
    "        y=df[\"lat\"],\n",
    "        c=df[\"color\"],\n",
    "        s=40,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "    )\n",
    "    ax.set_title(f\"Mesonet Site {geovar} Clusters\", size=16)\n",
    "    ax.set_xlabel(\"Longitude\", size=14)\n",
    "    ax.set_ylabel(\"Latitude\", size=14)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.grid()\n",
    "\n",
    "    # Create legend patches\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=color, label=f\"Category {key}\")\n",
    "        for key, color in color_dict.items()\n",
    "    ]\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    ax.legend(\n",
    "        handles=legend_patches,\n",
    "        loc=\"upper left\",  # Use 'upper left' to anchor the legend in the figure\n",
    "        bbox_to_anchor=(1.1, 1),  # Move the legend outside the plot to the right\n",
    "        borderaxespad=0,  # Adjust the padding between the legend and the axes\n",
    "        title=\"Categories\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/lstm_clusters.csv\")\n",
    "cluster_df[\"lon\"] = lons\n",
    "cluster_df[\"lat\"] = lats\n",
    "cluster_df[\"color\"] = cluster_df[\"elev_cat\"].map(color_dict)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurality_plot(cluster_df, \"Elevation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(shapefile_path):\n",
    "    # Define a list of colors for each shape file\n",
    "    # colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # projPC = crs.PlateCarree()\n",
    "    # latN = df[\"nysm_lat\"].max() + 0.5\n",
    "    # latS = df[\"nysm_lat\"].min() - 0.5\n",
    "    # lonW = df[\"nysm_lon\"].max() + 0.5\n",
    "    # lonE = df[\"nysm_lon\"].min() - 0.5\n",
    "    # cLat = (latN + latS) / 2\n",
    "    # cLon = (lonW + lonE) / 2\n",
    "    # projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(9, 15), subplot_kw={\"projection\": crs.PlateCarree()}\n",
    "    )\n",
    "    # ax.legend()\n",
    "    # # ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    # ax.add_feature(cfeature.LAND)\n",
    "    # ax.add_feature(cfeature.COASTLINE)\n",
    "    # ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    # ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    # ax.add_feature(cfeature.STATES)\n",
    "    # ax.xticklabels_top = False\n",
    "    # ax.ylabels_right = False\n",
    "    # ax.gridlines(\n",
    "    #     crs=crs.PlateCarree(),\n",
    "    #     draw_labels=True,\n",
    "    #     linewidth=2,\n",
    "    #     color=\"black\",\n",
    "    #     alpha=0.5,\n",
    "    #     linestyle=\"--\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"],\n",
    "    #     df[\"nysm_lat\"],\n",
    "    #     c=\"blue\",\n",
    "    #     s=70,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"NYSM Sites\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"].iloc[0],\n",
    "    #     df[\"nysm_lat\"].iloc[0],\n",
    "    #     c=\"green\",\n",
    "    #     marker=\"*\",\n",
    "    #     s=400,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"Southold\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"hrrr_lon\"],\n",
    "    #     df[\"hrrr_lat\"],\n",
    "    #     c='orange',\n",
    "    #     s = 70,\n",
    "    #     edgecolors='black',\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label='HRRR'\n",
    "    # )\n",
    "\n",
    "    # # Annotate each point in NYSM\n",
    "    # for i, txt in enumerate(df[\"station\"]):\n",
    "    #     plt.annotate(\n",
    "    #         txt,\n",
    "    #         (df[\"nysm_lon\"].iloc[i], df[\"nysm_lat\"].iloc[i]),\n",
    "    #         textcoords=\"offset points\",\n",
    "    #         xytext=(5, 10),\n",
    "    #         ha=\"center\",\n",
    "    #         fontsize=18,\n",
    "    #     )\n",
    "\n",
    "    # Load the shape file using geopandas\n",
    "    climate_divisions = gpd.read_file(shapefile_path)\n",
    "    # Plot climate divisions from the shape file\n",
    "    climate_divisions.plot(\n",
    "        ax=ax,\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=\"none\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1), loc=\"upper left\", borderaxespad=0, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"St. Lawrence Valley\",\n",
    "    \"Great Lakes\",\n",
    "    \"Northern Plateau\",\n",
    "    \"Champlain Valley\",\n",
    "    \"Hudson Valley\",\n",
    "    \"Mohawk Valley\",\n",
    "    \"Western Plateau\",\n",
    "    \"Eastern Pleateau\",\n",
    "    \"Coastal\",\n",
    "    \"Central Lakes\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clim_div = sorted(clim_div)\n",
    "image = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xCITE_gif(nysm_clim, fh, clim_div, logo):\n",
    "    # Create your dataframe df_\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define colors dictionary and randomly assign colors\n",
    "    nwp_dict = {0: \"green\", 1: \"red\", 2: \"blue\"}  # NAM  # HRRR  # GFS\n",
    "    nwps_all = [0, 1, 2]\n",
    "\n",
    "    # Randomly assign values from nwps_all to the 'lister'\n",
    "    lister = [random.choice(nwps_all) for _ in df_[\"stid\"]]\n",
    "    df_[\"color\"] = [nwp_dict[value] for value in lister]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    gdf_filtered = gdf_filtered.iloc[20:29]\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in range(len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower right\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-75.0, -72.0, 40.0, 44.0], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for NWP models\n",
    "    nam_patch = mpatches.Patch(color=\"green\", label=\"NAM\")\n",
    "    hrrr_patch = mpatches.Patch(color=\"red\", label=\"HRRR\")\n",
    "    gfs_patch = mpatches.Patch(color=\"blue\", label=\"GFS\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    ax.legend(\n",
    "        handles=[nam_patch, hrrr_patch, gfs_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=12,\n",
    "        title=\"NWP Models\",\n",
    "    )\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"Predicted Most Accurate Model for Hudson Valley,\\n Forecast Hour {fh}\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "    # Load and add the logo to the lower left\n",
    "    logo_img = mpimg.imread(logo)\n",
    "    ax.figure.figimage(\n",
    "        logo_img, 50, 50, zorder=20, alpha=0.5\n",
    "    )  # Adjust (x, y) position as needed\n",
    "\n",
    "    plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/xCITE_gif/mockup_fh{fh}.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.arange(1, 13):\n",
    "#     create_xCITE_gif(nysm_clim, i, clim_div, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs_path = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/AMS_2025/GFS_t2m_learners_csvs/\"\n",
    ")\n",
    "\n",
    "files = os.listdir(gfs_path)\n",
    "\n",
    "masters = pd.DataFrame()\n",
    "for f in files:\n",
    "    gfs1 = pd.read_csv(f\"{gfs_path}/{f}\")\n",
    "    masters = pd.concat([gfs1, masters])\n",
    "\n",
    "learners = masters[\"station\"].unique()\n",
    "learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim\n",
    "nysm_ = nysm_clim.copy()\n",
    "\n",
    "nysm_ = nysm_.rename(columns={\"lat [degrees]\": \"lat\", \"lon [degrees]\": \"lon\"})\n",
    "nysm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "def get_closest_stations(nysm_df, neighbors, target_station, nwp_model):\n",
    "    # Earth's radius in kilometers\n",
    "    EARTH_RADIUS_KM = 6378\n",
    "\n",
    "    lats = nysm_df[\"lat\"].unique()\n",
    "    lons = nysm_df[\"lon\"].unique()\n",
    "\n",
    "    locations_a = pd.DataFrame()\n",
    "    locations_a[\"lat\"] = lats\n",
    "    locations_a[\"lon\"] = lons\n",
    "\n",
    "    for column in locations_a[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    locations_b = locations_a\n",
    "\n",
    "    ball = BallTree(locations_a[[\"lat_rad\", \"lon_rad\"]].values, metric=\"haversine\")\n",
    "\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = neighbors\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "\n",
    "    # Convert distances from radians to kilometers\n",
    "    distances_km = distances * EARTH_RADIUS_KM\n",
    "\n",
    "    # source info to creare a dictionary\n",
    "    indices_list = [indices[x][0:k] for x in range(len(indices))]\n",
    "    distances_list = [distances_km[x][0:k] for x in range(len(distances_km))]\n",
    "    stations = nysm_df[\"stid\"].unique()\n",
    "\n",
    "    # create dictionary\n",
    "    station_dict = {}\n",
    "    for k, _ in enumerate(stations):\n",
    "        station_dict[stations[k]] = (indices_list[k], distances_list[k])\n",
    "\n",
    "    utilize_ls = []\n",
    "    vals, dists = station_dict.get(target_station)\n",
    "\n",
    "    if nwp_model == \"GFS\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 30 and len(utilize_ls) < 5:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"NAM\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 12 and len(utilize_ls) < 4:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"HRRR\":\n",
    "        for v, d in zip(vals, dists):\n",
    "            x = stations[v]\n",
    "            utilize_ls.append(x)\n",
    "\n",
    "    return utilize_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = nysm_clim[\"stid\"].unique()\n",
    "\n",
    "elev_delta = []\n",
    "\n",
    "for s in stations:\n",
    "    print(s)\n",
    "    utilize_ls = get_closest_stations(nysm_, 15, s, \"GFS\")\n",
    "    selection = nysm_[nysm_[\"stid\"].isin(utilize_ls)]\n",
    "    # Find the maximum value in 'col1'\n",
    "    max_value = selection[\"elevation [m]\"].max()\n",
    "    min_value = selection[\"elevation [m]\"].min()\n",
    "    delta = max_value - min_value\n",
    "    elev_delta.append(delta)\n",
    "elev_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners(nysm_clim, clim_div, learners):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - fh: Forecast hour.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # # Save the figure\n",
    "    # plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/xCITE_gif/mockup_fh{fh}.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gfs_learners(nysm_clim, clim_div, learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    - elev_delta: List containing the elevation delta values to determine scatter point size.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Ensure elev_delta is the same length as df_\n",
    "    if len(elev_delta) != len(df_):\n",
    "        raise ValueError(\n",
    "            \"Length of elev_delta must match the number of stations in the DataFrame\"\n",
    "        )\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points with sizes based on 'elev_delta'\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=elev_delta,  # Size of scatter points based on elev_delta\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('metpy-ams-2023': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e61dc22b1d7735d9ed6eeec2ec3738742023a22462180b494137f4b5863456cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
