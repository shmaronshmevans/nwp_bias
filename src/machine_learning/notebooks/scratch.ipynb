{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data import hrrr_data, nam_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_files(base_path):\n",
    "    required_files = [\n",
    "        \"fh18_u_total_HRRR_ml_output_linear\",\n",
    "        \"fh18_t2m_HRRR_ml_output_linear\",\n",
    "        \"fh18_tp_HRRR_ml_output_linear\",\n",
    "    ]\n",
    "    missing_info = {}\n",
    "\n",
    "    for dir_name in os.listdir(base_path):\n",
    "        dir_path = os.path.join(base_path, dir_name)\n",
    "        if os.path.isdir(dir_path):  # Ensure it's a directory\n",
    "            files_in_dir = set(os.listdir(dir_path))\n",
    "            missing_files = [\n",
    "                f for f in required_files if not any(f in file for file in files_in_dir)\n",
    "            ]\n",
    "\n",
    "            if missing_files:\n",
    "                missing_info[dir_name] = missing_files\n",
    "\n",
    "    if missing_info:\n",
    "        print(\"Directories with missing files:\")\n",
    "        for directory, files in missing_info.items():\n",
    "            print(f\"{directory}: Missing {', '.join(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_files(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nysms = os.listdir(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")\n",
    "len(nysms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "stations = nysm_clim[\"stid\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim[nysm_clim['climate_division_name']=='Northern Plateau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stations:\n",
    "    if not s in nysms:\n",
    "        print(\"Station not yet formulated... \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clim_div_filter(c):\n",
    "    nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "    df = nysm_clim[nysm_clim[\"climate_division_name\"] == c]\n",
    "    stations = df[\"stid\"].unique()\n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hudson_v = clim_div_filter('Hudson Valley')\n",
    "# coast = clim_div_filter('Coastal')\n",
    "# st_ = clim_div_filter('St. Lawrence Valley')\n",
    "# greats = clim_div_filter('Great Lakes')\n",
    "# west = clim_div_filter('Western Plateau')\n",
    "north = clim_div_filter(\"Northern Plateau\")\n",
    "# champ = clim_div_filter('Champlain Valley')\n",
    "# mohawk = clim_div_filter('Mohawk Valley')\n",
    "# central = clim_div_filter(\"Central Lakes\")\n",
    "# east = clim_div_filter('Eastern Plateau')\n",
    "\n",
    "\n",
    "# base_dir = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/2025035\"\n",
    "# dirs = os.listdir(base_dir)\n",
    "\n",
    "\n",
    "# for d in dirs:\n",
    "#     dir_path = os.path.join(base_dir, d)\n",
    "#     if d in st_ or d in greats:\n",
    "#         continue\n",
    "#     else:\n",
    "#         if os.path.isdir(dir_path):  # Ensure it's a directory before removing\n",
    "#             shutil.rmtree(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_directories(base_dir, merge_dir, clim_div):\n",
    "    \"\"\"\n",
    "    Moves files from directories in `base_dir` to corresponding directories in `merge_dir`\n",
    "    if the directory name is in `hudson_v` or `coast`.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the directory containing the source data.\n",
    "        merge_dir (str): Path to the directory where data should be moved.\n",
    "        hudson_v (list): List of valid Hudson Valley directories.\n",
    "        coast (list): List of valid Coastal directories.\n",
    "    \"\"\"\n",
    "    dirs = os.listdir(base_dir)\n",
    "\n",
    "    for d in dirs:\n",
    "        if d in clim_div:\n",
    "            src_path = os.path.join(base_dir, d)\n",
    "            dest_path = os.path.join(merge_dir, d)\n",
    "\n",
    "            if os.path.exists(src_path):\n",
    "                os.makedirs(\n",
    "                    dest_path, exist_ok=True\n",
    "                )  # Ensure destination directory exists\n",
    "\n",
    "                for file in os.listdir(src_path):\n",
    "                    src_file = os.path.join(src_path, file)\n",
    "                    dest_file = os.path.join(dest_path, file)\n",
    "\n",
    "                    if os.path.isfile(src_file):\n",
    "                        shutil.move(src_file, dest_file)  # Move the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")\n",
    "\n",
    "bases = os.listdir(base_dir)\n",
    "\n",
    "for d in bases:\n",
    "    dir_path = os.path.join(base_dir, d)\n",
    "    if os.path.isdir(dir_path):  # Ensure it's a directory\n",
    "        files = os.listdir(dir_path)\n",
    "        for f in files:\n",
    "            if \"full\" in f and f.endswith(\n",
    "                \".parquet\"\n",
    "            ):  # Check for 'full' and '.parquet'\n",
    "                file_path = os.path.join(dir_path, f)\n",
    "                os.remove(file_path)  # Delete the matching parquet file\n",
    "                print(f\"Deleted: {file_path}\")  # Print confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250401\"\n",
    "target = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus\"\n",
    ")\n",
    "# move_directories(base, target, champ)\n",
    "move_directories(base, target, north)\n",
    "# move_directories(base, target, champ)\n",
    "# move_directories(base, target, mohawk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th>fh</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">BKLN</th>\n",
       "      <th>1</th>\n",
       "      <td>0.263909</td>\n",
       "      <td>0.117712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.577666</td>\n",
       "      <td>3123.670955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.316837</td>\n",
       "      <td>46.864056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.269835</td>\n",
       "      <td>0.133802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.786417</td>\n",
       "      <td>13.666603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">WANT</th>\n",
       "      <th>14</th>\n",
       "      <td>1.061844</td>\n",
       "      <td>1.888566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.902845</td>\n",
       "      <td>1.406750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.904044</td>\n",
       "      <td>1.423022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.768050</td>\n",
       "      <td>1.076059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.854659</td>\n",
       "      <td>1.282846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mae          mse\n",
       "station fh                        \n",
       "BKLN    1    0.263909     0.117712\n",
       "        2   47.577666  3123.670955\n",
       "        3    5.316837    46.864056\n",
       "        4    0.269835     0.133802\n",
       "        5    2.786417    13.666603\n",
       "...               ...          ...\n",
       "WANT    14   1.061844     1.888566\n",
       "        15   0.902845     1.406750\n",
       "        16   0.904044     1.423022\n",
       "        17   0.768050     1.076059\n",
       "        18   0.854659     1.282846\n",
       "\n",
       "[144 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/error_visuals/Coastal/Coastal_t2m_error_metrics_master.parquet\"\n",
    ")\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oksm = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/oksm/hrrr_data/fh01/HRRR_2018_01_direct_compare_to_oksm_sites_mask_water.parquet\"\n",
    ")\n",
    "oksm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometer_df = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/radiometer_network.csv\"\n",
    ")\n",
    "radiometer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"/home/aevans/nwp_bias/src/machine_learning/data/profiler_images/2018/PROF_ALBA/PROF_ALBA_2018_010100.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.load(img).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh009/GFS_2018_08_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gfs[\"tp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250107/VOOR/01_07_2025_14:58:37_full_VOOR.parquet\"\n",
    ")\n",
    "for k in df.columns:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"diff\"] = df[f\"tp_VOOR\"] - df[f\"precip_total_VOOR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_filter(ldf, time1, time2):\n",
    "    ldf = ldf[ldf[\"valid_time\"] > time1]\n",
    "    ldf = ldf[ldf[\"valid_time\"] < time2]\n",
    "\n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime(2024, 1, 1, 0, 0, 0)\n",
    "time2 = datetime(2024, 1, 31, 23, 0, 0)\n",
    "\n",
    "df = date_filter(df, time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_output(df, station, fh):\n",
    "    fig, ax = plt.subplots(figsize=(24, 6))\n",
    "    x = df[\"valid_time\"]\n",
    "\n",
    "    # Convert datetime values to numerical values\n",
    "    x_numeric = mdates.date2num(x)\n",
    "\n",
    "    # Assuming your timestamps are in a datetime64 format\n",
    "    day_mask = (x.dt.hour >= 6) & (\n",
    "        x.dt.hour < 18\n",
    "    )  # Adjust the hours based on your day/night definition\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"u_total_{station}\"]),\n",
    "        c=\"mediumseagreen\",\n",
    "        linewidth=3,\n",
    "        label=\"NAM Prediction\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"wspd_sonic_mean_{station}\"]),\n",
    "        c=\"black\",\n",
    "        linewidth=1,\n",
    "        alpha=0.9,\n",
    "        label=\"NYSM Observation\",\n",
    "    )\n",
    "\n",
    "    # Fill daytime hours with white color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=day_mask, color=\"white\", alpha=0.5, label=\"Daytime\"\n",
    "    )\n",
    "\n",
    "    # Fill nighttime hours with grey color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=~day_mask, color=\"grey\", alpha=0.2, label=\"Nighttime\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"NAM Prediction v NYSM Observation: {station}: FH{fh}\", fontsize=28)\n",
    "    # plt.ylim(-5, 5.)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "met_output(df, \"VOOR\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "clim_div = nysm_clim[\"climate_division_name\"].unique()\n",
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_clim = nysm_clim[nysm_clim[\"climate_division_name\"] == \"Hudson Valley\"]\n",
    "# nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/AMS_2025/20250102/BKLN/BKLN_fh3_t2m_GFS_ml_output_og.parquet\"\n",
    ")\n",
    "parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parq[\"target_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/GFS/s2s/Western Plateau_t2m_GFS_lookup_linear.csv\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/parent_models/HRRR/s2s/Central Lakes_u_total_HRRR_lookup_quad.csv\"\n",
    ")\n",
    "# df = df[df[\"station\"] == \"ADDI\"]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'station' and set 'fh' as the index\n",
    "df_grouped = df.groupby(\"station\").apply(lambda x: x.set_index(\"forecast_hour\"))\n",
    "\n",
    "# # Optionally, reset the index to avoid multi-level indexing from `groupby().apply()`\n",
    "# df_grouped = df_grouped.reset_index(level=0, drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "nysm_clim[nysm_clim[\"climate_division_name\"] == \"Eastern Plateau\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_fh(fh, station, var, times):\n",
    "    hrrr_df_0 = hrrr_data.read_hrrr_data(str(fh + 2).zfill(2))\n",
    "    hrrr_df_1 = hrrr_data.read_hrrr_data(str(fh + 4).zfill(2))\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[hrrr_df_0[\"station\"] == station]\n",
    "    hrrr_df_1 = hrrr_df_1[hrrr_df_1[\"station\"] == station]\n",
    "\n",
    "    hrrr_df_0 = hrrr_df_0[[\"valid_time\", var]]\n",
    "    hrrr_df_1 = hrrr_df_1[[\"valid_time\", var]]\n",
    "\n",
    "    # Create a DataFrame for valid times\n",
    "    df = pd.DataFrame({\"valid_time\": times})\n",
    "    df = df.merge(hrrr_df_0, on=\"valid_time\", suffixes=(None, f\"_{station}_+2\"))\n",
    "    df = df.merge(hrrr_df_1, on=\"valid_time\", suffixes=(None, f\"_{station}_+4\"))\n",
    "    df = df.rename(columns={\"t2m\": f\"{var}_{station}_+2\"})\n",
    "    # df.fillna(-999, inplace=True)\n",
    "\n",
    "    fh2 = df[f\"{var}_{station}_+2\"].values\n",
    "    fh4 = df[f\"{var}_{station}_+4\"].values\n",
    "\n",
    "    print(len(fh2))\n",
    "    print(len(fh4))\n",
    "\n",
    "    return fh2, fh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nam_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2022\", \"2023\", \"2024\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    nam_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        for month in np.arange(1, 13):\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                print(\n",
    "                    f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                nam_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}NAM_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "            gc.collect()\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    nam_fcast_and_error_df = pd.concat(nam_fcast_and_error)\n",
    "    nam_fcast_and_error_df = nam_fcast_and_error_df.dropna()\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return nam_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gfs_data(fh):\n",
    "    \"\"\"\n",
    "    Reads and concatenates parquet files containing forecast and error data for HRRR weather models\n",
    "    for the years 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: of hrrr weather forecast information for each NYSM site.\n",
    "    \"\"\"\n",
    "\n",
    "    years = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "    savedir = f\"/home/aevans/nwp_bias/src/machine_learning/data/gfs_data/fh{fh}/\"\n",
    "\n",
    "    # create empty lists to hold dataframes for each model\n",
    "    gfs_fcast_and_error = []\n",
    "\n",
    "    # loop over years and read in parquet files for each model\n",
    "    for year in years:\n",
    "        print(\"compiling\", year)\n",
    "        for month in np.arange(1, 13):\n",
    "            print(month)\n",
    "            str_month = str(month).zfill(2)\n",
    "            if (\n",
    "                os.path.exists(\n",
    "                    f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                )\n",
    "                == True\n",
    "            ):\n",
    "                gfs_fcast_and_error.append(\n",
    "                    pd.read_parquet(\n",
    "                        f\"{savedir}GFS_{year}_{str_month}_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    # concatenate dataframes for each model\n",
    "    gfs_fcast_and_error_df = pd.concat(gfs_fcast_and_error)\n",
    "\n",
    "    # return dataframes for each model\n",
    "    return gfs_fcast_and_error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nysm_data():\n",
    "    \"\"\"\n",
    "    Load and concatenate NYSM (New York State Mesonet) data from parquet files.\n",
    "\n",
    "    NYSM data is resampled at 1-hour intervals and stored in separate parquet files\n",
    "    for each year from 2018 to 2022.\n",
    "\n",
    "    Returns:\n",
    "        nysm_1H_obs (pd.DataFrame): A DataFrame containing concatenated NYSM data with\n",
    "        missing values filled for the 'snow_depth' column.\n",
    "\n",
    "    This function reads NYSM data from parquet files, resamples it to a 1-hour interval,\n",
    "    and concatenates the data from multiple years. Missing values in the 'snow_depth'\n",
    "    column are filled with -999, and any rows with missing values are dropped before\n",
    "    returning the resulting DataFrame.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    nysm_data = load_nysm_data()\n",
    "    print(nysm_data.head())\n",
    "    ```\n",
    "\n",
    "    Note: Ensure that the parquet files are located in the specified path before using this function.\n",
    "    \"\"\"\n",
    "    # Define the path where NYSM parquet files are stored.\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    # Initialize an empty list to store data for each year.\n",
    "    nysm_1H = []\n",
    "\n",
    "    # Loop through the years from 2018 to 2022 and read the corresponding parquet files.\n",
    "    for year in np.arange(2018, 2025):\n",
    "        df = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "        df.reset_index(inplace=True)\n",
    "        nysm_1H.append(df)\n",
    "\n",
    "    # Concatenate data from different years into a single DataFrame.\n",
    "    nysm_1H_obs = pd.concat(nysm_1H)\n",
    "\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"snow_depth\"].fillna(-999, inplace=True)\n",
    "    # Fill missing values in the 'snow_depth' column with -999.\n",
    "    nysm_1H_obs[\"ta9m\"].fillna(-999, inplace=True)\n",
    "\n",
    "    # if nysm_1H_obs['ta9m'].isna().mean() > 0.8:\n",
    "    #     nysm_1H_obs.drop('ta9m', axis=1, inplace=True)\n",
    "\n",
    "    # nysm_1H_obs.dropna(inplace=True)\n",
    "\n",
    "    return nysm_1H_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/GFS_data/fh003/GFS_2018_01_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(nam_df[\"station\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/tmp.0wbSLwJm45/ipykernel_551632/4091016902.py:41: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nysm_1H_obs[\"snow_depth\"].fillna(-999, inplace=True)\n",
      "/tmp/tmp.0wbSLwJm45/ipykernel_551632/4091016902.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  nysm_1H_obs[\"ta9m\"].fillna(-999, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = load_nysm_data()\n",
    "\n",
    "# df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")\n",
    "\n",
    "# stations_ls = ['MANH', 'VOOR', 'HERK', 'ANDE', 'BUFF', 'SCIP', 'GROV', 'LOUI', 'ESSX', 'GABR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"station\"] == \"TUPP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>time_1H</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elev</th>\n",
       "      <th>tair</th>\n",
       "      <th>ta9m</th>\n",
       "      <th>td</th>\n",
       "      <th>relh</th>\n",
       "      <th>srad</th>\n",
       "      <th>pres</th>\n",
       "      <th>mslp</th>\n",
       "      <th>wspd_sonic_mean</th>\n",
       "      <th>wspd_sonic</th>\n",
       "      <th>wmax_sonic</th>\n",
       "      <th>wdir_sonic</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>precip_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80352</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>-25.487619</td>\n",
       "      <td>-24.142130</td>\n",
       "      <td>-28.191757</td>\n",
       "      <td>78.046242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>962.528809</td>\n",
       "      <td>971.364136</td>\n",
       "      <td>1.088154</td>\n",
       "      <td>0.705685</td>\n",
       "      <td>1.105910</td>\n",
       "      <td>245.807907</td>\n",
       "      <td>0.298630</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80353</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2018-01-01 01:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>-24.795410</td>\n",
       "      <td>-24.327789</td>\n",
       "      <td>-27.111542</td>\n",
       "      <td>81.011864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>962.668823</td>\n",
       "      <td>971.318665</td>\n",
       "      <td>1.036897</td>\n",
       "      <td>1.300928</td>\n",
       "      <td>2.555367</td>\n",
       "      <td>250.828094</td>\n",
       "      <td>0.293554</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80354</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>-26.389120</td>\n",
       "      <td>-24.916479</td>\n",
       "      <td>-28.842560</td>\n",
       "      <td>79.731209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>962.469177</td>\n",
       "      <td>971.549988</td>\n",
       "      <td>1.260014</td>\n",
       "      <td>1.012449</td>\n",
       "      <td>2.507221</td>\n",
       "      <td>266.065491</td>\n",
       "      <td>0.295817</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80355</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>-26.480761</td>\n",
       "      <td>-25.120541</td>\n",
       "      <td>-29.141174</td>\n",
       "      <td>78.187363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>962.645325</td>\n",
       "      <td>971.755188</td>\n",
       "      <td>1.440481</td>\n",
       "      <td>1.220518</td>\n",
       "      <td>1.782333</td>\n",
       "      <td>282.729492</td>\n",
       "      <td>0.297063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80356</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>-26.792910</td>\n",
       "      <td>-25.355450</td>\n",
       "      <td>-29.570541</td>\n",
       "      <td>77.275673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>962.633179</td>\n",
       "      <td>971.828857</td>\n",
       "      <td>1.136839</td>\n",
       "      <td>1.308577</td>\n",
       "      <td>2.217070</td>\n",
       "      <td>268.878601</td>\n",
       "      <td>0.297118</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063371</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2024-12-15 19:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063498</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2024-12-15 20:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063625</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2024-12-15 21:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063752</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2024-12-15 22:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063879</th>\n",
       "      <td>TUPP</td>\n",
       "      <td>2024-12-15 23:00:00</td>\n",
       "      <td>44.221279</td>\n",
       "      <td>-74.438263</td>\n",
       "      <td>502.82901</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60990 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station             time_1H        lat        lon       elev  \\\n",
       "80352      TUPP 2018-01-01 00:00:00  44.221279 -74.438263  502.82901   \n",
       "80353      TUPP 2018-01-01 01:00:00  44.221279 -74.438263  502.82901   \n",
       "80354      TUPP 2018-01-01 02:00:00  44.221279 -74.438263  502.82901   \n",
       "80355      TUPP 2018-01-01 03:00:00  44.221279 -74.438263  502.82901   \n",
       "80356      TUPP 2018-01-01 04:00:00  44.221279 -74.438263  502.82901   \n",
       "...         ...                 ...        ...        ...        ...   \n",
       "1063371    TUPP 2024-12-15 19:00:00  44.221279 -74.438263  502.82901   \n",
       "1063498    TUPP 2024-12-15 20:00:00  44.221279 -74.438263  502.82901   \n",
       "1063625    TUPP 2024-12-15 21:00:00  44.221279 -74.438263  502.82901   \n",
       "1063752    TUPP 2024-12-15 22:00:00  44.221279 -74.438263  502.82901   \n",
       "1063879    TUPP 2024-12-15 23:00:00  44.221279 -74.438263  502.82901   \n",
       "\n",
       "              tair        ta9m         td       relh  srad        pres  \\\n",
       "80352   -25.487619  -24.142130 -28.191757  78.046242   0.0  962.528809   \n",
       "80353   -24.795410  -24.327789 -27.111542  81.011864   0.0  962.668823   \n",
       "80354   -26.389120  -24.916479 -28.842560  79.731209   0.0  962.469177   \n",
       "80355   -26.480761  -25.120541 -29.141174  78.187363   0.0  962.645325   \n",
       "80356   -26.792910  -25.355450 -29.570541  77.275673   0.0  962.633179   \n",
       "...            ...         ...        ...        ...   ...         ...   \n",
       "1063371        NaN -999.000000        NaN        NaN   NaN         NaN   \n",
       "1063498        NaN -999.000000        NaN        NaN   NaN         NaN   \n",
       "1063625        NaN -999.000000        NaN        NaN   NaN         NaN   \n",
       "1063752        NaN -999.000000        NaN        NaN   NaN         NaN   \n",
       "1063879        NaN -999.000000        NaN        NaN   NaN         NaN   \n",
       "\n",
       "               mslp  wspd_sonic_mean  wspd_sonic  wmax_sonic  wdir_sonic  \\\n",
       "80352    971.364136         1.088154    0.705685    1.105910  245.807907   \n",
       "80353    971.318665         1.036897    1.300928    2.555367  250.828094   \n",
       "80354    971.549988         1.260014    1.012449    2.507221  266.065491   \n",
       "80355    971.755188         1.440481    1.220518    1.782333  282.729492   \n",
       "80356    971.828857         1.136839    1.308577    2.217070  268.878601   \n",
       "...             ...              ...         ...         ...         ...   \n",
       "1063371         NaN              NaN         NaN         NaN         NaN   \n",
       "1063498         NaN              NaN         NaN         NaN         NaN   \n",
       "1063625         NaN              NaN         NaN         NaN         NaN   \n",
       "1063752         NaN              NaN         NaN         NaN         NaN   \n",
       "1063879         NaN              NaN         NaN         NaN         NaN   \n",
       "\n",
       "         snow_depth  precip_total  \n",
       "80352      0.298630           NaN  \n",
       "80353      0.293554           0.0  \n",
       "80354      0.295817           0.0  \n",
       "80355      0.297063           0.0  \n",
       "80356      0.297118           0.0  \n",
       "...             ...           ...  \n",
       "1063371 -999.000000           0.0  \n",
       "1063498 -999.000000           0.0  \n",
       "1063625 -999.000000           0.0  \n",
       "1063752 -999.000000           0.0  \n",
       "1063879 -999.000000           0.0  \n",
       "\n",
       "[60990 rows x 18 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = read_gfs_data(\"009\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = df[\"lon [degrees]\"].values\n",
    "lats = df[\"lat [degrees]\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"stid\"].isin(stations_ls)]\n",
    "df = df[[\"lat [degrees]\", \"lon [degrees]\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"/home/aevans/nwp_bias/src/landtype/data/first_paper_stations_coords.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# gfs_df = read_gfs_data(\"006\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gfs_df[\"station\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = 6\n",
    "station = \"SOUT\"\n",
    "var = \"t2m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/nam_data/fh001/NAM_2022_04_direct_compare_to_nysm_sites_mask_water.parquet\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_df = read_nam_data(str(fh).zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hrrr_df = hrrr_data.read_hrrr_data(str(fh).zfill(2))\n",
    "\n",
    "# # Filter NYSM data to match valid times from HRRR data\n",
    "# mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "# fh2_, fh4_ = get_more_fh(fh, station, var, mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(mytimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a100_mae = [\n",
    "    0.07,\n",
    "    0.17,\n",
    "]\n",
    "a100_mse = [\n",
    "    0.07,\n",
    "    0.22,\n",
    "]\n",
    "a100_batch = [\n",
    "    1000,\n",
    "    5000,\n",
    "]\n",
    "a100_gpu = [8, 30]\n",
    "a100_runtime = [\n",
    "    timedelta(seconds=24, minutes=16, hours=0),\n",
    "    timedelta(seconds=5, minutes=16, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh200_mae = [0.06, 0.06]\n",
    "gh200_mse = [0.06, 0.07]\n",
    "gh200_batch = [1000, 10000]\n",
    "gh200_gpu = [8, 64]\n",
    "gh200_runtime = [\n",
    "    timedelta(seconds=22, minutes=6, hours=0),\n",
    "    timedelta(seconds=51, minutes=6, hours=0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def plot_runtime_bar_chart(a100_batch, a100_run_time):\n",
    "    # Convert timedelta objects to total minutes\n",
    "    run_time_minutes = [rt.total_seconds() / 60 for rt in a100_run_time]\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the bar chart\n",
    "    ax.bar(a100_batch, run_time_minutes, 1000, color=\"orange\", label=\"Run Time\")\n",
    "\n",
    "    # Adding scatter points with large X markers on top of bars\n",
    "    # ax.scatter(a100_batch, run_time_minutes, color='red', marker='x', s=100, label='Run Time Points')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"Run Time (minutes)\")\n",
    "    ax.set_title(\"Run Time by Batch Size gh200\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_metrics_bar(a100_mae, a100_mse, a100_batch):\n",
    "    # Number of bars\n",
    "    n = len(a100_mae)\n",
    "\n",
    "    # Create an array for the positions of the bars\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(n)\n",
    "\n",
    "    # Plotting the bars\n",
    "    fig, ax = plt.subplots()\n",
    "    bar1 = ax.bar(\n",
    "        index,\n",
    "        a100_mae,\n",
    "        bar_width,\n",
    "    )\n",
    "    # bar2 = ax.bar(index + bar_width, a100_mse, bar_width, label='MSE')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel(\"Batch Size\")\n",
    "    ax.set_ylabel(\"GPU Memory\")\n",
    "    ax.set_ylim(0, 90)\n",
    "    ax.set_title(\"GPU Memory by Batch Size for a100\")\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(a100_batch)\n",
    "    ax.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(a100_batch, a100_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_bar_chart(gh200_batch, gh200_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(a100_gpu, a100_mse, a100_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_bar(gh200_gpu, gh200_mse, gh200_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "color_dict = {\n",
    "    0: \"cyan\",\n",
    "    1: \"blue\",\n",
    "    2: \"yellow\",\n",
    "    3: \"green\",\n",
    "    # 4: 'red',\n",
    "    # 5: 'orange',\n",
    "    # 6: 'purple',\n",
    "    # 7: 'black',\n",
    "    # 8: 'white'\n",
    "}\n",
    "\n",
    "\n",
    "def plurality_plot(df, geovar):\n",
    "    projPC = crs.PlateCarree()\n",
    "    latN = df[\"lat\"].max() + 1\n",
    "    latS = df[\"lat\"].min() - 1\n",
    "    lonW = df[\"lon\"].max() + 1\n",
    "    lonE = df[\"lon\"].min() - 1\n",
    "    cLat = (latN + latS) / 2\n",
    "    cLon = (lonW + lonE) / 2\n",
    "    projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(6, 6), subplot_kw={\"projection\": crs.PlateCarree()}, dpi=400\n",
    "    )\n",
    "    ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    ax.add_feature(cfeature.STATES)\n",
    "    ax.xticklabels_top = False\n",
    "    ax.ylabels_right = False\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        x=df[\"lon\"],\n",
    "        y=df[\"lat\"],\n",
    "        c=df[\"color\"],\n",
    "        s=40,\n",
    "        marker=\"o\",\n",
    "        edgecolor=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "    )\n",
    "    ax.set_title(f\"Mesonet Site {geovar} Clusters\", size=16)\n",
    "    ax.set_xlabel(\"Longitude\", size=14)\n",
    "    ax.set_ylabel(\"Latitude\", size=14)\n",
    "    ax.tick_params(axis=\"x\", labelsize=12)\n",
    "    ax.tick_params(axis=\"y\", labelsize=12)\n",
    "    ax.grid()\n",
    "\n",
    "    # Create legend patches\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=color, label=f\"Category {key}\")\n",
    "        for key, color in color_dict.items()\n",
    "    ]\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    ax.legend(\n",
    "        handles=legend_patches,\n",
    "        loc=\"upper left\",  # Use 'upper left' to anchor the legend in the figure\n",
    "        bbox_to_anchor=(1.1, 1),  # Move the legend outside the plot to the right\n",
    "        borderaxespad=0,  # Adjust the padding between the legend and the axes\n",
    "        title=\"Categories\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/lstm_clusters.csv\")\n",
    "cluster_df[\"lon\"] = lons\n",
    "cluster_df[\"lat\"] = lats\n",
    "cluster_df[\"color\"] = cluster_df[\"elev_cat\"].map(color_dict)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plurality_plot(cluster_df, \"Elevation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as crs\n",
    "import cartopy.feature as cfeature\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(shapefile_path):\n",
    "    # Define a list of colors for each shape file\n",
    "    # colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "    # projPC = crs.PlateCarree()\n",
    "    # latN = df[\"nysm_lat\"].max() + 0.5\n",
    "    # latS = df[\"nysm_lat\"].min() - 0.5\n",
    "    # lonW = df[\"nysm_lon\"].max() + 0.5\n",
    "    # lonE = df[\"nysm_lon\"].min() - 0.5\n",
    "    # cLat = (latN + latS) / 2\n",
    "    # cLon = (lonW + lonE) / 2\n",
    "    # projLcc = crs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(9, 15), subplot_kw={\"projection\": crs.PlateCarree()}\n",
    "    )\n",
    "    # ax.legend()\n",
    "    # # ax.set_extent([lonW, lonE, latS, latN], crs=projPC)\n",
    "    # ax.add_feature(cfeature.LAND)\n",
    "    # ax.add_feature(cfeature.COASTLINE)\n",
    "    # ax.add_feature(cfeature.BORDERS, linestyle=\"--\")\n",
    "    # ax.add_feature(cfeature.LAKES, alpha=0.5)\n",
    "    # ax.add_feature(cfeature.STATES)\n",
    "    # ax.xticklabels_top = False\n",
    "    # ax.ylabels_right = False\n",
    "    # ax.gridlines(\n",
    "    #     crs=crs.PlateCarree(),\n",
    "    #     draw_labels=True,\n",
    "    #     linewidth=2,\n",
    "    #     color=\"black\",\n",
    "    #     alpha=0.5,\n",
    "    #     linestyle=\"--\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"],\n",
    "    #     df[\"nysm_lat\"],\n",
    "    #     c=\"blue\",\n",
    "    #     s=70,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"NYSM Sites\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"nysm_lon\"].iloc[0],\n",
    "    #     df[\"nysm_lat\"].iloc[0],\n",
    "    #     c=\"green\",\n",
    "    #     marker=\"*\",\n",
    "    #     s=400,\n",
    "    #     edgecolors=\"black\",\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label=\"Southold\",\n",
    "    # )\n",
    "\n",
    "    # plt.scatter(\n",
    "    #     df[\"hrrr_lon\"],\n",
    "    #     df[\"hrrr_lat\"],\n",
    "    #     c='orange',\n",
    "    #     s = 70,\n",
    "    #     edgecolors='black',\n",
    "    #     transform=crs.PlateCarree(),\n",
    "    #     zorder=5,\n",
    "    #     label='HRRR'\n",
    "    # )\n",
    "\n",
    "    # # Annotate each point in NYSM\n",
    "    # for i, txt in enumerate(df[\"station\"]):\n",
    "    #     plt.annotate(\n",
    "    #         txt,\n",
    "    #         (df[\"nysm_lon\"].iloc[i], df[\"nysm_lat\"].iloc[i]),\n",
    "    #         textcoords=\"offset points\",\n",
    "    #         xytext=(5, 10),\n",
    "    #         ha=\"center\",\n",
    "    #         fontsize=18,\n",
    "    #     )\n",
    "\n",
    "    # Load the shape file using geopandas\n",
    "    climate_divisions = gpd.read_file(shapefile_path)\n",
    "    # Plot climate divisions from the shape file\n",
    "    climate_divisions.plot(\n",
    "        ax=ax,\n",
    "        edgecolor=\"black\",\n",
    "        facecolor=\"none\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.1, 1), loc=\"upper left\", borderaxespad=0, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"St. Lawrence Valley\",\n",
    "    \"Great Lakes\",\n",
    "    \"Northern Plateau\",\n",
    "    \"Champlain Valley\",\n",
    "    \"Hudson Valley\",\n",
    "    \"Mohawk Valley\",\n",
    "    \"Western Plateau\",\n",
    "    \"Eastern Pleateau\",\n",
    "    \"Coastal\",\n",
    "    \"Central Lakes\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clim_div = sorted(clim_div)\n",
    "image = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xCITE_gif(nysm_clim, fh, clim_div, logo):\n",
    "    # Create your dataframe df_\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define colors dictionary and randomly assign colors\n",
    "    nwp_dict = {0: \"green\", 1: \"red\", 2: \"blue\"}  # NAM  # HRRR  # GFS\n",
    "    nwps_all = [0, 1, 2]\n",
    "\n",
    "    # Randomly assign values from nwps_all to the 'lister'\n",
    "    lister = [random.choice(nwps_all) for _ in df_[\"stid\"]]\n",
    "    df_[\"color\"] = [nwp_dict[value] for value in lister]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    gdf_filtered = gdf_filtered.iloc[20:29]\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in range(len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower right\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-75.0, -72.0, 40.0, 44.0], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for NWP models\n",
    "    nam_patch = mpatches.Patch(color=\"green\", label=\"NAM\")\n",
    "    hrrr_patch = mpatches.Patch(color=\"red\", label=\"HRRR\")\n",
    "    gfs_patch = mpatches.Patch(color=\"blue\", label=\"GFS\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    ax.legend(\n",
    "        handles=[nam_patch, hrrr_patch, gfs_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=12,\n",
    "        title=\"NWP Models\",\n",
    "    )\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"Predicted Most Accurate Model for Hudson Valley,\\n Forecast Hour {fh}\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "    # Load and add the logo to the lower left\n",
    "    logo_img = mpimg.imread(logo)\n",
    "    ax.figure.figimage(\n",
    "        logo_img, 50, 50, zorder=20, alpha=0.5\n",
    "    )  # Adjust (x, y) position as needed\n",
    "\n",
    "    plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/xCITE_gif/mockup_fh{fh}.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in np.arange(1, 13):\n",
    "#     create_xCITE_gif(nysm_clim, i, clim_div, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs_path = (\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/AMS_2025/GFS_t2m_learners_csvs/\"\n",
    ")\n",
    "\n",
    "files = os.listdir(gfs_path)\n",
    "\n",
    "masters = pd.DataFrame()\n",
    "for f in files:\n",
    "    gfs1 = pd.read_csv(f\"{gfs_path}/{f}\")\n",
    "    masters = pd.concat([gfs1, masters])\n",
    "\n",
    "learners = masters[\"station\"].unique()\n",
    "learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_clim\n",
    "nysm_ = nysm_clim.copy()\n",
    "\n",
    "nysm_ = nysm_.rename(columns={\"lat [degrees]\": \"lat\", \"lon [degrees]\": \"lon\"})\n",
    "nysm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "def get_closest_stations(nysm_df, neighbors, target_station, nwp_model):\n",
    "    # Earth's radius in kilometers\n",
    "    EARTH_RADIUS_KM = 6378\n",
    "\n",
    "    lats = nysm_df[\"lat\"].unique()\n",
    "    lons = nysm_df[\"lon\"].unique()\n",
    "\n",
    "    locations_a = pd.DataFrame()\n",
    "    locations_a[\"lat\"] = lats\n",
    "    locations_a[\"lon\"] = lons\n",
    "\n",
    "    for column in locations_a[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    locations_b = locations_a\n",
    "\n",
    "    ball = BallTree(locations_a[[\"lat_rad\", \"lon_rad\"]].values, metric=\"haversine\")\n",
    "\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = neighbors\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "\n",
    "    # Convert distances from radians to kilometers\n",
    "    distances_km = distances * EARTH_RADIUS_KM\n",
    "\n",
    "    # source info to creare a dictionary\n",
    "    indices_list = [indices[x][0:k] for x in range(len(indices))]\n",
    "    distances_list = [distances_km[x][0:k] for x in range(len(distances_km))]\n",
    "    stations = nysm_df[\"stid\"].unique()\n",
    "\n",
    "    # create dictionary\n",
    "    station_dict = {}\n",
    "    for k, _ in enumerate(stations):\n",
    "        station_dict[stations[k]] = (indices_list[k], distances_list[k])\n",
    "\n",
    "    utilize_ls = []\n",
    "    vals, dists = station_dict.get(target_station)\n",
    "\n",
    "    if nwp_model == \"GFS\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 30 and len(utilize_ls) < 5:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"NAM\":\n",
    "        utilize_ls.append(target_station)\n",
    "        for v, d in zip(vals, dists):\n",
    "            if d >= 12 and len(utilize_ls) < 4:\n",
    "                x = stations[v]\n",
    "                utilize_ls.append(x)\n",
    "\n",
    "    if nwp_model == \"HRRR\":\n",
    "        for v, d in zip(vals, dists):\n",
    "            x = stations[v]\n",
    "            utilize_ls.append(x)\n",
    "\n",
    "    return utilize_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = nysm_clim[\"stid\"].unique()\n",
    "\n",
    "elev_delta = []\n",
    "\n",
    "for s in stations:\n",
    "    print(s)\n",
    "    utilize_ls = get_closest_stations(nysm_, 15, s, \"GFS\")\n",
    "    selection = nysm_[nysm_[\"stid\"].isin(utilize_ls)]\n",
    "    # Find the maximum value in 'col1'\n",
    "    max_value = selection[\"elevation [m]\"].max()\n",
    "    min_value = selection[\"elevation [m]\"].min()\n",
    "    delta = max_value - min_value\n",
    "    elev_delta.append(delta)\n",
    "elev_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners(nysm_clim, clim_div, learners):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - fh: Forecast hour.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=250,\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # # Save the figure\n",
    "    # plt.savefig(f\"/home/aevans/nwp_bias/src/landtype/data/xCITE_gif/mockup_fh{fh}.png\")\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gfs_learners(nysm_clim, clim_div, learners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta):\n",
    "    \"\"\"\n",
    "    Create a GIF frame showing NWP bias correction with stations colored by their inclusion in the learners list.\n",
    "\n",
    "    Parameters:\n",
    "    - nysm_clim: DataFrame containing station data.\n",
    "    - clim_div: List of climate division names.\n",
    "    - learners: List of station IDs classified as learners.\n",
    "    - elev_delta: List containing the elevation delta values to determine scatter point size.\n",
    "    \"\"\"\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Define color mapping based on whether the station is in the learners list\n",
    "    df_[\"color\"] = [\"green\" if stid in learners else \"black\" for stid in df_[\"stid\"]]\n",
    "\n",
    "    # Ensure elev_delta is the same length as df_\n",
    "    if len(elev_delta) != len(df_):\n",
    "        raise ValueError(\n",
    "            \"Length of elev_delta must match the number of stations in the DataFrame\"\n",
    "        )\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    subset = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "    gdf_filtered = subset.copy()\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in np.arange(0, len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"lower left\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    legend1.set_title(\n",
    "        \"Climate Divisions\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.1], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points with sizes based on 'elev_delta'\n",
    "    ax.scatter(\n",
    "        df_[\"lon [degrees]\"],\n",
    "        df_[\"lat [degrees]\"],\n",
    "        c=df_[\"color\"],\n",
    "        s=elev_delta,  # Size of scatter points based on elev_delta\n",
    "        edgecolors=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "    )\n",
    "\n",
    "    # Create custom legend for learner status\n",
    "    learner_patch = mpatches.Patch(color=\"green\", label=\"Learner\")\n",
    "    non_learner_patch = mpatches.Patch(color=\"black\", label=\"Non-Learner\")\n",
    "\n",
    "    # Add second legend to the plot\n",
    "    legend = ax.legend(\n",
    "        handles=[learner_patch, non_learner_patch],\n",
    "        loc=\"upper left\",\n",
    "        fontsize=18,\n",
    "        title=\"Station Classification\",\n",
    "    )\n",
    "    legend.set_title(\n",
    "        \"Station Classification\", prop={\"size\": 18}\n",
    "    )  # Custom font size for the title\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"GFS T2M Error : NYSM Stations that Can Learn\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_gfs_learners_delta(nysm_clim, clim_div, learners, elev_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
