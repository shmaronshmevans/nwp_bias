{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import statistics as st\n",
    "from datetime import datetime\n",
    "import cartopy.crs as crs\n",
    "import geopandas as gpd\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldf = pd.DataFrame()\n",
    "# for i in np.arange(9, 10):\n",
    "#     model_path_LSTM = f\"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/hrrr_prospectus/LOUI/LOUI_fh{i}_tp_HRRR_ml_output_linear.parquet\"\n",
    "\n",
    "#     # Read parquet file\n",
    "#     ldf_ = pd.read_parquet(model_path_LSTM)\n",
    "\n",
    "#     # Concatenate with the main DataFrame\n",
    "#     ldf = pd.concat([ldf_, ldf], ignore_index=True)\n",
    "#     print(ldf.shape)\n",
    "\n",
    "# # Sort by valid_time\n",
    "# ldf = ldf.sort_values(by=\"valid_time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20250502/OLEA/OLEA_fh12_t2m_HRRR_ml_output_og_300.parquet\"\n",
    ")\n",
    "# for c in ldf.columns:\n",
    "#     if c != \"valid_time\":\n",
    "#         ldf[c] = ldf[c] / 3000\n",
    "# ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ldf[\"diff\"])\n",
    "plt.ylim(-15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.data import nysm_data\n",
    "from src.data import gfs_data\n",
    "from src.data import nam_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df = nysm_data.load_nysm_data(gfs=False)\n",
    "# nysm_df = nysm_df.rename(columns={'time_1H':'valid_time'})\n",
    "# gfs_df = gfs_data.read_gfs_data('012')\n",
    "station = \"Deleware\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nysm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.dropna(inplace=True)\n",
    "ldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_fold(df, fold):\n",
    "    length = len(df)\n",
    "    test_len = int(length * 0.2)\n",
    "    df_train = pd.DataFrame()\n",
    "\n",
    "    for n in np.arange(0, 5):\n",
    "        if n != fold:\n",
    "            df1 = df.iloc[int(0.2 * n * length) : int(0.2 * (n + 1) * length)]\n",
    "            df_train = pd.concat([df_train, df1])\n",
    "        else:\n",
    "            df_test = df.iloc[int(0.2 * n * length) : int(0.2 * (n + 1) * length)]\n",
    "            f = int(len(df_test) * 0.5)\n",
    "            df_val = df_test.iloc[:f]\n",
    "            df_test = df_test[-f:]\n",
    "\n",
    "    return df_train[\"valid_time\"], df_test[\"valid_time\"], df_val[\"valid_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldf[\"valid_time\"] = pd.to_datetime(ldf[\"valid_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test, df_val = which_fold(ldf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_start = df_test.iloc[0]\n",
    "test_set_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_finish = df_test.iloc[-1]\n",
    "test_set_finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_filter(ldf, time1, time2):\n",
    "    ldf = ldf[ldf[\"valid_time\"] > time1]\n",
    "    ldf = ldf[ldf[\"valid_time\"] < time2]\n",
    "\n",
    "    return ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def met_output(df, station, fh):\n",
    "    fig, ax = plt.subplots(figsize=(24, 6))\n",
    "    x = df[\"valid_time\"]\n",
    "\n",
    "    # Convert datetime values to numerical values\n",
    "    x_numeric = mdates.date2num(x)\n",
    "\n",
    "    # Assuming your timestamps are in a datetime64 format\n",
    "    day_mask = (x.dt.hour >= 6) & (\n",
    "        x.dt.hour < 18\n",
    "    )  # Adjust the hours based on your day/night definition\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"u_total_{station}\"]),\n",
    "        c=\"mediumseagreen\",\n",
    "        linewidth=3,\n",
    "        label=\"NAM Prediction\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[\"reforecast\"]),\n",
    "        c=\"red\",\n",
    "        linewidth=3,\n",
    "        linestyle=\"--\",  # Dashed line\n",
    "        label=\"LSTM Reforecast\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[f\"wspd_sonic_mean_{station}\"]),\n",
    "        c=\"black\",\n",
    "        linewidth=1,\n",
    "        alpha=0.9,\n",
    "        label=\"NYSM Observation\",\n",
    "    )\n",
    "\n",
    "    # Fill daytime hours with white color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=day_mask, color=\"white\", alpha=0.5, label=\"Daytime\"\n",
    "    )\n",
    "\n",
    "    # Fill nighttime hours with grey color\n",
    "    ax.fill_between(\n",
    "        x_numeric, 0, 10, where=~day_mask, color=\"grey\", alpha=0.2, label=\"Nighttime\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"GFS Prediction v NYSM Observation: {station}: FH{fh}\", fontsize=28)\n",
    "    # plt.ylim(-5, 5.)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = nysm_df[\"station\"].values\n",
    "lats = nysm_df[\"lat\"].values\n",
    "lons = nysm_df[\"lon\"].values\n",
    "\n",
    "station_coords = {k: (lat, lon) for k, lat, lon in zip(keys, lats, lons)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1 = datetime(2024, 3, 1, 0, 0, 0)\n",
    "time2 = datetime(2024, 12, 31, 23, 0, 0)\n",
    "\n",
    "ldf = date_filter(ldf, time1, time2)\n",
    "nysm_df = date_filter(nysm_df, time1, time2)\n",
    "# gfs_df = date_filter(gfs_df, time1, time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ml_output(\n",
    "    df,\n",
    "    fold,\n",
    "    station,\n",
    "    test_set_start,\n",
    "    test_set_finish,\n",
    "    fill_between_min,\n",
    "    fill_between_max,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(24, 6))\n",
    "    x = df[\"valid_time\"]\n",
    "\n",
    "    # Convert datetime values to numerical values\n",
    "    x_numeric = mdates.date2num(x)\n",
    "\n",
    "    # Assuming your timestamps are in a datetime64 format\n",
    "    day_mask = (x.dt.hour >= 6) & (\n",
    "        x.dt.hour < 18\n",
    "    )  # Adjust the hours based on your day/night definition\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[\"target_error_lead_0\"]),\n",
    "        c=\"black\",\n",
    "        linewidth=1,\n",
    "        label=\"Target\",\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        np.array(x),\n",
    "        np.array(df[\"Model forecast\"]),\n",
    "        c=\"red\",\n",
    "        linewidth=3,\n",
    "        alpha=0.7,\n",
    "        label=\"LSTM Output\",\n",
    "    )\n",
    "    # plt.plot(\n",
    "    #     np.array(x),\n",
    "    #     np.array(df2[\"Model forecast\"]),\n",
    "    #     c=\"blue\",\n",
    "    #     linewidth=3,\n",
    "    #     alpha=0.7,\n",
    "    #     label=\"LSTM Output: alpha2\",\n",
    "    # )\n",
    "    # plt.plot(\n",
    "    #     np.array(x),\n",
    "    #     np.array(df3[\"Model forecast\"]),\n",
    "    #     c=\"darkviolet\",\n",
    "    #     linewidth=3,\n",
    "    #     alpha=0.7,\n",
    "    #     label=\"LSTM Output:alpha3\",\n",
    "    # )\n",
    "\n",
    "    # Fill daytime hours with white color\n",
    "    ax.fill_between(\n",
    "        x_numeric,\n",
    "        fill_between_min,\n",
    "        fill_between_max,\n",
    "        where=day_mask,\n",
    "        color=\"white\",\n",
    "        alpha=0.5,\n",
    "        label=\"Daytime\",\n",
    "    )\n",
    "\n",
    "    # Fill nighttime hours with grey color\n",
    "    ax.fill_between(\n",
    "        x_numeric,\n",
    "        fill_between_min,\n",
    "        fill_between_max,\n",
    "        where=~day_mask,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=\"Nighttime\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"NAM Temperature Error LSTM Output v Target: {station}\", fontsize=28)\n",
    "    # plt.ylim(-5, 5.)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf[\"diff\"] = ldf.iloc[:, 0] - ldf.iloc[:, 1]\n",
    "# ldf[\"target_error_lead_0\"] = ldf[\"target_error\"] *2\n",
    "ldf[\"Model forecast\"] = ldf[\"Model forecast\"] * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = st.mean(abs(ldf[\"diff\"]))\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_s = st.mean(ldf[\"diff\"] ** 2)\n",
    "mean_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet('/home/aevans/nwp_bias/src/machine_learning/data/oksm/hrrr_data/fh01/HRRR_2018_01_direct_compare_to_oksm_sites_mask_water.parquet').reset_index()\n",
    "# df = df[df['station']=='BRIS']\n",
    "# plt.plot(df['u_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_parquet('/home/aevans/nwp_bias/data/oksm/oksm_1H_obs_2024.parquet').reset_index()\n",
    "# df2 = df2[df2['station']=='BRIS']\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ml_output(ldf, 4, station, test_set_start, test_set_finish, -5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fh_drift(mae_ls, clim_divs, nwp_model, metvar):\n",
    "    \"\"\"\n",
    "    Plots Mean Absolute Error (MAE) as a function of forecast hour (fh) for multiple climate divisions.\n",
    "\n",
    "    Parameters:\n",
    "        mae_dict (dict): Dictionary where keys are climate divisions and values are lists of MAE values.\n",
    "        fh (list): Forecast hours.\n",
    "        station (str): Station identifier.\n",
    "        nwp_model (str): Numerical weather prediction model name.\n",
    "        metvar (str): Meteorological variable.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    fh = np.arange(1, 19)\n",
    "\n",
    "    colors = [\n",
    "        \"blue\",\n",
    "        \"green\",\n",
    "        \"red\",\n",
    "        \"orange\",\n",
    "        \"darkviolet\",\n",
    "        \"brown\",\n",
    "        \"black\",\n",
    "        \"gray\",\n",
    "        \"darkcyan\",\n",
    "        \"deeppink\",\n",
    "    ]  # Color palette\n",
    "    for i, _ in enumerate(mae_ls):\n",
    "        color = colors[i % len(colors)]  # Cycle through colors\n",
    "        plt.plot(\n",
    "            fh, mae_ls[i], label=clim_divs[i], marker=\"o\", linestyle=\"-\", color=color\n",
    "        )\n",
    "        plt.scatter(fh, mae_ls[i], marker=\"o\", s=100, color=color)\n",
    "\n",
    "        # # Annotate points\n",
    "        # for j, txt in enumerate(mae_ls[i]):\n",
    "        #     plt.annotate(\n",
    "        #         f\"{txt:.2f}\",\n",
    "        #         (fh[j], mae_ls[i][j]),\n",
    "        #         textcoords=\"offset points\",\n",
    "        #         xytext=(0, 10),\n",
    "        #         ha=\"center\",\n",
    "        #         fontsize=10,\n",
    "        #         color=color,\n",
    "        #     )\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.xlabel(\"Forecast Hour (FH)\", fontsize=16)\n",
    "    plt.ylabel(\"Mean Absolute Error (MAE)\", fontsize=16)\n",
    "    plt.title(\n",
    "        f\"MAE as a Function of Forecast Hour\\n{nwp_model}, {metvar}-Error\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    plt.legend(title=\"Climate Division\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Ensure x-ticks are integers\n",
    "    plt.xticks(ticks=range(int(min(fh)), int(max(fh)) + 1), fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_mae_boxplot(wind_df_list, temp_df_list, precip_df_list, climate_divisions):\n",
    "    \"\"\"\n",
    "    Creates a grouped boxplot where each climate division has three boxplots\n",
    "    representing wind error, temperature error, and precipitation error.\n",
    "\n",
    "    Parameters:\n",
    "        wind_df_list (list of pd.DataFrame): List of DataFrames containing 'mae' for wind error.\n",
    "        temp_df_list (list of pd.DataFrame): List of DataFrames containing 'mae' for temperature error.\n",
    "        precip_df_list (list of pd.DataFrame): List of DataFrames containing 'mae' for precipitation error.\n",
    "        climate_divisions (list of str): Names of the corresponding climate divisions.\n",
    "    \"\"\"\n",
    "    # Create a list to store the MAE values with their associated climate division and variable type\n",
    "    plot_data = []\n",
    "\n",
    "    for clim_div, wind_df, temp_df, precip_df in zip(\n",
    "        climate_divisions, wind_df_list, temp_df_list, precip_df_list\n",
    "    ):\n",
    "        if \"mae\" in wind_df.columns:\n",
    "            for mae_val in wind_df[\"mae\"]:\n",
    "                plot_data.append(\n",
    "                    {\"Climate Division\": clim_div, \"MAE\": mae_val, \"Variable\": \"Wind\"}\n",
    "                )\n",
    "\n",
    "        if \"mae\" in temp_df.columns:\n",
    "            for mae_val in temp_df[\"mae\"]:\n",
    "                plot_data.append(\n",
    "                    {\n",
    "                        \"Climate Division\": clim_div,\n",
    "                        \"MAE\": mae_val,\n",
    "                        \"Variable\": \"Temperature\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if \"mae\" in precip_df.columns:\n",
    "            for mae_val in precip_df[\"mae\"]:\n",
    "                plot_data.append(\n",
    "                    {\n",
    "                        \"Climate Division\": clim_div,\n",
    "                        \"MAE\": mae_val,\n",
    "                        \"Variable\": \"Precipitation\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Create the boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(\n",
    "        x=\"Climate Division\", y=\"MAE\", hue=\"Variable\", data=plot_df, palette=\"Set2\"\n",
    "    )\n",
    "\n",
    "    # Format the plot\n",
    "    plt.xlabel(\"Climate Division\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Absolute Error (MAE)\", fontsize=14)\n",
    "    plt.title(\n",
    "        \"MAE Distribution Across Climate Divisions for Forecast Error\", fontsize=16\n",
    "    )\n",
    "    plt.xticks(rotation=45)  # Rotate labels for readability\n",
    "    plt.legend(title=\"Variable\", fontsize=12, loc=\"upper right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"/home/aevans/nwp_bias/src/machine_learning/notebooks/clim_div_boxplots.png\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_info_df = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/lstm_clusters.csv\")\n",
    "\n",
    "lulc_dict = geo_info_df.set_index(\"station\")[\"lulc_cat\"].to_dict()\n",
    "elev_dict = geo_info_df.set_index(\"station\")[\"elev_cat\"].to_dict()\n",
    "slope_dict = geo_info_df.set_index(\"station\")[\"slope_cat\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_map(df, lulc_dict, elev_dict, slope_dict):\n",
    "    df[\"lulc\"] = df[\"station\"].map(lulc_dict)\n",
    "    df[\"elev\"] = df[\"station\"].map(elev_dict)\n",
    "    df[\"slope\"] = df[\"station\"].map(slope_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_path = \"/home/aevans/nwp_bias/src/machine_learning/data/error_visuals\"\n",
    "\n",
    "error_dir = os.listdir(error_path)\n",
    "error_dir = sorted(error_dir)\n",
    "\n",
    "temp_master_ls = []\n",
    "wind_master_ls = []\n",
    "tp_master_ls = []\n",
    "\n",
    "temp_df_ls = []\n",
    "wind_df_ls = []\n",
    "precip_df_ls = []\n",
    "\n",
    "for d in error_dir:\n",
    "    print(d)\n",
    "    error_df1 = pd.read_parquet(\n",
    "        f\"{error_path}/{d}/{d}_tp_error_metrics_master_normalized.parquet\"\n",
    "    )\n",
    "    error_df2 = pd.read_parquet(\n",
    "        f\"{error_path}/{d}/{d}_u_total_error_metrics_master_normalized.parquet\"\n",
    "    )\n",
    "    error_df3 = pd.read_parquet(\n",
    "        f\"{error_path}/{d}/{d}_t2m_error_metrics_master_normalized.parquet\"\n",
    "    )\n",
    "    # Group by forecast hour (fh) and compute the mean MAE\n",
    "    mean_mae_by_fh1 = error_df1.groupby(\"fh\")[\"mae\"].mean().reset_index()\n",
    "    mean_mae_by_fh2 = error_df2.groupby(\"fh\")[\"mae\"].mean().reset_index()\n",
    "    mean_mae_by_fh3 = error_df3.groupby(\"fh\")[\"mae\"].mean().reset_index()\n",
    "\n",
    "    temp_df_ls.append(error_df3.reset_index())\n",
    "    wind_df_ls.append(error_df2.reset_index())\n",
    "    precip_df_ls.append(error_df1.reset_index())\n",
    "\n",
    "    mae1 = mean_mae_by_fh1[\"mae\"].values\n",
    "    mae2 = mean_mae_by_fh2[\"mae\"].values\n",
    "    mae3 = mean_mae_by_fh3[\"mae\"].values\n",
    "\n",
    "    temp_master_ls.append(mae3)\n",
    "    wind_master_ls.append(mae2)\n",
    "    tp_master_ls.append(mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df = pd.concat(precip_df_ls)\n",
    "st.mean(mae_df[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = mae_df.groupby(\"station\")[\"mae\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.DataFrame({\"station\": grouped.index, \"mae\": grouped.values})\n",
    "\n",
    "# Add lat/lon from your station_coords dictionary\n",
    "grouped_df[\"lat_lon\"] = grouped_df[\"station\"].map(station_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df[[\"lat\", \"lon\"]] = pd.DataFrame(\n",
    "    grouped_df[\"lat_lon\"].tolist(), index=grouped_df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clim_div = [\n",
    "    \"St. Lawrence Valley\",\n",
    "    \"Great Lakes\",\n",
    "    \"Northern Plateau\",\n",
    "    \"Champlain Valley\",\n",
    "    \"Hudson Valley\",\n",
    "    \"Mohawk Valley\",\n",
    "    \"Western Plateau\",\n",
    "    \"Eastern Pleateau\",\n",
    "    \"Coastal\",\n",
    "    \"Central Lakes\",\n",
    "]\n",
    "# clim_div = sorted(clim_div)\n",
    "image = \"/home/aevans/nwp_bias/src/landtype/data/NCEI_logo.png\"\n",
    "nysm_clim = pd.read_csv(\"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_xCITE_gif(grouped_df, clim_div=clim_div, nysm_clim=nysm_clim, logo=image):\n",
    "    # Create your dataframe df_\n",
    "    df_ = nysm_clim.copy()\n",
    "\n",
    "    # Create plot\n",
    "    fig = plt.figure(figsize=(24, 16))\n",
    "    ax = fig.add_subplot(\n",
    "        1,\n",
    "        1,\n",
    "        1,\n",
    "        projection=crs.LambertConformal(\n",
    "            central_longitude=-75.0, standard_parallels=(49, 77)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Load the shapefile for boundaries\n",
    "    shapefile_path = \"/home/aevans/nwp_bias/src/machine_learning/notebooks/data/GIS.OFFICIAL_CLIM_DIVISIONS.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "    ny_state_boundaries_path = \"/home/aevans/nwp_bias/src/landtype/data/State.shx\"\n",
    "    ny_state_boundaries_geo = gpd.read_file(ny_state_boundaries_path).to_crs(epsg=4326)\n",
    "\n",
    "    ny_bbox = ny_state_boundaries_geo.total_bounds\n",
    "    gdf_filtered = gdf.cx[ny_bbox[0] : ny_bbox[2], ny_bbox[1] : ny_bbox[3]]\n",
    "    gdf_filtered = pd.concat([gdf_filtered.iloc[20:29], gdf_filtered.iloc[[32]]])\n",
    "\n",
    "    # Create a categorical column for plotting\n",
    "    gdf_filtered[\"category\"] = np.arange(len(gdf_filtered))\n",
    "\n",
    "    # Plot shapefile with climate divisions (remove the automatic legend)\n",
    "    gdf_filtered.plot(\n",
    "        ax=ax,\n",
    "        transform=crs.PlateCarree(),\n",
    "        column=\"category\",\n",
    "        cmap=\"tab10\",\n",
    "        alpha=0.3,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Create legend for climate divisions using the colors from the 'tab10' colormap and labels from 'clim_div'\n",
    "    division_patches = [\n",
    "        mpatches.Patch(\n",
    "            color=plt.cm.tab10(i / len(gdf_filtered)), alpha=0.3, label=clim_div[i]\n",
    "        )\n",
    "        for i in range(len(gdf_filtered))\n",
    "    ]\n",
    "\n",
    "    # Add the climate divisions legend\n",
    "    legend1 = ax.legend(\n",
    "        handles=division_patches,\n",
    "        loc=\"upper right\",\n",
    "        title=\"Climate Divisions\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "    ax.add_artist(legend1)  # Ensure the first legend is added to the plot\n",
    "\n",
    "    # Set extent for the plot\n",
    "    ax.set_extent([-80.0, -72.0, 40.0, 45.5], crs=crs.PlateCarree())\n",
    "\n",
    "    # Add features\n",
    "    ax.add_feature(cfeature.BORDERS.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(\"50m\"), linestyle=\":\", zorder=1)\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(\"50m\"), zorder=1)\n",
    "    ax.gridlines(\n",
    "        crs=crs.PlateCarree(),\n",
    "        draw_labels=True,\n",
    "        linewidth=2,\n",
    "        color=\"black\",\n",
    "        alpha=0.5,\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    # Annotate scatter points with station IDs\n",
    "    for i, row in df_.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"stid\"],\n",
    "            (row[\"lon [degrees]\"], row[\"lat [degrees]\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(0, 7),\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            color=\"black\",\n",
    "            transform=crs.PlateCarree(),\n",
    "        )\n",
    "\n",
    "    # Plot scatter points\n",
    "    sc = ax.scatter(\n",
    "        grouped_df[\"lon\"],\n",
    "        grouped_df[\"lat\"],\n",
    "        s=grouped_df[\"mae\"] * 1000,\n",
    "        c=grouped_df[\"mae\"],\n",
    "        cmap=\"viridis\",\n",
    "        edgecolor=\"black\",\n",
    "        transform=crs.PlateCarree(),\n",
    "        zorder=10,\n",
    "        vmin=0,\n",
    "        vmax=0.8,\n",
    "    )\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(sc, ax=ax, orientation=\"vertical\", shrink=0.6, pad=0.05)\n",
    "    cbar.set_label(\"MAE\", fontsize=14)\n",
    "\n",
    "    # Add plot title\n",
    "    plt.title(\n",
    "        f\"NCEI New York State Climate Divisions\",\n",
    "        fontsize=24,\n",
    "    )\n",
    "    # Load and add the logo to the lower left\n",
    "    logo_img = mpimg.imread(logo)\n",
    "    ax.figure.figimage(\n",
    "        logo_img, 50, 50, zorder=20, alpha=0.5\n",
    "    )  # Adjust (x, y) position as needed\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    grouped_df[\"lon\"],\n",
    "    grouped_df[\"lat\"],\n",
    "    s=grouped_df[\"mae\"],\n",
    "    c=grouped_df[\"mae\"],\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.colorbar(label=\"MAE\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.title(\"Station MAE colored by value, Precip\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_xCITE_gif(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fh_drift(wind_master_ls, error_dir, \"HRRR\", \"Wind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mae_boxplot(temp_df_ls, wind_df_ls, precip_df_ls, error_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mae_boxplot_geo(wind_df_list, temp_df_list, precip_df_list, geo):\n",
    "    \"\"\"\n",
    "    Creates a grouped boxplot where each climate division has three boxplots\n",
    "    representing wind error, temperature error, and precipitation error.\n",
    "\n",
    "    Parameters:\n",
    "        wind_df_list (list of pd.DataFrame): List of DataFrames containing 'mae' for wind error.\n",
    "        temp_df_list (list of pd.DataFrame): List of DataFrames containing 'mae' for temperature error.\n",
    "        precip_df_list (list of pd.DataFrame): List of DataFrames containing 'mae' for precipitation error.\n",
    "        climate_divisions (list of str): Names of the corresponding climate divisions.\n",
    "    \"\"\"\n",
    "    # Create a list to store the MAE values with their associated climate division and variable type\n",
    "    plot_data = []\n",
    "\n",
    "    for clim_div, wind_df, temp_df, precip_df in zip(\n",
    "        geo, wind_df_list, temp_df_list, precip_df_list\n",
    "    ):\n",
    "        if \"mae\" in wind_df.columns:\n",
    "            for mae_val in wind_df[\"mae\"]:\n",
    "                plot_data.append(\n",
    "                    {\"Climate Division\": clim_div, \"MAE\": mae_val, \"Variable\": \"Wind\"}\n",
    "                )\n",
    "\n",
    "        if \"mae\" in temp_df.columns:\n",
    "            for mae_val in temp_df[\"mae\"]:\n",
    "                plot_data.append(\n",
    "                    {\n",
    "                        \"Climate Division\": clim_div,\n",
    "                        \"MAE\": mae_val,\n",
    "                        \"Variable\": \"Temperature\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        if \"mae\" in precip_df.columns:\n",
    "            for mae_val in precip_df[\"mae\"]:\n",
    "                plot_data.append(\n",
    "                    {\n",
    "                        \"Climate Division\": clim_div,\n",
    "                        \"MAE\": mae_val,\n",
    "                        \"Variable\": \"Precipitation\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Create the boxplot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.boxplot(\n",
    "        x=\"Climate Division\", y=\"MAE\", hue=\"Variable\", data=plot_df, palette=\"Set2\"\n",
    "    )\n",
    "\n",
    "    # Format the plot\n",
    "    plt.xlabel(\"Climate Division\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Absolute Error (MAE)\", fontsize=14)\n",
    "    plt.title(\n",
    "        \"MAE Distribution Across Climate Divisions for Forecast Error\", fontsize=16\n",
    "    )\n",
    "    plt.xticks(rotation=45)  # Rotate labels for readability\n",
    "    plt.legend(title=\"Variable\", fontsize=12, loc=\"upper left\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_path = \"/home/aevans/nwp_bias/src/machine_learning/data/error_visuals\"\n",
    "\n",
    "error_dir = os.listdir(error_path)\n",
    "error_dir = sorted(error_dir)\n",
    "\n",
    "temp_master_ls = []\n",
    "wind_master_ls = []\n",
    "tp_master_ls = []\n",
    "\n",
    "temp_df_ls = []\n",
    "wind_df_ls = []\n",
    "precip_df_ls = []\n",
    "\n",
    "for d in error_dir:\n",
    "    print(d)\n",
    "    if d not in [\"Northern Plateau\"]:\n",
    "        error_df1 = pd.read_parquet(\n",
    "            f\"{error_path}/{d}/{d}_tp_error_metrics_master.parquet\"\n",
    "        ).reset_index()\n",
    "        error_df2 = pd.read_parquet(\n",
    "            f\"{error_path}/{d}/{d}_u_total_error_metrics_master.parquet\"\n",
    "        ).reset_index()\n",
    "        error_df3 = pd.read_parquet(\n",
    "            f\"{error_path}/{d}/{d}_t2m_error_metrics_master.parquet\"\n",
    "        ).reset_index()\n",
    "\n",
    "        error_df1 = geo_map(error_df1, lulc_dict, elev_dict, slope_dict)\n",
    "        error_df2 = geo_map(error_df2, lulc_dict, elev_dict, slope_dict)\n",
    "        error_df3 = geo_map(error_df3, lulc_dict, elev_dict, slope_dict)\n",
    "\n",
    "        # Group by forecast hour (fh) and compute the mean MAE\n",
    "        mean_mae_by_fh1 = error_df1.groupby(\"lulc\")[\"mae\"].mean().reset_index()\n",
    "        mean_mae_by_fh2 = error_df2.groupby(\"lulc\")[\"mae\"].mean().reset_index()\n",
    "        mean_mae_by_fh3 = error_df3.groupby(\"lulc\")[\"mae\"].mean().reset_index()\n",
    "\n",
    "        temp_df_ls.append(error_df3.reset_index())\n",
    "        wind_df_ls.append(error_df2.reset_index())\n",
    "        precip_df_ls.append(error_df1.reset_index())\n",
    "\n",
    "        mae1 = mean_mae_by_fh1[\"mae\"].values\n",
    "        mae2 = mean_mae_by_fh2[\"mae\"].values\n",
    "        mae3 = mean_mae_by_fh3[\"mae\"].values\n",
    "\n",
    "        temp_master_ls.append(mae3)\n",
    "        wind_master_ls.append(mae2)\n",
    "        tp_master_ls.append(mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mae_boxplot_geo(\n",
    "    wind_df_ls, temp_df_ls, precip_df_ls, geo_info_df[\"lulc_cat\"].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# met_output(nysm_df, \"DELE\", 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in full_df.columns:\n",
    "#     if re.search(\n",
    "#         f\"{station}|valid|Model forecast|Abs_err\",\n",
    "#         k,\n",
    "#     ):\n",
    "#         continue\n",
    "#     else:\n",
    "# full_df = full_df.drop(columns=[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_len = int(len(full_df['valid_time'])*0.2)\n",
    "# full_df = full_df.iloc[0:test_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_month(df, col):\n",
    "    df = df[df[col] > -999]\n",
    "    GB = df.groupby([(df.valid_time.dt.month), (df.valid_time.dt.year)])[col].mean()\n",
    "    the_list = GB.tolist()\n",
    "    fig, ax = plt.subplots(figsize=(21, 6))\n",
    "    x = np.arange(1, len(the_list) + 1)\n",
    "    plt.bar(x, the_list)\n",
    "    ax.set_xticklabels([2018, 2019, 2020, 2021, 2022, 2023])\n",
    "    ax.set_xticks(np.arange(1, len(the_list) + 1, int((len(the_list) + 1) / 6)))\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_by_month = groupby_month(ldf, \"diff\")\n",
    "err_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_time(df, col):\n",
    "    df = df[df[col] > -999]\n",
    "    GB = df.groupby([(df.valid_time.dt.hour)])[col].mean()\n",
    "    the_list = GB.tolist()\n",
    "    fig, ax = plt.subplots(figsize=(21, 6))\n",
    "    x = np.arange(0, len(the_list))\n",
    "    plt.bar(x, the_list, color=\"g\")\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_by_time = groupby_time(ldf, \"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "\n",
    "def groupby_month_total(df, col):\n",
    "    df = df[df[col] > -999]\n",
    "    GB = df.groupby([(df.valid_time.dt.month)])[col].mean()\n",
    "    the_list = GB.tolist()\n",
    "    fig, ax = plt.subplots(figsize=(21, 6))\n",
    "    x = np.arange(0, len(the_list))\n",
    "\n",
    "    # Get a colormap\n",
    "    cmap = plt.get_cmap(\"RdBu\")\n",
    "    # Normalize your data to map to the colormap\n",
    "    norm = plt.Normalize(min(the_list), max(the_list))\n",
    "    colors = cmap(norm(the_list))\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.bar(x, the_list, color=colors)\n",
    "    ax.set_xticks(x)  # Set x-ticks to be at the positions of the months\n",
    "    month_labels = [calendar.month_name[month] for month in GB.index]\n",
    "    ax.set_xticklabels(month_labels)  # Label x-ticks with month numbers\n",
    "\n",
    "    plt.show()\n",
    "    return the_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_month_total(ldf, \"diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myround(x, base):\n",
    "    return base * round(x / base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get unique values\n",
    "def unique(list1):\n",
    "    # initialize a null list\n",
    "    unique_list = []\n",
    "\n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_bucket(full_df, met_col, rounded_base):\n",
    "    temps = []\n",
    "    for i, _ in enumerate(full_df[met_col]):\n",
    "        rounded = myround(full_df[met_col].iloc[i], rounded_base)\n",
    "        temps.append(rounded)\n",
    "\n",
    "    unique_temps = unique(temps)\n",
    "\n",
    "    zeros = np.zeros(len(unique_temps))\n",
    "    rs = np.resize(zeros, (len(unique_temps), len(unique_temps)))\n",
    "\n",
    "    temp_df = pd.DataFrame(\n",
    "        data=rs, index=[np.arange(len(unique_temps))], columns=sorted(unique_temps)\n",
    "    )\n",
    "\n",
    "    for i, _ in enumerate(full_df[met_col]):\n",
    "        rounded = myround(full_df[met_col].iloc[i], rounded_base)\n",
    "        err = full_df[\"Abs_err\"].iloc[i]\n",
    "        temp_df[rounded].iloc[0] = abs(err) + abs(temp_df[rounded].iloc[0])\n",
    "        temp_df[rounded].iloc[-1] += 1\n",
    "\n",
    "    instances = temp_df.iloc[-1]\n",
    "    temp_df = temp_df.iloc[0]\n",
    "\n",
    "    return temp_df, instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_buckets(temp_df, instances, var_name, cmap, width):\n",
    "    my_cmap = plt.get_cmap(cmap)\n",
    "    averages = temp_df / instances\n",
    "    y = averages\n",
    "    rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))\n",
    "    the_list = averages.tolist()\n",
    "    fig, ax = plt.subplots(\n",
    "        figsize=(21, 6), facecolor=\"slategrey\", constrained_layout=True\n",
    "    )\n",
    "    bars = plt.bar(temp_df.keys(), the_list, color=my_cmap(rescale(y)), width=width)\n",
    "    ax.set_title(\"Absolute Error of LSTM\", fontsize=28, c=\"white\")\n",
    "    ax.set_xlabel(var_name, fontsize=18, c=\"white\")\n",
    "    ax.set_ylabel(\"Mean Absolute Error\", fontsize=18, c=\"white\")\n",
    "    # Iterating over the bars one-by-one\n",
    "    # Annotate each bar with its value\n",
    "    # Annotate each bar with the number of instances\n",
    "    for bar, value, instance_count in zip(bars, the_list, instances):\n",
    "        yval = value + 0.01  # Adjust the vertical position of the label\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval,\n",
    "            f\"n={instance_count}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            color=\"black\",\n",
    "            fontsize=12,\n",
    "            rotation=90,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in full_df.keys():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diector = sorted(os.listdir('/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20231129/'))\n",
    "\n",
    "# for d in diector:\n",
    "#         full_df = pd.read_parquet(\n",
    "#         f\"/home/aevans/nwp_bias/src/machine_learning/data/lstm_eval_csvs/20231129/{d}\"\n",
    "#     )\n",
    "#         for k in full_df.columns:\n",
    "#             if re.search(\n",
    "#                 f\"{station}\",\n",
    "#                 k,\n",
    "#             ):\n",
    "#                 print(\"Succes!!\")\n",
    "#                 print(d)\n",
    "#             else:\n",
    "#                 continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index for both DataFrames to ensure alignment\n",
    "full_df = nysm_df.copy()\n",
    "full_df = full_df[full_df[\"station\"] == \"DELE\"]\n",
    "full_df = full_df.merge(ldf, on=\"valid_time\")\n",
    "full_df = full_df.rename(columns={\"diff\": \"Abs_err\"})\n",
    "full_df = full_df[full_df[\"Abs_err\"].abs() > 1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"Abs_err\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df, instances = err_bucket(full_df, f\"tair\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(temp_df, instances, \"Temperature (C)\", \"Wistia\", 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relh_df, instances = err_bucket(full_df, f\"relh\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(relh_df, instances, \"Relative Humidity (%)\", \"copper\", 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df, instances = err_bucket(full_df, f\"srad\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(new_df, instances, \"Solar Radiation (W / m*m)\", \"copper\", 50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_df, instances = err_bucket(full_df, f\"pres\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The mean pressure at {station} is {full_df[f\"pres\"].mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(pres_df, instances, \"Surface Pressure (mb)\", \"copper\", 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_small(full_df, met_col, rounded_base):\n",
    "    temps = []\n",
    "    for i, _ in enumerate(full_df[met_col]):\n",
    "        if full_df[met_col].iloc[i] < 1.0 and full_df[met_col].iloc[i] >= 0:\n",
    "            rounded = round(full_df[met_col].iloc[i], rounded_base)\n",
    "            temps.append(rounded)\n",
    "        else:\n",
    "            rounded = myround(full_df[met_col].iloc[i], 1)\n",
    "            temps.append(rounded)\n",
    "\n",
    "    unique_temps = unique(temps)\n",
    "\n",
    "    zeros = np.zeros(len(unique_temps))\n",
    "    rs = np.resize(zeros, (len(unique_temps), len(unique_temps)))\n",
    "    temp_df = pd.DataFrame(\n",
    "        data=rs, index=[np.arange(len(unique_temps))], columns=sorted(unique_temps)\n",
    "    )\n",
    "\n",
    "    for i, _ in enumerate(full_df[met_col]):\n",
    "        if full_df[met_col].iloc[i] < 1.0 and full_df[met_col].iloc[i] >= 0:\n",
    "            rounded = round(full_df[met_col].iloc[i], rounded_base)\n",
    "        else:\n",
    "            rounded = float(myround(full_df[met_col].iloc[i], base=1))\n",
    "        err = float(full_df[\"Abs_err\"].iloc[i])\n",
    "        temp_df[rounded].iloc[0] = abs(err) + abs(temp_df[rounded].iloc[0])\n",
    "        temp_df[rounded].iloc[-1] += 1\n",
    "\n",
    "    instances = temp_df.iloc[-1]\n",
    "    temp_df = temp_df.iloc[0]\n",
    "    temp_df = temp_df.loc[~(temp_df == 0)]\n",
    "    instances = instances.loc[~(instances == 0)]\n",
    "    return temp_df, instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df, instances = err_bucket(full_df, f\"precip_total\", 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(rain_df, instances, \"Precipitation [mm/hr]\", \"winter\", 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df, instances = round_small(full_df, f\"snow_depth\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_df = snow_df.iloc[1:]\n",
    "instances = instances.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(snow_df, instances, \"Accumulated Snow (m)\", \"cool\", 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmax, instances = err_bucket(full_df, f\"wmax_sonic\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(wmax, instances, \"Wind Max (m/s)\", \"copper\", 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir, instances = err_bucket(full_df, f\"wdir_sonic\", 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_buckets(wdir, instances, \"Wind Dir (degrees)\", \"copper\", 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "168fdf287636bbedc06224370453d1ea17ee31ef28776649e24f81e171f8fc2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
