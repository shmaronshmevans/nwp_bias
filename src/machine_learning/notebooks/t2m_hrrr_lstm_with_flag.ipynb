{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop all cols with strings and unvaluable info like index\n",
    "Always make sure it is sorted correctly by time and station\n",
    "edit time of year with cos/sin \"encode\"\n",
    "df['day_of_year'] = df['valid_time'].dt.dayofyear\n",
    "?? subrtract a row in setting forecast hours ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from comet_ml import Experiment, Artifact\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import (\n",
    "    CPUOffload,\n",
    "    BackwardPrefetch,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    size_based_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.processing import col_drop\n",
    "from src.processing import get_flag\n",
    "from src.processing import encode\n",
    "from src.processing import normalize\n",
    "from src.processing import get_error\n",
    "\n",
    "from src.data import hrrr_data\n",
    "from src.data import nysm_data\n",
    "\n",
    "from src.visuals import loss_curves\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.feature_selection import mutual_info_classif as MIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_drop(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"level_0\",\n",
    "            \"index\",\n",
    "            \"lead time\",\n",
    "            \"lsm\",\n",
    "            \"index_nysm\",\n",
    "            \"station_nysm\",\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffix(df, stations):\n",
    "    cols = [\"valid_time\", \"time\"]\n",
    "    df = df.rename(\n",
    "        columns={c: c + f\"_{stations[0]}\" for c in df.columns if c not in cols}\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_fold(df, fold):\n",
    "    length = len(df)\n",
    "    test_len = int(length * 0.2)\n",
    "    df_train = pd.DataFrame()\n",
    "\n",
    "    for n in np.arange(0, 5):\n",
    "        if n != fold:\n",
    "            df1 = df.iloc[int(0.2 * n * length) : int(0.2 * (n + 1) * length)]\n",
    "            df_train = pd.concat([df_train, df1])\n",
    "        else:\n",
    "            df_test = df.iloc[int(0.2 * n * length) : int(0.2 * (n + 1) * length)]\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df = nysm_data.load_nysm_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADDI', 'ANDE', 'BATA', 'BEAC', 'BELD', 'BELL', 'BELM', 'BERK',\n",
       "       'BING', 'BKLN', 'BRAN', 'BREW', 'BROC', 'BRON', 'BROO', 'BSPA',\n",
       "       'BUFF', 'BURD', 'BURT', 'CAMD', 'CAPE', 'CHAZ', 'CHES', 'CINC',\n",
       "       'CLAR', 'CLIF', 'CLYM', 'COBL', 'COHO', 'COLD', 'COPA', 'COPE',\n",
       "       'CSQR', 'DELE', 'DEPO', 'DOVE', 'DUAN', 'EAUR', 'EDIN', 'EDWA',\n",
       "       'ELDR', 'ELLE', 'ELMI', 'ESSX', 'FAYE', 'FRED', 'GABR', 'GFAL',\n",
       "       'GFLD', 'GROT', 'GROV', 'HAMM', 'HARP', 'HARR', 'HART', 'HERK',\n",
       "       'HFAL', 'ILAK', 'JOHN', 'JORD', 'KIND', 'LAUR', 'LOUI', 'MALO',\n",
       "       'MEDI', 'MEDU', 'MORR', 'NBRA', 'NEWC', 'NHUD', 'OLDF', 'OLEA',\n",
       "       'ONTA', 'OPPE', 'OSCE', 'OSWE', 'OTIS', 'OWEG', 'PENN', 'PHIL',\n",
       "       'PISE', 'POTS', 'RAND', 'REDF', 'REDH', 'ROXB', 'RUSH', 'SARA',\n",
       "       'SBRI', 'SCHA', 'SCHO', 'SCHU', 'SCIP', 'SHER', 'SOME', 'SOUT',\n",
       "       'SPRA', 'SPRI', 'STAT', 'STEP', 'SUFF', 'TANN', 'TICO', 'TULL',\n",
       "       'TUPP', 'TYRO', 'VOOR', 'WALL', 'WALT', 'WANT', 'WARS', 'WARW',\n",
       "       'WATE', 'WBOU', 'WELL', 'WEST', 'WFMB', 'WGAT', 'WHIT', 'WOLC',\n",
       "       'YORK', 'CROG', 'RAQU', 'STON'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nysm_df[\"station\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_model(station):\n",
    "    \"\"\"\n",
    "    This function creates and processes data for a LSTM machine learning model.\n",
    "\n",
    "    Args:\n",
    "        station (str): The station identifier for which data is being processed.\n",
    "\n",
    "    Returns:\n",
    "        new_df (pandas DataFrame): A DataFrame containing processed data.\n",
    "        df_train (pandas DataFrame): A DataFrame for training the machine learning model.\n",
    "        df_test (pandas DataFrame): A DataFrame for testing the machine learning model.\n",
    "        features (list): A list of feature names.\n",
    "        forecast_lead (int): The lead time for the target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print a message indicating the current station being processed.\n",
    "    print(f\"Targeting Error for {station}\")\n",
    "\n",
    "    # Load data from NYSM and HRRR sources.\n",
    "    print(\"-- loading data from NYSM --\")\n",
    "    nysm_df = nysm_data.load_nysm_data()\n",
    "    nysm_df.reset_index(inplace=True)\n",
    "    print(\"-- loading data from HRRR --\")\n",
    "    hrrr_df = hrrr_data.read_hrrr_data()\n",
    "\n",
    "    # Rename columns for consistency.\n",
    "    nysm_df = nysm_df.rename(columns={\"time_1H\": \"valid_time\"})\n",
    "\n",
    "    # Filter NYSM data to match valid times from HRRR data and save it to a CSV file.\n",
    "    mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "    nysm_df = nysm_df[nysm_df[\"valid_time\"].isin(mytimes)]\n",
    "    nysm_df.to_csv(\"/home/aevans/nwp_bias/src/machine_learning/frankenstein/test.csv\")\n",
    "\n",
    "    # Set the path for tabular data.\n",
    "    nysm_cats_path = \"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\"\n",
    "\n",
    "    # Load tabular data as a DataFrame.\n",
    "    print(\"-- adding geo data --\")\n",
    "    nysm_cats_df = pd.read_csv(nysm_cats_path)\n",
    "\n",
    "    # Identify the target data by climate division.\n",
    "    print(\"-- locating target data --\")\n",
    "    category = \"Western Plateau\"\n",
    "    nysm_cats_df1 = nysm_cats_df[nysm_cats_df[\"climate_division_name\"] == category]\n",
    "    stations = nysm_cats_df1[\"stid\"].tolist()\n",
    "    hrrr_df1 = hrrr_df[hrrr_df[\"station\"].isin(stations)]\n",
    "    nysm_df1 = nysm_df[nysm_df[\"station\"].isin(stations)]\n",
    "\n",
    "    # Clean the target data, merging HRRR and NYSM data and removing duplicates.\n",
    "    print(\"-- cleaning target data --\")\n",
    "    master_df = hrrr_df1.merge(nysm_df1, on=\"valid_time\", suffixes=(None, \"_nysm\"))\n",
    "    master_df = master_df.drop_duplicates(\n",
    "        subset=[\"valid_time\", \"station\", \"t2m\"], keep=\"first\"\n",
    "    )\n",
    "\n",
    "    # Finalize the DataFrame by dropping specific columns and adding suffixes.\n",
    "    print(\"-- finalizing dataframe --\")\n",
    "    df = columns_drop(master_df)\n",
    "    stations = df[\"station\"].unique()\n",
    "    master_df = df[df[\"station\"] == stations[0]]\n",
    "    master_df = add_suffix(master_df, stations)\n",
    "\n",
    "    # Merge data for different stations into a single DataFrame.\n",
    "    for station in stations:\n",
    "        df1 = df[df[\"station\"] == station]\n",
    "        master_df = master_df.merge(\n",
    "            df1, on=\"valid_time\", suffixes=(None, f\"_{station}\")\n",
    "        )\n",
    "\n",
    "    # Copy the master DataFrame for further processing.\n",
    "    the_df = master_df.copy()\n",
    "\n",
    "    # Drop rows with missing values.\n",
    "    the_df.dropna(inplace=True)\n",
    "    print(\"getting flag and error\")\n",
    "    the_df = get_flag.get_flag(the_df)\n",
    "\n",
    "    # Calculate the error using NWP data.\n",
    "    the_df = get_error.nwp_error(\"t2m\", \"OLEA\", the_df)\n",
    "    new_df = the_df.copy()\n",
    "\n",
    "    # Get the list of valid times.\n",
    "    valid_times = new_df[\"valid_time\"].tolist()\n",
    "\n",
    "    # Define columns to reintegrate back into the DataFrame after model training.\n",
    "    cols_to_carry = [\"valid_time\", \"flag\"]\n",
    "\n",
    "    # Set the forecast lead time.\n",
    "    forecast_lead = 1\n",
    "\n",
    "    # Define the target variable name.\n",
    "    target_sensor = \"target_error\"\n",
    "\n",
    "    # Normalize the data, create the target variable, and remove last rows.\n",
    "    lstm_df, features = normalize.normalize_df(new_df, valid_times, forecast_lead)\n",
    "    target = f\"{target_sensor}_lead_{forecast_lead}\"\n",
    "    lstm_df[target] = lstm_df[target_sensor].shift(-forecast_lead)\n",
    "    lstm_df = lstm_df.iloc[:-forecast_lead]\n",
    "\n",
    "    # Split the data into training and testing sets.\n",
    "    length = len(lstm_df)\n",
    "    test_len = int(length * 0.2)\n",
    "    df_train, df_test = which_fold(lstm_df, 1)\n",
    "    print(\"Test Set Fraction\", len(df_test) / len(lstm_df))\n",
    "\n",
    "    # Fill missing values with zeros in the training and testing DataFrames.\n",
    "    df_train = df_train.fillna(0)\n",
    "    df_test = df_test.fillna(0)\n",
    "\n",
    "    # Reintegrate the specified columns back into the training and testing DataFrames.\n",
    "    for c in cols_to_carry:\n",
    "        df_train[c] = the_df[c]\n",
    "        df_test[c] = the_df[c]\n",
    "\n",
    "    # Print a message indicating that data processing is complete.\n",
    "    print(\"Data Processed\")\n",
    "    print(\"--init model LSTM--\")\n",
    "\n",
    "    return new_df, df_train, df_test, features, forecast_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting Error for OLEA\n",
      "-- loading data from NYSM --\n",
      "-- loading data from HRRR --\n",
      "-- adding geo data --\n",
      "-- locating target data --\n"
     ]
    }
   ],
   "source": [
    "# Print a message indicating the current station being processed.\n",
    "station = \"OLEA\"\n",
    "print(f\"Targeting Error for {station}\")\n",
    "\n",
    "# Load data from NYSM and HRRR sources.\n",
    "print(\"-- loading data from NYSM --\")\n",
    "nysm_df = nysm_data.load_nysm_data()\n",
    "nysm_df.reset_index(inplace=True)\n",
    "print(\"-- loading data from HRRR --\")\n",
    "hrrr_df = hrrr_data.read_hrrr_data()\n",
    "\n",
    "# Rename columns for consistency.\n",
    "nysm_df = nysm_df.rename(columns={\"time_1H\": \"valid_time\"})\n",
    "\n",
    "# Filter NYSM data to match valid times from HRRR data and save it to a CSV file.\n",
    "mytimes = hrrr_df[\"valid_time\"].tolist()\n",
    "nysm_df = nysm_df[nysm_df[\"valid_time\"].isin(mytimes)]\n",
    "nysm_df.to_csv(\"/home/aevans/nwp_bias/src/machine_learning/frankenstein/test.csv\")\n",
    "\n",
    "# Set the path for tabular data.\n",
    "nysm_cats_path = \"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\"\n",
    "\n",
    "# Load tabular data as a DataFrame.\n",
    "print(\"-- adding geo data --\")\n",
    "nysm_cats_df = pd.read_csv(nysm_cats_path)\n",
    "\n",
    "# Identify the target data by climate division.\n",
    "print(\"-- locating target data --\")\n",
    "category = \"Western Plateau\"\n",
    "nysm_cats_df1 = nysm_cats_df[nysm_cats_df[\"climate_division_name\"] == category]\n",
    "stations = nysm_cats_df1[\"stid\"].tolist()\n",
    "hrrr_df1 = hrrr_df[hrrr_df[\"station\"].isin(stations)]\n",
    "nysm_df1 = nysm_df[nysm_df[\"station\"].isin(stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_drop_hrrr(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"level_0\",\n",
    "            \"index\",\n",
    "            \"lead time\",\n",
    "            \"lsm\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"time\",\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_suffix(master_df, station):\n",
    "    cols = [\"valid_time\", \"time\"]\n",
    "    master_df = master_df.rename(\n",
    "        columns={c: c + f\"_{station}\" for c in master_df.columns if c not in cols}\n",
    "    )\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_wrapper(stations, df):\n",
    "    master_df = df[df[\"station\"] == stations[0]]\n",
    "    master_df = add_suffix(master_df, stations[0])\n",
    "    for station in stations[1:3]:\n",
    "        df1 = df[df[\"station\"] == station]\n",
    "        df1 = add_suffix(df1, station)\n",
    "        master_df = master_df.merge(\n",
    "            df1, on=\"valid_time\", suffixes=(None, f\"_{station}\")\n",
    "        )\n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(data, col, max_val):\n",
    "    data[\"day_of_year\"] = data[\"valid_time\"].dt.dayofyear\n",
    "    sin = np.sin(2 * np.pi * data[\"day_of_year\"] / max_val)\n",
    "    data.insert(loc=(1), column=f\"{col}_sin\", value=sin)\n",
    "    cos = np.cos(2 * np.pi * data[\"day_of_year\"] / max_val)\n",
    "    data.insert(loc=(1), column=f\"{col}_cos\", value=cos)\n",
    "    data = data.drop(columns=[\"day_of_year\"])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_df1 = columns_drop_hrrr(hrrr_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = dataframe_wrapper(stations, hrrr_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['valid_time', 'station_ADDI', 't2m_ADDI', 'sh2_ADDI', 'd2m_ADDI',\n",
       "       'r2_ADDI', 'u10_ADDI', 'v10_ADDI', 'tp_ADDI', 'mslma_ADDI', 'orog_ADDI',\n",
       "       'tcc_ADDI', 'asnow_ADDI', 'cape_ADDI', 'dswrf_ADDI', 'dlwrf_ADDI',\n",
       "       'gh_ADDI', 'u_total_ADDI', 'u_dir_ADDI', 'new_tp_ADDI', 'station_BELM',\n",
       "       't2m_BELM', 'sh2_BELM', 'd2m_BELM', 'r2_BELM', 'u10_BELM', 'v10_BELM',\n",
       "       'tp_BELM', 'mslma_BELM', 'orog_BELM', 'tcc_BELM', 'asnow_BELM',\n",
       "       'cape_BELM', 'dswrf_BELM', 'dlwrf_BELM', 'gh_BELM', 'u_total_BELM',\n",
       "       'u_dir_BELM', 'new_tp_BELM', 'station_COHO', 't2m_COHO', 'sh2_COHO',\n",
       "       'd2m_COHO', 'r2_COHO', 'u10_COHO', 'v10_COHO', 'tp_COHO', 'mslma_COHO',\n",
       "       'orog_COHO', 'tcc_COHO', 'asnow_COHO', 'cape_COHO', 'dswrf_COHO',\n",
       "       'dlwrf_COHO', 'gh_COHO', 'u_total_COHO', 'u_dir_COHO', 'new_tp_COHO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df1 = nysm_df1.drop(\n",
    "    columns=[\n",
    "        \"index\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df2 = dataframe_wrapper(stations, nysm_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['station_ADDI', 'valid_time', 'lat_ADDI', 'lon_ADDI', 'elev_ADDI',\n",
       "       'tair_ADDI', 'ta9m_ADDI', 'td_ADDI', 'relh_ADDI', 'srad_ADDI',\n",
       "       'pres_ADDI', 'mslp_ADDI', 'wspd_sonic_ADDI', 'wmax_sonic_ADDI',\n",
       "       'wdir_sonic_ADDI', 'precip_total_ADDI', 'snow_depth_ADDI',\n",
       "       'station_BELM', 'lat_BELM', 'lon_BELM', 'elev_BELM', 'tair_BELM',\n",
       "       'ta9m_BELM', 'td_BELM', 'relh_BELM', 'srad_BELM', 'pres_BELM',\n",
       "       'mslp_BELM', 'wspd_sonic_BELM', 'wmax_sonic_BELM', 'wdir_sonic_BELM',\n",
       "       'precip_total_BELM', 'snow_depth_BELM', 'station_COHO', 'lat_COHO',\n",
       "       'lon_COHO', 'elev_COHO', 'tair_COHO', 'ta9m_COHO', 'td_COHO',\n",
       "       'relh_COHO', 'srad_COHO', 'pres_COHO', 'mslp_COHO', 'wspd_sonic_COHO',\n",
       "       'wmax_sonic_COHO', 'wdir_sonic_COHO', 'precip_total_COHO',\n",
       "       'snow_depth_COHO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df.merge(master_df2, on=\"valid_time\", suffixes=(None, f\"_xab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = which_fold(master_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_time</th>\n",
       "      <th>station_ADDI</th>\n",
       "      <th>t2m_ADDI</th>\n",
       "      <th>sh2_ADDI</th>\n",
       "      <th>d2m_ADDI</th>\n",
       "      <th>r2_ADDI</th>\n",
       "      <th>u10_ADDI</th>\n",
       "      <th>v10_ADDI</th>\n",
       "      <th>tp_ADDI</th>\n",
       "      <th>mslma_ADDI</th>\n",
       "      <th>...</th>\n",
       "      <th>td_COHO</th>\n",
       "      <th>relh_COHO</th>\n",
       "      <th>srad_COHO</th>\n",
       "      <th>pres_COHO</th>\n",
       "      <th>mslp_COHO</th>\n",
       "      <th>wspd_sonic_COHO</th>\n",
       "      <th>wmax_sonic_COHO</th>\n",
       "      <th>wdir_sonic_COHO</th>\n",
       "      <th>precip_total_COHO</th>\n",
       "      <th>snow_depth_COHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.199194</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.993216</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>3.321081</td>\n",
       "      <td>-2.272873</td>\n",
       "      <td>0.001</td>\n",
       "      <td>103034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.236069</td>\n",
       "      <td>83.005333</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>952.700378</td>\n",
       "      <td>961.160767</td>\n",
       "      <td>4.049551</td>\n",
       "      <td>5.268734</td>\n",
       "      <td>320.398987</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.118689</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.918570</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>3.298830</td>\n",
       "      <td>-1.821096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.697083</td>\n",
       "      <td>82.664978</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>952.789917</td>\n",
       "      <td>961.068542</td>\n",
       "      <td>3.752380</td>\n",
       "      <td>5.467729</td>\n",
       "      <td>302.989014</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.439062</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>-22.570868</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>2.992857</td>\n",
       "      <td>-0.877062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103027.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.546570</td>\n",
       "      <td>81.791618</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>952.710815</td>\n",
       "      <td>960.902649</td>\n",
       "      <td>4.310488</td>\n",
       "      <td>6.174314</td>\n",
       "      <td>306.762512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.760291</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>-22.821158</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>3.115693</td>\n",
       "      <td>-0.789671</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103071.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.348312</td>\n",
       "      <td>82.621620</td>\n",
       "      <td>0.059596</td>\n",
       "      <td>952.575623</td>\n",
       "      <td>960.738586</td>\n",
       "      <td>4.632712</td>\n",
       "      <td>6.812093</td>\n",
       "      <td>296.427094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.860175</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>-22.993704</td>\n",
       "      <td>74.099998</td>\n",
       "      <td>3.268302</td>\n",
       "      <td>-0.054269</td>\n",
       "      <td>0.001</td>\n",
       "      <td>103052.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.324707</td>\n",
       "      <td>82.821564</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>952.858276</td>\n",
       "      <td>961.028748</td>\n",
       "      <td>3.748342</td>\n",
       "      <td>5.010695</td>\n",
       "      <td>292.594391</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38348</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>11.485986</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>8.142847</td>\n",
       "      <td>80.300003</td>\n",
       "      <td>0.073768</td>\n",
       "      <td>5.399014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100762.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.202820</td>\n",
       "      <td>76.627151</td>\n",
       "      <td>56.429420</td>\n",
       "      <td>938.602417</td>\n",
       "      <td>938.342163</td>\n",
       "      <td>2.888842</td>\n",
       "      <td>4.793458</td>\n",
       "      <td>187.385101</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38349</th>\n",
       "      <td>2022-12-31 20:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>10.447198</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>7.376611</td>\n",
       "      <td>80.400002</td>\n",
       "      <td>-0.416093</td>\n",
       "      <td>5.292988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100785.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.407135</td>\n",
       "      <td>89.403557</td>\n",
       "      <td>20.635929</td>\n",
       "      <td>938.083618</td>\n",
       "      <td>938.075562</td>\n",
       "      <td>4.008613</td>\n",
       "      <td>6.724010</td>\n",
       "      <td>213.705399</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38350</th>\n",
       "      <td>2022-12-31 21:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>9.658258</td>\n",
       "      <td>0.00709</td>\n",
       "      <td>8.101648</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.234075</td>\n",
       "      <td>5.295895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100879.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.600677</td>\n",
       "      <td>91.210388</td>\n",
       "      <td>7.036471</td>\n",
       "      <td>938.252014</td>\n",
       "      <td>938.270630</td>\n",
       "      <td>4.102826</td>\n",
       "      <td>7.027788</td>\n",
       "      <td>211.484207</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38351</th>\n",
       "      <td>2022-12-31 22:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>9.118555</td>\n",
       "      <td>0.00711</td>\n",
       "      <td>8.145227</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.285457</td>\n",
       "      <td>5.515205</td>\n",
       "      <td>0.038</td>\n",
       "      <td>100849.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.706879</td>\n",
       "      <td>93.957100</td>\n",
       "      <td>0.338529</td>\n",
       "      <td>938.007629</td>\n",
       "      <td>938.103821</td>\n",
       "      <td>3.016000</td>\n",
       "      <td>5.212557</td>\n",
       "      <td>189.334000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38352</th>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>8.866113</td>\n",
       "      <td>0.00724</td>\n",
       "      <td>8.439722</td>\n",
       "      <td>94.900002</td>\n",
       "      <td>0.120182</td>\n",
       "      <td>5.397731</td>\n",
       "      <td>0.121</td>\n",
       "      <td>100790.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.577820</td>\n",
       "      <td>97.349640</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>937.305176</td>\n",
       "      <td>937.550110</td>\n",
       "      <td>2.162834</td>\n",
       "      <td>4.067811</td>\n",
       "      <td>169.763000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.005298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30682 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               valid_time station_ADDI   t2m_ADDI  sh2_ADDI   d2m_ADDI  \\\n",
       "0     2018-01-01 03:00:00         ADDI -19.199194   0.00068 -21.993216   \n",
       "1     2018-01-01 04:00:00         ADDI -19.118689   0.00068 -21.918570   \n",
       "2     2018-01-01 05:00:00         ADDI -19.439062   0.00065 -22.570868   \n",
       "3     2018-01-01 06:00:00         ADDI -19.760291   0.00063 -22.821158   \n",
       "4     2018-01-01 07:00:00         ADDI -19.860175   0.00062 -22.993704   \n",
       "...                   ...          ...        ...       ...        ...   \n",
       "38348 2022-12-31 19:00:00         ADDI  11.485986   0.00710   8.142847   \n",
       "38349 2022-12-31 20:00:00         ADDI  10.447198   0.00676   7.376611   \n",
       "38350 2022-12-31 21:00:00         ADDI   9.658258   0.00709   8.101648   \n",
       "38351 2022-12-31 22:00:00         ADDI   9.118555   0.00711   8.145227   \n",
       "38352 2022-12-31 23:00:00         ADDI   8.866113   0.00724   8.439722   \n",
       "\n",
       "         r2_ADDI  u10_ADDI  v10_ADDI  tp_ADDI  mslma_ADDI  ...    td_COHO  \\\n",
       "0      76.800003  3.321081 -2.272873    0.001    103034.0  ... -22.236069   \n",
       "1      76.500000  3.298830 -1.821096    0.000    103019.0  ... -21.697083   \n",
       "2      73.800003  2.992857 -0.877062    0.000    103027.0  ... -21.546570   \n",
       "3      74.400002  3.115693 -0.789671    0.000    103071.0  ... -21.348312   \n",
       "4      74.099998  3.268302 -0.054269    0.001    103052.0  ... -21.324707   \n",
       "...          ...       ...       ...      ...         ...  ...        ...   \n",
       "38348  80.300003  0.073768  5.399014    0.000    100762.0  ...   7.202820   \n",
       "38349  80.400002 -0.416093  5.292988    0.000    100785.0  ...   8.407135   \n",
       "38350  88.000000  0.234075  5.295895    0.000    100879.0  ...   8.600677   \n",
       "38351  92.000000  0.285457  5.515205    0.038    100849.0  ...   8.706879   \n",
       "38352  94.900002  0.120182  5.397731    0.121    100790.0  ...   8.577820   \n",
       "\n",
       "       relh_COHO  srad_COHO   pres_COHO   mslp_COHO  wspd_sonic_COHO  \\\n",
       "0      83.005333   0.059599  952.700378  961.160767         4.049551   \n",
       "1      82.664978   0.046355  952.789917  961.068542         3.752380   \n",
       "2      81.791618   0.059598  952.710815  960.902649         4.310488   \n",
       "3      82.621620   0.059596  952.575623  960.738586         4.632712   \n",
       "4      82.821564   0.072838  952.858276  961.028748         3.748342   \n",
       "...          ...        ...         ...         ...              ...   \n",
       "38348  76.627151  56.429420  938.602417  938.342163         2.888842   \n",
       "38349  89.403557  20.635929  938.083618  938.075562         4.008613   \n",
       "38350  91.210388   7.036471  938.252014  938.270630         4.102826   \n",
       "38351  93.957100   0.338529  938.007629  938.103821         3.016000   \n",
       "38352  97.349640   0.200355  937.305176  937.550110         2.162834   \n",
       "\n",
       "       wmax_sonic_COHO  wdir_sonic_COHO  precip_total_COHO  snow_depth_COHO  \n",
       "0             5.268734       320.398987               0.00         0.011778  \n",
       "1             5.467729       302.989014               0.00         0.013398  \n",
       "2             6.174314       306.762512               0.00         0.014291  \n",
       "3             6.812093       296.427094               0.00         0.013012  \n",
       "4             5.010695       292.594391               0.00         0.013992  \n",
       "...                ...              ...                ...              ...  \n",
       "38348         4.793458       187.385101               0.00         0.006084  \n",
       "38349         6.724010       213.705399               0.07         0.005659  \n",
       "38350         7.027788       211.484207               0.00         0.003971  \n",
       "38351         5.212557       189.334000               0.00         0.005106  \n",
       "38352         4.067811       169.763000               0.09         0.005298  \n",
       "\n",
       "[30682 rows x 106 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_time</th>\n",
       "      <th>station_ADDI</th>\n",
       "      <th>t2m_ADDI</th>\n",
       "      <th>sh2_ADDI</th>\n",
       "      <th>d2m_ADDI</th>\n",
       "      <th>r2_ADDI</th>\n",
       "      <th>u10_ADDI</th>\n",
       "      <th>v10_ADDI</th>\n",
       "      <th>tp_ADDI</th>\n",
       "      <th>mslma_ADDI</th>\n",
       "      <th>...</th>\n",
       "      <th>td_COHO</th>\n",
       "      <th>relh_COHO</th>\n",
       "      <th>srad_COHO</th>\n",
       "      <th>pres_COHO</th>\n",
       "      <th>mslp_COHO</th>\n",
       "      <th>wspd_sonic_COHO</th>\n",
       "      <th>wmax_sonic_COHO</th>\n",
       "      <th>wdir_sonic_COHO</th>\n",
       "      <th>precip_total_COHO</th>\n",
       "      <th>snow_depth_COHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7670</th>\n",
       "      <td>2019-04-02 11:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-2.800116</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>-10.229285</td>\n",
       "      <td>54.200001</td>\n",
       "      <td>-0.840036</td>\n",
       "      <td>4.357924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102469.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.027161</td>\n",
       "      <td>59.026249</td>\n",
       "      <td>7.567856</td>\n",
       "      <td>952.435913</td>\n",
       "      <td>956.011108</td>\n",
       "      <td>3.505187</td>\n",
       "      <td>6.217631</td>\n",
       "      <td>187.397400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7671</th>\n",
       "      <td>2019-04-02 12:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-2.094031</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>-9.271704</td>\n",
       "      <td>56.500000</td>\n",
       "      <td>-0.815294</td>\n",
       "      <td>3.543884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102545.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.858948</td>\n",
       "      <td>53.888851</td>\n",
       "      <td>102.628220</td>\n",
       "      <td>952.309509</td>\n",
       "      <td>955.505249</td>\n",
       "      <td>3.480735</td>\n",
       "      <td>5.642790</td>\n",
       "      <td>180.033997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7672</th>\n",
       "      <td>2019-04-02 13:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-0.055823</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>-8.094641</td>\n",
       "      <td>55.299999</td>\n",
       "      <td>-0.840937</td>\n",
       "      <td>4.726081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102528.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.607452</td>\n",
       "      <td>53.674629</td>\n",
       "      <td>230.324387</td>\n",
       "      <td>951.843994</td>\n",
       "      <td>954.945740</td>\n",
       "      <td>4.695154</td>\n",
       "      <td>7.205263</td>\n",
       "      <td>163.311996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7673</th>\n",
       "      <td>2019-04-02 14:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>2.602563</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>-6.657996</td>\n",
       "      <td>52.099998</td>\n",
       "      <td>-0.615318</td>\n",
       "      <td>6.954502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102432.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.999878</td>\n",
       "      <td>50.521992</td>\n",
       "      <td>336.970825</td>\n",
       "      <td>951.247986</td>\n",
       "      <td>953.948547</td>\n",
       "      <td>4.490968</td>\n",
       "      <td>8.405516</td>\n",
       "      <td>172.641693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>2019-04-02 15:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>4.740533</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>-6.153021</td>\n",
       "      <td>47.299999</td>\n",
       "      <td>-0.794386</td>\n",
       "      <td>7.123545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102371.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.736786</td>\n",
       "      <td>43.522881</td>\n",
       "      <td>667.342163</td>\n",
       "      <td>950.524414</td>\n",
       "      <td>952.599670</td>\n",
       "      <td>7.000066</td>\n",
       "      <td>9.654021</td>\n",
       "      <td>184.360901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15336</th>\n",
       "      <td>2020-02-20 00:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-4.032324</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>-9.651648</td>\n",
       "      <td>65.599998</td>\n",
       "      <td>5.325153</td>\n",
       "      <td>-1.649920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103026.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.441040</td>\n",
       "      <td>70.623993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>955.438721</td>\n",
       "      <td>960.122375</td>\n",
       "      <td>6.698401</td>\n",
       "      <td>10.974760</td>\n",
       "      <td>283.211700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15337</th>\n",
       "      <td>2020-02-20 01:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-4.166479</td>\n",
       "      <td>0.00185</td>\n",
       "      <td>-9.974219</td>\n",
       "      <td>64.699997</td>\n",
       "      <td>5.410713</td>\n",
       "      <td>-1.507067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103038.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.768555</td>\n",
       "      <td>68.303574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>955.605530</td>\n",
       "      <td>960.266174</td>\n",
       "      <td>5.499091</td>\n",
       "      <td>8.733680</td>\n",
       "      <td>277.621307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15338</th>\n",
       "      <td>2020-02-20 02:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-5.727576</td>\n",
       "      <td>0.00177</td>\n",
       "      <td>-10.524817</td>\n",
       "      <td>68.300003</td>\n",
       "      <td>4.351043</td>\n",
       "      <td>-1.059071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.626312</td>\n",
       "      <td>83.362663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>955.613586</td>\n",
       "      <td>960.379639</td>\n",
       "      <td>6.359388</td>\n",
       "      <td>9.124758</td>\n",
       "      <td>275.665100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15339</th>\n",
       "      <td>2020-02-20 03:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-6.150916</td>\n",
       "      <td>0.00174</td>\n",
       "      <td>-10.665686</td>\n",
       "      <td>69.500000</td>\n",
       "      <td>3.748579</td>\n",
       "      <td>-1.038778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.033295</td>\n",
       "      <td>88.666862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>955.681885</td>\n",
       "      <td>960.504211</td>\n",
       "      <td>3.431629</td>\n",
       "      <td>5.589961</td>\n",
       "      <td>291.429596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15340</th>\n",
       "      <td>2020-02-20 04:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-6.428687</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>-11.183630</td>\n",
       "      <td>68.199997</td>\n",
       "      <td>3.623866</td>\n",
       "      <td>-0.981035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103091.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.170074</td>\n",
       "      <td>90.479271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>955.877625</td>\n",
       "      <td>960.815491</td>\n",
       "      <td>3.880804</td>\n",
       "      <td>5.875144</td>\n",
       "      <td>294.783508</td>\n",
       "      <td>0.169998</td>\n",
       "      <td>0.101902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7671 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               valid_time station_ADDI  t2m_ADDI  sh2_ADDI   d2m_ADDI  \\\n",
       "7670  2019-04-02 11:00:00         ADDI -2.800116   0.00181 -10.229285   \n",
       "7671  2019-04-02 12:00:00         ADDI -2.094031   0.00195  -9.271704   \n",
       "7672  2019-04-02 13:00:00         ADDI -0.055823   0.00215  -8.094641   \n",
       "7673  2019-04-02 14:00:00         ADDI  2.602563   0.00240  -6.657996   \n",
       "7674  2019-04-02 15:00:00         ADDI  4.740533   0.00250  -6.153021   \n",
       "...                   ...          ...       ...       ...        ...   \n",
       "15336 2020-02-20 00:00:00         ADDI -4.032324   0.00189  -9.651648   \n",
       "15337 2020-02-20 01:00:00         ADDI -4.166479   0.00185  -9.974219   \n",
       "15338 2020-02-20 02:00:00         ADDI -5.727576   0.00177 -10.524817   \n",
       "15339 2020-02-20 03:00:00         ADDI -6.150916   0.00174 -10.665686   \n",
       "15340 2020-02-20 04:00:00         ADDI -6.428687   0.00167 -11.183630   \n",
       "\n",
       "         r2_ADDI  u10_ADDI  v10_ADDI  tp_ADDI  mslma_ADDI  ...    td_COHO  \\\n",
       "7670   54.200001 -0.840036  4.357924      0.0    102469.0  ... -10.027161   \n",
       "7671   56.500000 -0.815294  3.543884      0.0    102545.0  ...  -9.858948   \n",
       "7672   55.299999 -0.840937  4.726081      0.0    102528.0  ...  -9.607452   \n",
       "7673   52.099998 -0.615318  6.954502      0.0    102432.0  ...  -8.999878   \n",
       "7674   47.299999 -0.794386  7.123545      0.0    102371.0  ...  -8.736786   \n",
       "...          ...       ...       ...      ...         ...  ...        ...   \n",
       "15336  65.599998  5.325153 -1.649920      0.0    103026.0  ... -11.441040   \n",
       "15337  64.699997  5.410713 -1.507067      0.0    103038.0  ... -11.768555   \n",
       "15338  68.300003  4.351043 -1.059071      0.0    103114.0  ...  -9.626312   \n",
       "15339  69.500000  3.748579 -1.038778      0.0    103120.0  ...  -9.033295   \n",
       "15340  68.199997  3.623866 -0.981035      0.0    103091.0  ...  -9.170074   \n",
       "\n",
       "       relh_COHO   srad_COHO   pres_COHO   mslp_COHO  wspd_sonic_COHO  \\\n",
       "7670   59.026249    7.567856  952.435913  956.011108         3.505187   \n",
       "7671   53.888851  102.628220  952.309509  955.505249         3.480735   \n",
       "7672   53.674629  230.324387  951.843994  954.945740         4.695154   \n",
       "7673   50.521992  336.970825  951.247986  953.948547         4.490968   \n",
       "7674   43.522881  667.342163  950.524414  952.599670         7.000066   \n",
       "...          ...         ...         ...         ...              ...   \n",
       "15336  70.623993    0.000000  955.438721  960.122375         6.698401   \n",
       "15337  68.303574    0.000000  955.605530  960.266174         5.499091   \n",
       "15338  83.362663    0.000000  955.613586  960.379639         6.359388   \n",
       "15339  88.666862    0.000000  955.681885  960.504211         3.431629   \n",
       "15340  90.479271    0.000000  955.877625  960.815491         3.880804   \n",
       "\n",
       "       wmax_sonic_COHO  wdir_sonic_COHO  precip_total_COHO  snow_depth_COHO  \n",
       "7670          6.217631       187.397400           0.000000         0.009952  \n",
       "7671          5.642790       180.033997           0.000000         0.011029  \n",
       "7672          7.205263       163.311996           0.000000         0.011903  \n",
       "7673          8.405516       172.641693           0.000000         0.011782  \n",
       "7674          9.654021       184.360901           0.000000         0.013715  \n",
       "...                ...              ...                ...              ...  \n",
       "15336        10.974760       283.211700           0.000000         0.102824  \n",
       "15337         8.733680       277.621307           0.000000         0.103490  \n",
       "15338         9.124758       275.665100           0.000000         0.102813  \n",
       "15339         5.589961       291.429596           0.000000         0.102500  \n",
       "15340         5.875144       294.783508           0.169998         0.101902  \n",
       "\n",
       "[7671 rows x 106 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations_for_ball_tree(df):\n",
    "    locations_a = df.reset_index()[[\"lat\", \"lon\"]]\n",
    "    locations_b = df.reset_index()[[\"lat\", \"lon\"]]\n",
    "\n",
    "    # ball tree to find nysm site locations\n",
    "    # locations_a ==> build the tree\n",
    "    # locations_b ==> query the tree\n",
    "    # Creates new columns converting coordinate degrees to radians.\n",
    "    for column in locations_a[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    for column in locations_b[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_b[column].values)\n",
    "        locations_b[f\"{column}_rad\"] = rad\n",
    "\n",
    "    return locations_a, locations_b\n",
    "\n",
    "\n",
    "def get_ball_tree_indices(model_data):\n",
    "    locations_a, locations_b = get_locations_for_ball_tree(model_data)\n",
    "    # Takes the first group's latitude and longitude values to construct the ball tree.\n",
    "\n",
    "    ball = BallTree(locations_a[[\"lat_rad\", \"lon_rad\"]].values, metric=\"haversine\")\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = 3\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "    # get indices in a format where we can query the df\n",
    "    indices_list = [indices[x][0] for x in range(len(indices))]\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "def df_with_nysm_locations(df, df_nysm, indices_list):\n",
    "    df_closest_locs = df.iloc[indices_list][[\"latitude\", \"longitude\"]].reset_index()\n",
    "    df_nysm_station_locs = df_nysm.groupby(\"station\")[[\"lat\", \"lon\"]].mean()\n",
    "\n",
    "    for x in range(len(df_nysm_station_locs.index)):\n",
    "        df_dummy = df[\n",
    "            (df.latitude == df_closest_locs.latitude[x])\n",
    "            & (df.longitude == df_closest_locs.longitude[x])\n",
    "        ]\n",
    "        df_dummy = df_dummy.reset_index()\n",
    "        df_dummy[\"station\"] = df_nysm_station_locs.index[x]\n",
    "        if x == 0:\n",
    "            df_save = df_dummy\n",
    "        else:\n",
    "            df_save = pd.concat([df_save, df_dummy])\n",
    "    print(\"complete\")\n",
    "    return df_save.set_index([\"station\", \"valid_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_stations(nysm_df, neighbors, target_station):\n",
    "    lats = nysm_df[\"lat\"].unique()\n",
    "    lons = nysm_df[\"lon\"].unique()\n",
    "\n",
    "    locations_a = pd.DataFrame()\n",
    "    locations_a[\"lat\"] = lats\n",
    "    locations_a[\"lon\"] = lons\n",
    "\n",
    "    for column in locations_a[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    locations_b = locations_a\n",
    "\n",
    "    ball = BallTree(locations_a[[\"lat_rad\", \"lon_rad\"]].values, metric=\"haversine\")\n",
    "\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = neighbors\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "\n",
    "    indices_list = [indices[x][0:k] for x in range(len(indices))]\n",
    "\n",
    "    stations = nysm_df[\"station\"].unique()\n",
    "\n",
    "    station_dict = {}\n",
    "\n",
    "    for k, _ in enumerate(stations):\n",
    "        station_dict[stations[k]] = indices_list[k]\n",
    "\n",
    "    utilize_ls = []\n",
    "    vals = station_dict.get(target_station)\n",
    "    vals\n",
    "    for v in vals:\n",
    "        x = stations[v]\n",
    "        utilize_ls.append(x)\n",
    "\n",
    "    return utilize_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nwp_error(target, station, df):\n",
    "    \"\"\"\n",
    "    Calculate the error between NWP model data and NYSM data for a specific target variable.\n",
    "\n",
    "    Args:\n",
    "        target (str): The target variable name (e.g., 't2m' for temperature).\n",
    "        station (str): The station identifier for which data is being compared.\n",
    "        df (pd.DataFrame): The input DataFrame containing NWP and NYSM data.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The input DataFrame with the 'target_error' column added.\n",
    "\n",
    "    This function calculates the error between the NWP (Numerical Weather Prediction) modeldata and NYSM (New York State Mesonet) data for a specific target variable at a given station. The error is computed by subtracting the NYSM data from the NWP model data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a dictionary to map NWP variable names to NYSM variable names.\n",
    "    vars_dict = {\n",
    "        \"t2m\": \"tair\",\n",
    "        \"mslma\": \"pres\",\n",
    "        # Add more variable mappings as needed.\n",
    "    }\n",
    "\n",
    "    # Get the NYSM variable name corresponding to the target variable.\n",
    "    nysm_var = vars_dict.get(target)\n",
    "\n",
    "    # Calculate the 'target_error' by subtracting NYSM data from NWP model data.\n",
    "    target_error = df[f\"{target}_{station}\"] - df[f\"{nysm_var}_{station}\"]\n",
    "    df.insert(loc=(1), column=f\"target_error\", value=target_error)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_time</th>\n",
       "      <th>station_ADDI</th>\n",
       "      <th>t2m_ADDI</th>\n",
       "      <th>sh2_ADDI</th>\n",
       "      <th>d2m_ADDI</th>\n",
       "      <th>r2_ADDI</th>\n",
       "      <th>u10_ADDI</th>\n",
       "      <th>v10_ADDI</th>\n",
       "      <th>tp_ADDI</th>\n",
       "      <th>mslma_ADDI</th>\n",
       "      <th>...</th>\n",
       "      <th>td_COHO</th>\n",
       "      <th>relh_COHO</th>\n",
       "      <th>srad_COHO</th>\n",
       "      <th>pres_COHO</th>\n",
       "      <th>mslp_COHO</th>\n",
       "      <th>wspd_sonic_COHO</th>\n",
       "      <th>wmax_sonic_COHO</th>\n",
       "      <th>wdir_sonic_COHO</th>\n",
       "      <th>precip_total_COHO</th>\n",
       "      <th>snow_depth_COHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.199194</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.993216</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>3.321081</td>\n",
       "      <td>-2.272873</td>\n",
       "      <td>0.001</td>\n",
       "      <td>103034.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.236069</td>\n",
       "      <td>83.005333</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>952.700378</td>\n",
       "      <td>961.160767</td>\n",
       "      <td>4.049551</td>\n",
       "      <td>5.268734</td>\n",
       "      <td>320.398987</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.118689</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.918570</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>3.298830</td>\n",
       "      <td>-1.821096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103019.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.697083</td>\n",
       "      <td>82.664978</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>952.789917</td>\n",
       "      <td>961.068542</td>\n",
       "      <td>3.752380</td>\n",
       "      <td>5.467729</td>\n",
       "      <td>302.989014</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.439062</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>-22.570868</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>2.992857</td>\n",
       "      <td>-0.877062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103027.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.546570</td>\n",
       "      <td>81.791618</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>952.710815</td>\n",
       "      <td>960.902649</td>\n",
       "      <td>4.310488</td>\n",
       "      <td>6.174314</td>\n",
       "      <td>306.762512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.760291</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>-22.821158</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>3.115693</td>\n",
       "      <td>-0.789671</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103071.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.348312</td>\n",
       "      <td>82.621620</td>\n",
       "      <td>0.059596</td>\n",
       "      <td>952.575623</td>\n",
       "      <td>960.738586</td>\n",
       "      <td>4.632712</td>\n",
       "      <td>6.812093</td>\n",
       "      <td>296.427094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.860175</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>-22.993704</td>\n",
       "      <td>74.099998</td>\n",
       "      <td>3.268302</td>\n",
       "      <td>-0.054269</td>\n",
       "      <td>0.001</td>\n",
       "      <td>103052.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.324707</td>\n",
       "      <td>82.821564</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>952.858276</td>\n",
       "      <td>961.028748</td>\n",
       "      <td>3.748342</td>\n",
       "      <td>5.010695</td>\n",
       "      <td>292.594391</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38348</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>11.485986</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>8.142847</td>\n",
       "      <td>80.300003</td>\n",
       "      <td>0.073768</td>\n",
       "      <td>5.399014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100762.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.202820</td>\n",
       "      <td>76.627151</td>\n",
       "      <td>56.429420</td>\n",
       "      <td>938.602417</td>\n",
       "      <td>938.342163</td>\n",
       "      <td>2.888842</td>\n",
       "      <td>4.793458</td>\n",
       "      <td>187.385101</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38349</th>\n",
       "      <td>2022-12-31 20:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>10.447198</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>7.376611</td>\n",
       "      <td>80.400002</td>\n",
       "      <td>-0.416093</td>\n",
       "      <td>5.292988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100785.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.407135</td>\n",
       "      <td>89.403557</td>\n",
       "      <td>20.635929</td>\n",
       "      <td>938.083618</td>\n",
       "      <td>938.075562</td>\n",
       "      <td>4.008613</td>\n",
       "      <td>6.724010</td>\n",
       "      <td>213.705399</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38350</th>\n",
       "      <td>2022-12-31 21:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>9.658258</td>\n",
       "      <td>0.00709</td>\n",
       "      <td>8.101648</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.234075</td>\n",
       "      <td>5.295895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>100879.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.600677</td>\n",
       "      <td>91.210388</td>\n",
       "      <td>7.036471</td>\n",
       "      <td>938.252014</td>\n",
       "      <td>938.270630</td>\n",
       "      <td>4.102826</td>\n",
       "      <td>7.027788</td>\n",
       "      <td>211.484207</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38351</th>\n",
       "      <td>2022-12-31 22:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>9.118555</td>\n",
       "      <td>0.00711</td>\n",
       "      <td>8.145227</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.285457</td>\n",
       "      <td>5.515205</td>\n",
       "      <td>0.038</td>\n",
       "      <td>100849.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.706879</td>\n",
       "      <td>93.957100</td>\n",
       "      <td>0.338529</td>\n",
       "      <td>938.007629</td>\n",
       "      <td>938.103821</td>\n",
       "      <td>3.016000</td>\n",
       "      <td>5.212557</td>\n",
       "      <td>189.334000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38352</th>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>8.866113</td>\n",
       "      <td>0.00724</td>\n",
       "      <td>8.439722</td>\n",
       "      <td>94.900002</td>\n",
       "      <td>0.120182</td>\n",
       "      <td>5.397731</td>\n",
       "      <td>0.121</td>\n",
       "      <td>100790.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.577820</td>\n",
       "      <td>97.349640</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>937.305176</td>\n",
       "      <td>937.550110</td>\n",
       "      <td>2.162834</td>\n",
       "      <td>4.067811</td>\n",
       "      <td>169.763000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.005298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38353 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               valid_time station_ADDI   t2m_ADDI  sh2_ADDI   d2m_ADDI  \\\n",
       "0     2018-01-01 03:00:00         ADDI -19.199194   0.00068 -21.993216   \n",
       "1     2018-01-01 04:00:00         ADDI -19.118689   0.00068 -21.918570   \n",
       "2     2018-01-01 05:00:00         ADDI -19.439062   0.00065 -22.570868   \n",
       "3     2018-01-01 06:00:00         ADDI -19.760291   0.00063 -22.821158   \n",
       "4     2018-01-01 07:00:00         ADDI -19.860175   0.00062 -22.993704   \n",
       "...                   ...          ...        ...       ...        ...   \n",
       "38348 2022-12-31 19:00:00         ADDI  11.485986   0.00710   8.142847   \n",
       "38349 2022-12-31 20:00:00         ADDI  10.447198   0.00676   7.376611   \n",
       "38350 2022-12-31 21:00:00         ADDI   9.658258   0.00709   8.101648   \n",
       "38351 2022-12-31 22:00:00         ADDI   9.118555   0.00711   8.145227   \n",
       "38352 2022-12-31 23:00:00         ADDI   8.866113   0.00724   8.439722   \n",
       "\n",
       "         r2_ADDI  u10_ADDI  v10_ADDI  tp_ADDI  mslma_ADDI  ...    td_COHO  \\\n",
       "0      76.800003  3.321081 -2.272873    0.001    103034.0  ... -22.236069   \n",
       "1      76.500000  3.298830 -1.821096    0.000    103019.0  ... -21.697083   \n",
       "2      73.800003  2.992857 -0.877062    0.000    103027.0  ... -21.546570   \n",
       "3      74.400002  3.115693 -0.789671    0.000    103071.0  ... -21.348312   \n",
       "4      74.099998  3.268302 -0.054269    0.001    103052.0  ... -21.324707   \n",
       "...          ...       ...       ...      ...         ...  ...        ...   \n",
       "38348  80.300003  0.073768  5.399014    0.000    100762.0  ...   7.202820   \n",
       "38349  80.400002 -0.416093  5.292988    0.000    100785.0  ...   8.407135   \n",
       "38350  88.000000  0.234075  5.295895    0.000    100879.0  ...   8.600677   \n",
       "38351  92.000000  0.285457  5.515205    0.038    100849.0  ...   8.706879   \n",
       "38352  94.900002  0.120182  5.397731    0.121    100790.0  ...   8.577820   \n",
       "\n",
       "       relh_COHO  srad_COHO   pres_COHO   mslp_COHO  wspd_sonic_COHO  \\\n",
       "0      83.005333   0.059599  952.700378  961.160767         4.049551   \n",
       "1      82.664978   0.046355  952.789917  961.068542         3.752380   \n",
       "2      81.791618   0.059598  952.710815  960.902649         4.310488   \n",
       "3      82.621620   0.059596  952.575623  960.738586         4.632712   \n",
       "4      82.821564   0.072838  952.858276  961.028748         3.748342   \n",
       "...          ...        ...         ...         ...              ...   \n",
       "38348  76.627151  56.429420  938.602417  938.342163         2.888842   \n",
       "38349  89.403557  20.635929  938.083618  938.075562         4.008613   \n",
       "38350  91.210388   7.036471  938.252014  938.270630         4.102826   \n",
       "38351  93.957100   0.338529  938.007629  938.103821         3.016000   \n",
       "38352  97.349640   0.200355  937.305176  937.550110         2.162834   \n",
       "\n",
       "       wmax_sonic_COHO  wdir_sonic_COHO  precip_total_COHO  snow_depth_COHO  \n",
       "0             5.268734       320.398987               0.00         0.011778  \n",
       "1             5.467729       302.989014               0.00         0.013398  \n",
       "2             6.174314       306.762512               0.00         0.014291  \n",
       "3             6.812093       296.427094               0.00         0.013012  \n",
       "4             5.010695       292.594391               0.00         0.013992  \n",
       "...                ...              ...                ...              ...  \n",
       "38348         4.793458       187.385101               0.00         0.006084  \n",
       "38349         6.724010       213.705399               0.07         0.005659  \n",
       "38350         7.027788       211.484207               0.00         0.003971  \n",
       "38351         5.212557       189.334000               0.00         0.005106  \n",
       "38352         4.067811       169.763000               0.09         0.005298  \n",
       "\n",
       "[38353 rows x 106 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_time</th>\n",
       "      <th>target_error</th>\n",
       "      <th>station_ADDI</th>\n",
       "      <th>t2m_ADDI</th>\n",
       "      <th>sh2_ADDI</th>\n",
       "      <th>d2m_ADDI</th>\n",
       "      <th>r2_ADDI</th>\n",
       "      <th>u10_ADDI</th>\n",
       "      <th>v10_ADDI</th>\n",
       "      <th>tp_ADDI</th>\n",
       "      <th>...</th>\n",
       "      <th>td_COHO</th>\n",
       "      <th>relh_COHO</th>\n",
       "      <th>srad_COHO</th>\n",
       "      <th>pres_COHO</th>\n",
       "      <th>mslp_COHO</th>\n",
       "      <th>wspd_sonic_COHO</th>\n",
       "      <th>wmax_sonic_COHO</th>\n",
       "      <th>wdir_sonic_COHO</th>\n",
       "      <th>precip_total_COHO</th>\n",
       "      <th>snow_depth_COHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>-0.368794</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.199194</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.993216</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>3.321081</td>\n",
       "      <td>-2.272873</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.236069</td>\n",
       "      <td>83.005333</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>952.700378</td>\n",
       "      <td>961.160767</td>\n",
       "      <td>4.049551</td>\n",
       "      <td>5.268734</td>\n",
       "      <td>320.398987</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>-0.600048</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.118689</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.918570</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>3.298830</td>\n",
       "      <td>-1.821096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.697083</td>\n",
       "      <td>82.664978</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>952.789917</td>\n",
       "      <td>961.068542</td>\n",
       "      <td>3.752380</td>\n",
       "      <td>5.467729</td>\n",
       "      <td>302.989014</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>-0.803223</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.439062</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>-22.570868</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>2.992857</td>\n",
       "      <td>-0.877062</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.546570</td>\n",
       "      <td>81.791618</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>952.710815</td>\n",
       "      <td>960.902649</td>\n",
       "      <td>4.310488</td>\n",
       "      <td>6.174314</td>\n",
       "      <td>306.762512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>-1.059481</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.760291</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>-22.821158</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>3.115693</td>\n",
       "      <td>-0.789671</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.348312</td>\n",
       "      <td>82.621620</td>\n",
       "      <td>0.059596</td>\n",
       "      <td>952.575623</td>\n",
       "      <td>960.738586</td>\n",
       "      <td>4.632712</td>\n",
       "      <td>6.812093</td>\n",
       "      <td>296.427094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>-0.031725</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>-19.860175</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>-22.993704</td>\n",
       "      <td>74.099998</td>\n",
       "      <td>3.268302</td>\n",
       "      <td>-0.054269</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.324707</td>\n",
       "      <td>82.821564</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>952.858276</td>\n",
       "      <td>961.028748</td>\n",
       "      <td>3.748342</td>\n",
       "      <td>5.010695</td>\n",
       "      <td>292.594391</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38348</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>0.660996</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>11.485986</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>8.142847</td>\n",
       "      <td>80.300003</td>\n",
       "      <td>0.073768</td>\n",
       "      <td>5.399014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.202820</td>\n",
       "      <td>76.627151</td>\n",
       "      <td>56.429420</td>\n",
       "      <td>938.602417</td>\n",
       "      <td>938.342163</td>\n",
       "      <td>2.888842</td>\n",
       "      <td>4.793458</td>\n",
       "      <td>187.385101</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38349</th>\n",
       "      <td>2022-12-31 20:00:00</td>\n",
       "      <td>0.637534</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>10.447198</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>7.376611</td>\n",
       "      <td>80.400002</td>\n",
       "      <td>-0.416093</td>\n",
       "      <td>5.292988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.407135</td>\n",
       "      <td>89.403557</td>\n",
       "      <td>20.635929</td>\n",
       "      <td>938.083618</td>\n",
       "      <td>938.075562</td>\n",
       "      <td>4.008613</td>\n",
       "      <td>6.724010</td>\n",
       "      <td>213.705399</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38350</th>\n",
       "      <td>2022-12-31 21:00:00</td>\n",
       "      <td>-0.462142</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>9.658258</td>\n",
       "      <td>0.00709</td>\n",
       "      <td>8.101648</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.234075</td>\n",
       "      <td>5.295895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.600677</td>\n",
       "      <td>91.210388</td>\n",
       "      <td>7.036471</td>\n",
       "      <td>938.252014</td>\n",
       "      <td>938.270630</td>\n",
       "      <td>4.102826</td>\n",
       "      <td>7.027788</td>\n",
       "      <td>211.484207</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38351</th>\n",
       "      <td>2022-12-31 22:00:00</td>\n",
       "      <td>-0.493114</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>9.118555</td>\n",
       "      <td>0.00711</td>\n",
       "      <td>8.145227</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.285457</td>\n",
       "      <td>5.515205</td>\n",
       "      <td>0.038</td>\n",
       "      <td>...</td>\n",
       "      <td>8.706879</td>\n",
       "      <td>93.957100</td>\n",
       "      <td>0.338529</td>\n",
       "      <td>938.007629</td>\n",
       "      <td>938.103821</td>\n",
       "      <td>3.016000</td>\n",
       "      <td>5.212557</td>\n",
       "      <td>189.334000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38352</th>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.067485</td>\n",
       "      <td>ADDI</td>\n",
       "      <td>8.866113</td>\n",
       "      <td>0.00724</td>\n",
       "      <td>8.439722</td>\n",
       "      <td>94.900002</td>\n",
       "      <td>0.120182</td>\n",
       "      <td>5.397731</td>\n",
       "      <td>0.121</td>\n",
       "      <td>...</td>\n",
       "      <td>8.577820</td>\n",
       "      <td>97.349640</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>937.305176</td>\n",
       "      <td>937.550110</td>\n",
       "      <td>2.162834</td>\n",
       "      <td>4.067811</td>\n",
       "      <td>169.763000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.005298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38353 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               valid_time  target_error station_ADDI   t2m_ADDI  sh2_ADDI  \\\n",
       "0     2018-01-01 03:00:00     -0.368794         ADDI -19.199194   0.00068   \n",
       "1     2018-01-01 04:00:00     -0.600048         ADDI -19.118689   0.00068   \n",
       "2     2018-01-01 05:00:00     -0.803223         ADDI -19.439062   0.00065   \n",
       "3     2018-01-01 06:00:00     -1.059481         ADDI -19.760291   0.00063   \n",
       "4     2018-01-01 07:00:00     -0.031725         ADDI -19.860175   0.00062   \n",
       "...                   ...           ...          ...        ...       ...   \n",
       "38348 2022-12-31 19:00:00      0.660996         ADDI  11.485986   0.00710   \n",
       "38349 2022-12-31 20:00:00      0.637534         ADDI  10.447198   0.00676   \n",
       "38350 2022-12-31 21:00:00     -0.462142         ADDI   9.658258   0.00709   \n",
       "38351 2022-12-31 22:00:00     -0.493114         ADDI   9.118555   0.00711   \n",
       "38352 2022-12-31 23:00:00      0.067485         ADDI   8.866113   0.00724   \n",
       "\n",
       "        d2m_ADDI    r2_ADDI  u10_ADDI  v10_ADDI  tp_ADDI  ...    td_COHO  \\\n",
       "0     -21.993216  76.800003  3.321081 -2.272873    0.001  ... -22.236069   \n",
       "1     -21.918570  76.500000  3.298830 -1.821096    0.000  ... -21.697083   \n",
       "2     -22.570868  73.800003  2.992857 -0.877062    0.000  ... -21.546570   \n",
       "3     -22.821158  74.400002  3.115693 -0.789671    0.000  ... -21.348312   \n",
       "4     -22.993704  74.099998  3.268302 -0.054269    0.001  ... -21.324707   \n",
       "...          ...        ...       ...       ...      ...  ...        ...   \n",
       "38348   8.142847  80.300003  0.073768  5.399014    0.000  ...   7.202820   \n",
       "38349   7.376611  80.400002 -0.416093  5.292988    0.000  ...   8.407135   \n",
       "38350   8.101648  88.000000  0.234075  5.295895    0.000  ...   8.600677   \n",
       "38351   8.145227  92.000000  0.285457  5.515205    0.038  ...   8.706879   \n",
       "38352   8.439722  94.900002  0.120182  5.397731    0.121  ...   8.577820   \n",
       "\n",
       "       relh_COHO  srad_COHO   pres_COHO   mslp_COHO  wspd_sonic_COHO  \\\n",
       "0      83.005333   0.059599  952.700378  961.160767         4.049551   \n",
       "1      82.664978   0.046355  952.789917  961.068542         3.752380   \n",
       "2      81.791618   0.059598  952.710815  960.902649         4.310488   \n",
       "3      82.621620   0.059596  952.575623  960.738586         4.632712   \n",
       "4      82.821564   0.072838  952.858276  961.028748         3.748342   \n",
       "...          ...        ...         ...         ...              ...   \n",
       "38348  76.627151  56.429420  938.602417  938.342163         2.888842   \n",
       "38349  89.403557  20.635929  938.083618  938.075562         4.008613   \n",
       "38350  91.210388   7.036471  938.252014  938.270630         4.102826   \n",
       "38351  93.957100   0.338529  938.007629  938.103821         3.016000   \n",
       "38352  97.349640   0.200355  937.305176  937.550110         2.162834   \n",
       "\n",
       "       wmax_sonic_COHO  wdir_sonic_COHO  precip_total_COHO  snow_depth_COHO  \n",
       "0             5.268734       320.398987               0.00         0.011778  \n",
       "1             5.467729       302.989014               0.00         0.013398  \n",
       "2             6.174314       306.762512               0.00         0.014291  \n",
       "3             6.812093       296.427094               0.00         0.013012  \n",
       "4             5.010695       292.594391               0.00         0.013992  \n",
       "...                ...              ...                ...              ...  \n",
       "38348         4.793458       187.385101               0.00         0.006084  \n",
       "38349         6.724010       213.705399               0.07         0.005659  \n",
       "38350         7.027788       211.484207               0.00         0.003971  \n",
       "38351         5.212557       189.334000               0.00         0.005106  \n",
       "38352         4.067811       169.763000               0.09         0.005298  \n",
       "\n",
       "[38353 rows x 107 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the error using NWP data.\n",
    "the_df = nwp_error(\"t2m\", \"ADDI\", master_df)\n",
    "the_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df = encode(the_df, \"valid_time\", 366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df = the_df[the_df.columns.drop(list(the_df.filter(regex=\"station\")))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_time</th>\n",
       "      <th>valid_time_cos</th>\n",
       "      <th>valid_time_sin</th>\n",
       "      <th>target_error</th>\n",
       "      <th>t2m_ADDI</th>\n",
       "      <th>sh2_ADDI</th>\n",
       "      <th>d2m_ADDI</th>\n",
       "      <th>r2_ADDI</th>\n",
       "      <th>u10_ADDI</th>\n",
       "      <th>v10_ADDI</th>\n",
       "      <th>...</th>\n",
       "      <th>td_COHO</th>\n",
       "      <th>relh_COHO</th>\n",
       "      <th>srad_COHO</th>\n",
       "      <th>pres_COHO</th>\n",
       "      <th>mslp_COHO</th>\n",
       "      <th>wspd_sonic_COHO</th>\n",
       "      <th>wmax_sonic_COHO</th>\n",
       "      <th>wdir_sonic_COHO</th>\n",
       "      <th>precip_total_COHO</th>\n",
       "      <th>snow_depth_COHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>-0.368794</td>\n",
       "      <td>-19.199194</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.993216</td>\n",
       "      <td>76.800003</td>\n",
       "      <td>3.321081</td>\n",
       "      <td>-2.272873</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.236069</td>\n",
       "      <td>83.005333</td>\n",
       "      <td>0.059599</td>\n",
       "      <td>952.700378</td>\n",
       "      <td>961.160767</td>\n",
       "      <td>4.049551</td>\n",
       "      <td>5.268734</td>\n",
       "      <td>320.398987</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.011778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>-0.600048</td>\n",
       "      <td>-19.118689</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>-21.918570</td>\n",
       "      <td>76.500000</td>\n",
       "      <td>3.298830</td>\n",
       "      <td>-1.821096</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.697083</td>\n",
       "      <td>82.664978</td>\n",
       "      <td>0.046355</td>\n",
       "      <td>952.789917</td>\n",
       "      <td>961.068542</td>\n",
       "      <td>3.752380</td>\n",
       "      <td>5.467729</td>\n",
       "      <td>302.989014</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>-0.803223</td>\n",
       "      <td>-19.439062</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>-22.570868</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>2.992857</td>\n",
       "      <td>-0.877062</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.546570</td>\n",
       "      <td>81.791618</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>952.710815</td>\n",
       "      <td>960.902649</td>\n",
       "      <td>4.310488</td>\n",
       "      <td>6.174314</td>\n",
       "      <td>306.762512</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.014291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>-1.059481</td>\n",
       "      <td>-19.760291</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>-22.821158</td>\n",
       "      <td>74.400002</td>\n",
       "      <td>3.115693</td>\n",
       "      <td>-0.789671</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.348312</td>\n",
       "      <td>82.621620</td>\n",
       "      <td>0.059596</td>\n",
       "      <td>952.575623</td>\n",
       "      <td>960.738586</td>\n",
       "      <td>4.632712</td>\n",
       "      <td>6.812093</td>\n",
       "      <td>296.427094</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>-0.031725</td>\n",
       "      <td>-19.860175</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>-22.993704</td>\n",
       "      <td>74.099998</td>\n",
       "      <td>3.268302</td>\n",
       "      <td>-0.054269</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.324707</td>\n",
       "      <td>82.821564</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>952.858276</td>\n",
       "      <td>961.028748</td>\n",
       "      <td>3.748342</td>\n",
       "      <td>5.010695</td>\n",
       "      <td>292.594391</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.013992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38348</th>\n",
       "      <td>2022-12-31 19:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>0.660996</td>\n",
       "      <td>11.485986</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>8.142847</td>\n",
       "      <td>80.300003</td>\n",
       "      <td>0.073768</td>\n",
       "      <td>5.399014</td>\n",
       "      <td>...</td>\n",
       "      <td>7.202820</td>\n",
       "      <td>76.627151</td>\n",
       "      <td>56.429420</td>\n",
       "      <td>938.602417</td>\n",
       "      <td>938.342163</td>\n",
       "      <td>2.888842</td>\n",
       "      <td>4.793458</td>\n",
       "      <td>187.385101</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38349</th>\n",
       "      <td>2022-12-31 20:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>0.637534</td>\n",
       "      <td>10.447198</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>7.376611</td>\n",
       "      <td>80.400002</td>\n",
       "      <td>-0.416093</td>\n",
       "      <td>5.292988</td>\n",
       "      <td>...</td>\n",
       "      <td>8.407135</td>\n",
       "      <td>89.403557</td>\n",
       "      <td>20.635929</td>\n",
       "      <td>938.083618</td>\n",
       "      <td>938.075562</td>\n",
       "      <td>4.008613</td>\n",
       "      <td>6.724010</td>\n",
       "      <td>213.705399</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.005659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38350</th>\n",
       "      <td>2022-12-31 21:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>-0.462142</td>\n",
       "      <td>9.658258</td>\n",
       "      <td>0.00709</td>\n",
       "      <td>8.101648</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.234075</td>\n",
       "      <td>5.295895</td>\n",
       "      <td>...</td>\n",
       "      <td>8.600677</td>\n",
       "      <td>91.210388</td>\n",
       "      <td>7.036471</td>\n",
       "      <td>938.252014</td>\n",
       "      <td>938.270630</td>\n",
       "      <td>4.102826</td>\n",
       "      <td>7.027788</td>\n",
       "      <td>211.484207</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.003971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38351</th>\n",
       "      <td>2022-12-31 22:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>-0.493114</td>\n",
       "      <td>9.118555</td>\n",
       "      <td>0.00711</td>\n",
       "      <td>8.145227</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.285457</td>\n",
       "      <td>5.515205</td>\n",
       "      <td>...</td>\n",
       "      <td>8.706879</td>\n",
       "      <td>93.957100</td>\n",
       "      <td>0.338529</td>\n",
       "      <td>938.007629</td>\n",
       "      <td>938.103821</td>\n",
       "      <td>3.016000</td>\n",
       "      <td>5.212557</td>\n",
       "      <td>189.334000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38352</th>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-0.017166</td>\n",
       "      <td>0.067485</td>\n",
       "      <td>8.866113</td>\n",
       "      <td>0.00724</td>\n",
       "      <td>8.439722</td>\n",
       "      <td>94.900002</td>\n",
       "      <td>0.120182</td>\n",
       "      <td>5.397731</td>\n",
       "      <td>...</td>\n",
       "      <td>8.577820</td>\n",
       "      <td>97.349640</td>\n",
       "      <td>0.200355</td>\n",
       "      <td>937.305176</td>\n",
       "      <td>937.550110</td>\n",
       "      <td>2.162834</td>\n",
       "      <td>4.067811</td>\n",
       "      <td>169.763000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.005298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38353 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               valid_time  valid_time_cos  valid_time_sin  target_error  \\\n",
       "0     2018-01-01 03:00:00        0.999853        0.017166     -0.368794   \n",
       "1     2018-01-01 04:00:00        0.999853        0.017166     -0.600048   \n",
       "2     2018-01-01 05:00:00        0.999853        0.017166     -0.803223   \n",
       "3     2018-01-01 06:00:00        0.999853        0.017166     -1.059481   \n",
       "4     2018-01-01 07:00:00        0.999853        0.017166     -0.031725   \n",
       "...                   ...             ...             ...           ...   \n",
       "38348 2022-12-31 19:00:00        0.999853       -0.017166      0.660996   \n",
       "38349 2022-12-31 20:00:00        0.999853       -0.017166      0.637534   \n",
       "38350 2022-12-31 21:00:00        0.999853       -0.017166     -0.462142   \n",
       "38351 2022-12-31 22:00:00        0.999853       -0.017166     -0.493114   \n",
       "38352 2022-12-31 23:00:00        0.999853       -0.017166      0.067485   \n",
       "\n",
       "        t2m_ADDI  sh2_ADDI   d2m_ADDI    r2_ADDI  u10_ADDI  v10_ADDI  ...  \\\n",
       "0     -19.199194   0.00068 -21.993216  76.800003  3.321081 -2.272873  ...   \n",
       "1     -19.118689   0.00068 -21.918570  76.500000  3.298830 -1.821096  ...   \n",
       "2     -19.439062   0.00065 -22.570868  73.800003  2.992857 -0.877062  ...   \n",
       "3     -19.760291   0.00063 -22.821158  74.400002  3.115693 -0.789671  ...   \n",
       "4     -19.860175   0.00062 -22.993704  74.099998  3.268302 -0.054269  ...   \n",
       "...          ...       ...        ...        ...       ...       ...  ...   \n",
       "38348  11.485986   0.00710   8.142847  80.300003  0.073768  5.399014  ...   \n",
       "38349  10.447198   0.00676   7.376611  80.400002 -0.416093  5.292988  ...   \n",
       "38350   9.658258   0.00709   8.101648  88.000000  0.234075  5.295895  ...   \n",
       "38351   9.118555   0.00711   8.145227  92.000000  0.285457  5.515205  ...   \n",
       "38352   8.866113   0.00724   8.439722  94.900002  0.120182  5.397731  ...   \n",
       "\n",
       "         td_COHO  relh_COHO  srad_COHO   pres_COHO   mslp_COHO  \\\n",
       "0     -22.236069  83.005333   0.059599  952.700378  961.160767   \n",
       "1     -21.697083  82.664978   0.046355  952.789917  961.068542   \n",
       "2     -21.546570  81.791618   0.059598  952.710815  960.902649   \n",
       "3     -21.348312  82.621620   0.059596  952.575623  960.738586   \n",
       "4     -21.324707  82.821564   0.072838  952.858276  961.028748   \n",
       "...          ...        ...        ...         ...         ...   \n",
       "38348   7.202820  76.627151  56.429420  938.602417  938.342163   \n",
       "38349   8.407135  89.403557  20.635929  938.083618  938.075562   \n",
       "38350   8.600677  91.210388   7.036471  938.252014  938.270630   \n",
       "38351   8.706879  93.957100   0.338529  938.007629  938.103821   \n",
       "38352   8.577820  97.349640   0.200355  937.305176  937.550110   \n",
       "\n",
       "       wspd_sonic_COHO  wmax_sonic_COHO  wdir_sonic_COHO  precip_total_COHO  \\\n",
       "0             4.049551         5.268734       320.398987               0.00   \n",
       "1             3.752380         5.467729       302.989014               0.00   \n",
       "2             4.310488         6.174314       306.762512               0.00   \n",
       "3             4.632712         6.812093       296.427094               0.00   \n",
       "4             3.748342         5.010695       292.594391               0.00   \n",
       "...                ...              ...              ...                ...   \n",
       "38348         2.888842         4.793458       187.385101               0.00   \n",
       "38349         4.008613         6.724010       213.705399               0.07   \n",
       "38350         4.102826         7.027788       211.484207               0.00   \n",
       "38351         3.016000         5.212557       189.334000               0.00   \n",
       "38352         2.162834         4.067811       169.763000               0.09   \n",
       "\n",
       "       snow_depth_COHO  \n",
       "0             0.011778  \n",
       "1             0.013398  \n",
       "2             0.014291  \n",
       "3             0.013012  \n",
       "4             0.013992  \n",
       "...                ...  \n",
       "38348         0.006084  \n",
       "38349         0.005659  \n",
       "38350         0.003971  \n",
       "38351         0.005106  \n",
       "38352         0.005298  \n",
       "\n",
       "[38353 rows x 103 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_carry = [\"valid_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = the_df.drop(columns=cols_to_carry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "\n",
    "cols = [\"valid_time_cos\", \"valid_time_sin\"]\n",
    "for k, r in new_df.items():\n",
    "    if k in (cols):\n",
    "        continue\n",
    "    else:\n",
    "        means = st.mean(new_df[k])\n",
    "        stdevs = st.pstdev(new_df[k])\n",
    "        new_df[k] = (new_df[k] - means) / stdevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asnow_ADDI', 'asnow_BELM', 'asnow_COHO', 'cape_ADDI', 'cape_BELM', 'cape_COHO', 'd2m_ADDI', 'd2m_BELM', 'd2m_COHO', 'dlwrf_ADDI', 'dlwrf_BELM', 'dlwrf_COHO', 'dswrf_ADDI', 'dswrf_BELM', 'dswrf_COHO', 'elev_ADDI', 'elev_BELM', 'elev_COHO', 'gh_ADDI', 'gh_BELM', 'gh_COHO', 'lat_ADDI', 'lat_BELM', 'lat_COHO', 'lon_ADDI', 'lon_BELM', 'lon_COHO', 'mslma_ADDI', 'mslma_BELM', 'mslma_COHO', 'mslp_ADDI', 'mslp_BELM', 'mslp_COHO', 'new_tp_ADDI', 'new_tp_BELM', 'new_tp_COHO', 'orog_ADDI', 'orog_BELM', 'orog_COHO', 'precip_total_ADDI', 'precip_total_BELM', 'precip_total_COHO', 'pres_ADDI', 'pres_BELM', 'pres_COHO', 'r2_ADDI', 'r2_BELM', 'r2_COHO', 'relh_ADDI', 'relh_BELM', 'relh_COHO', 'sh2_ADDI', 'sh2_BELM', 'sh2_COHO', 'snow_depth_ADDI', 'snow_depth_BELM', 'snow_depth_COHO', 'srad_ADDI', 'srad_BELM', 'srad_COHO', 't2m_ADDI', 't2m_BELM', 't2m_COHO', 'ta9m_ADDI', 'ta9m_BELM', 'ta9m_COHO', 'tair_ADDI', 'tair_BELM', 'tair_COHO', 'tcc_ADDI', 'tcc_BELM', 'tcc_COHO', 'td_ADDI', 'td_BELM', 'td_COHO', 'tp_ADDI', 'tp_BELM', 'tp_COHO', 'u10_ADDI', 'u10_BELM', 'u10_COHO', 'u_dir_ADDI', 'u_dir_BELM', 'u_dir_COHO', 'u_total_ADDI', 'u_total_BELM', 'u_total_COHO', 'v10_ADDI', 'v10_BELM', 'v10_COHO', 'valid_time_cos', 'valid_time_sin', 'wdir_sonic_ADDI', 'wdir_sonic_BELM', 'wdir_sonic_COHO', 'wmax_sonic_ADDI', 'wmax_sonic_BELM', 'wmax_sonic_COHO', 'wspd_sonic_ADDI', 'wspd_sonic_BELM', 'wspd_sonic_COHO']\n"
     ]
    }
   ],
   "source": [
    "features = list(new_df.columns.difference([\"target_error\"]))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_score(the_df):\n",
    "    the_df = the_df.fillna(-999)\n",
    "    X = the_df.loc[:, the_df.columns != \"target_error\"]\n",
    "    y = the_df[\"target_error\"]\n",
    "    # convert y values to categorical values\n",
    "    lab = preprocessing.LabelEncoder()\n",
    "    y_transformed = lab.fit_transform(y)\n",
    "    mi_score = MIC(X, y_transformed)\n",
    "    df = pd.DataFrame()\n",
    "    df[\"feature\"] = [n for n in the_df.columns if n != \"target_error\"]\n",
    "    df[\"mi_score\"] = mi_score\n",
    "\n",
    "    df = df[df[\"mi_score\"] > 0.2]\n",
    "    features = df[\"feature\"].tolist()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mi_score(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t2m_ADDI',\n",
       " 'tp_ADDI',\n",
       " 'orog_ADDI',\n",
       " 'tcc_ADDI',\n",
       " 'asnow_ADDI',\n",
       " 'cape_ADDI',\n",
       " 'dswrf_ADDI',\n",
       " 'new_tp_ADDI',\n",
       " 't2m_BELM',\n",
       " 'tp_BELM',\n",
       " 'orog_BELM',\n",
       " 'tcc_BELM',\n",
       " 'asnow_BELM',\n",
       " 'cape_BELM',\n",
       " 'dswrf_BELM',\n",
       " 'new_tp_BELM',\n",
       " 't2m_COHO',\n",
       " 'tp_COHO',\n",
       " 'orog_COHO',\n",
       " 'tcc_COHO',\n",
       " 'asnow_COHO',\n",
       " 'cape_COHO',\n",
       " 'dswrf_COHO',\n",
       " 'new_tp_COHO',\n",
       " 'lat_ADDI',\n",
       " 'lon_ADDI',\n",
       " 'elev_ADDI',\n",
       " 'tair_ADDI',\n",
       " 'ta9m_ADDI',\n",
       " 'mslp_ADDI',\n",
       " 'precip_total_ADDI',\n",
       " 'snow_depth_ADDI',\n",
       " 'lat_BELM',\n",
       " 'lon_BELM',\n",
       " 'elev_BELM',\n",
       " 'tair_BELM',\n",
       " 'ta9m_BELM',\n",
       " 'pres_BELM',\n",
       " 'mslp_BELM',\n",
       " 'precip_total_BELM',\n",
       " 'snow_depth_BELM',\n",
       " 'lat_COHO',\n",
       " 'lon_COHO',\n",
       " 'elev_COHO',\n",
       " 'tair_COHO',\n",
       " 'ta9m_COHO',\n",
       " 'precip_total_COHO',\n",
       " 'snow_depth_COHO']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df = new_df.copy()\n",
    "target_sensor = \"target_error\"\n",
    "forecast_lead = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = f\"{target_sensor}_lead_{forecast_lead}\"\n",
    "lstm_df.insert(\n",
    "    loc=(0), column=target, value=lstm_df[target_sensor].shift(-forecast_lead)\n",
    ")\n",
    "lstm_df = lstm_df.drop(columns=[target_sensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df = lstm_df.iloc[:-forecast_lead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets.\n",
    "length = len(lstm_df)\n",
    "test_len = int(length * 0.2)\n",
    "df_train = lstm_df.iloc[test_len:].copy()\n",
    "df_test = lstm_df.iloc[:test_len].copy()\n",
    "print(\"Test Set Fraction\", len(df_test) / len(lstm_df))\n",
    "\n",
    "# Fill missing values with zeros in the training and testing DataFrames.\n",
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=\"target_error_lead_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[:2, -int(3 * 15) :] = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in df_train.keys():\n",
    "    print(k)\n",
    "    print(df_train[k].iloc[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM Model\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, dataframe, target, features, stations, sequence_length, forecast_hr\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stations = stations\n",
    "        self.forecast_hr = forecast_hr\n",
    "        self.y = (\n",
    "            torch.tensor(dataframe[target].values)\n",
    "            .float()\n",
    "            .to(int(os.environ[\"RANK\"]) % torch.cuda.device_count())\n",
    "        )\n",
    "        self.X = (\n",
    "            torch.tensor(dataframe[features].values)\n",
    "            .float()\n",
    "            .to(int(os.environ[\"RANK\"]) % torch.cuda.device_count())\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start : (i + 1), :]\n",
    "            # zero out NYSM vars from before present\n",
    "            x[:forecast_hr, -int(len(stations) * 15) :] = -999.0\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0 : (i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_climate_df(data_path):\n",
    "    \"\"\"\n",
    "    Formats a climate data file located at the specified `data_path` into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The file path for the climate data file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the climate data, with the first column renamed to \"year\".\n",
    "    \"\"\"\n",
    "    raw_index = np.loadtxt(f\"{data_path}\")\n",
    "    cl_index = pd.DataFrame(raw_index)\n",
    "    cl_index = cl_index.rename(columns={0: \"year\"})\n",
    "    return cl_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clim_indexes(df, valid_times):\n",
    "    \"\"\"\n",
    "    Fetch climate indexes data and add corresponding index values to the input DataFrame.\n",
    "\n",
    "    This function takes a DataFrame (`df`) containing weather data with a 'valid_time' column representing\n",
    "    timestamps. It reads climate indexes data from text files in the specified directory and extracts index\n",
    "    values corresponding to the month and year of each timestamp in the DataFrame. The extracted index values\n",
    "    are then added to the DataFrame with new columns named after each index.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame containing weather data with a 'valid_time' column.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with additional columns for each climate index containing their values.\n",
    "    \"\"\"\n",
    "\n",
    "    clim_df_path = \"/home/aevans/nwp_bias/src/correlation/data/indexes/\"\n",
    "    directory = os.listdir(clim_df_path)\n",
    "    df[\"valid_time\"] = valid_times\n",
    "\n",
    "    # Loop through each file in the specified directory\n",
    "    for d in directory:\n",
    "        if d.endswith(\".txt\"):\n",
    "            # Read the climate index data from the file and format it into a DataFrame\n",
    "            clim_df = format_climate_df(f\"{clim_df_path}{d}\")\n",
    "            index_name = d.split(\".\")[0]\n",
    "\n",
    "            clim_ind_ls = []\n",
    "            for t, _ in enumerate(df[\"valid_time\"]):\n",
    "                time_obj = df[\"valid_time\"].iloc[t]\n",
    "                dt_object = parse(str(time_obj))\n",
    "                year = dt_object.strftime(\"%Y\")\n",
    "                month = dt_object.strftime(\"%m\")\n",
    "                # Filter the climate DataFrame to get data for the specific year\n",
    "                df1 = clim_df.loc[clim_df[\"year\"] == int(year)]\n",
    "                df1 = df1.drop(columns=\"year\")\n",
    "                row_list = df1.values\n",
    "                keys = df1.keys()\n",
    "                key_vals = keys.tolist()\n",
    "\n",
    "                # Extract the index value corresponding to the month of the timestamp\n",
    "                the_list = []\n",
    "                for n, _ in enumerate(key_vals):\n",
    "                    val1 = key_vals[n]\n",
    "                    val2 = row_list[0, n]\n",
    "                    tup = (val1, val2)\n",
    "                    the_list.append(tup)\n",
    "                for k, r in the_list:\n",
    "                    if str(k).zfill(2) == month:\n",
    "                        clim_ind_ls.append(r)\n",
    "\n",
    "            # Add the climate index values as a new column in the DataFrame\n",
    "            df[index_name] = clim_ind_ls\n",
    "\n",
    "    df = df.drop(columns=\"valid_time\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"/home/aevans/nwp_bias/src/machine_learning/data/clean_parquets/nysm_cats/cleaned_rough_lstm_nysmcat_Western Plateau.parquet\"\n",
    ")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_times = df[\"valid_time\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in df.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to reintigrate back into the df after model is done running\n",
    "cols_to_carry = cols_to_carry = [\n",
    "    \"valid_time\",\n",
    "    \"flag\",\n",
    "    \"day_of_year_sin\",\n",
    "    \"day_of_year_cos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_flag(df)\n",
    "df = nwp_error(\"t2m\", \"ADDI\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df, valid_times):\n",
    "    print(\"init normalizer\")\n",
    "    df = col_drop(df)\n",
    "    the_df = df.dropna()\n",
    "    for k, r in the_df.items():\n",
    "        if len(the_df[k].unique()) == 1:\n",
    "            org_str = str(k)\n",
    "            my_str = org_str[:-5]\n",
    "            vals = the_df.filter(regex=my_str)\n",
    "            vals = vals.loc[0].tolist()\n",
    "            means = st.mean(vals)\n",
    "            stdevs = st.pstdev(vals)\n",
    "            the_df[k] = (the_df[k] - means) / stdevs\n",
    "\n",
    "            the_df = the_df.fillna(0)\n",
    "            # |sh2|d2m|r2|u10|v10|tp|mslma|tcc|asnow|cape|dswrf|dlwrf|gh|utotal|u_dir|new_tp\n",
    "        if re.search(\n",
    "            \"t2m\",\n",
    "            k,\n",
    "        ):\n",
    "            ind_val = the_df.columns.get_loc(k)\n",
    "            x = the_df[k]\n",
    "            imf = emd.sift.sift(x)\n",
    "            the_df = the_df.drop(columns=k)\n",
    "            for i in range(imf.shape[1]):\n",
    "                imf_ls = imf[:, i].tolist()\n",
    "                # Inserting the column at the\n",
    "                # beginning in the DataFrame\n",
    "                my_loc = ind_val + i\n",
    "                the_df.insert(loc=(my_loc), column=f\"{k}_imf_{i}\", value=imf_ls)\n",
    "\n",
    "        else:\n",
    "            means = st.mean(the_df[k])\n",
    "            stdevs = st.pstdev(the_df[k])\n",
    "            the_df[k] = (the_df[k] - means) / stdevs\n",
    "\n",
    "    final_df = the_df.fillna(0)\n",
    "    print(\"!!! Dropping Columns !!!\")\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"latitude\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"longitude\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"u_total\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"mslp\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"orog\")))]\n",
    "\n",
    "    print(\"--- configuring data ---\")\n",
    "    final_df = encode(final_df, \"day_of_year\", 366, valid_times)\n",
    "    final_df = get_clim_indexes(final_df, valid_times)\n",
    "    new_features = list(final_df.columns.difference([\"target_error\"]))\n",
    "    print(\"---normalize successful---\")\n",
    "\n",
    "    return final_df, new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df_station(df):\n",
    "    print(\"init normalizer\")\n",
    "    the_df = df.dropna()\n",
    "    for k, r in the_df.items():\n",
    "        if len(the_df[k].unique()) == 1:\n",
    "            org_str = str(k)\n",
    "            my_str = org_str[:-5]\n",
    "            vals = the_df.filter(regex=my_str)\n",
    "            vals = vals.loc[0].tolist()\n",
    "            means = st.mean(vals)\n",
    "            stdevs = st.pstdev(vals)\n",
    "            the_df[k] = (the_df[k] - means) / stdevs\n",
    "\n",
    "            the_df = the_df.fillna(0)\n",
    "        if not (len(the_df[k].unique()) == 1) and re.search(\"_ADDI\", k):\n",
    "            ind_val = the_df.columns.get_loc(k)\n",
    "            x = the_df[k]\n",
    "            imf = emd.sift.sift(x)\n",
    "            the_df = the_df.drop(columns=k)\n",
    "            for i in range(imf.shape[1]):\n",
    "                imf_ls = imf[:, i].tolist()\n",
    "                # Inserting the column at the\n",
    "                # beginning in the DataFrame\n",
    "                my_loc = ind_val + i\n",
    "                the_df.insert(loc=(my_loc), column=f\"{k}_imf_{i}\", value=imf_ls)\n",
    "\n",
    "        else:\n",
    "            means = st.mean(the_df[k])\n",
    "            stdevs = st.pstdev(the_df[k])\n",
    "            the_df[k] = (the_df[k] - means) / stdevs\n",
    "\n",
    "    final_df = the_df.fillna(0)\n",
    "    print(\"!!! Dropping Columns !!!\")\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"latitude\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"longitude\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"u_total\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"mslp\")))]\n",
    "    final_df = final_df[final_df.columns.drop(list(final_df.filter(regex=\"orog\")))]\n",
    "    new_features = list(final_df.columns.difference([\"target_error\"]))\n",
    "    print(\"---normalize successful---\")\n",
    "    return final_df, new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sensor = \"target_error\"\n",
    "the_df, new_features = normalize_df(new_df, valid_times)\n",
    "\n",
    "forecast_lead = 30\n",
    "target = f\"{target_sensor}_lead_{forecast_lead}\"\n",
    "\n",
    "the_df[target] = the_df[target_sensor].shift(-forecast_lead)\n",
    "the_df = the_df.iloc[:-forecast_lead]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in the_df.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(the_df)\n",
    "\n",
    "test_len = int(length * 0.75)\n",
    "\n",
    "df_train = the_df.iloc[:test_len].copy()\n",
    "df_test = the_df.iloc[test_len:].copy()\n",
    "\n",
    "print(\"Test Set Fraction\", len(df_test) / len(the_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cols_to_carry:\n",
    "    df_train[c] = df[c]\n",
    "    df_test[c] = df[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna(0)\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# data = torch.tensor(df_train.values)\n",
    "\n",
    "\n",
    "# # Perform PCA\n",
    "# U, S, V = torch.pca_lowrank(data)\n",
    "\n",
    "# # Print the results\n",
    "# print(U)\n",
    "# print(S)\n",
    "# print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=10)\n",
    "# pca.fit(df_train)\n",
    "# print(pca.components_) # Reformat and view results\n",
    "# loadings = pd.DataFrame(pca.components_.T,\n",
    "# columns=['PC%s' % _ for _ in range((pca.components_.shape[0]))],\n",
    "# index=df_train.columns)\n",
    "# print(loadings)\n",
    "\n",
    "# plt.plot(pca.explained_variance_ratio_)\n",
    "# plt.ylabel('Explained Variance')\n",
    "# plt.xlabel('Components')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(pca.components_)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, target, features, sequence_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.y = torch.tensor(dataframe[target].values).float()\n",
    "        self.X = torch.tensor(dataframe[features].values).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        keep_sample = self.dataframe.iloc[i][\"flag\"]\n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start : (i + 1), :]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0 : (i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "\n",
    "        return x, self.y[i], keep_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PeepholeLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PeepholeLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_i = nn.Linear(input_size, hidden_size)\n",
    "        self.W_f = nn.Linear(input_size, hidden_size)\n",
    "        self.W_o = nn.Linear(input_size, hidden_size)\n",
    "        self.W_c = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.U_i = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_f = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_o = nn.Linear(hidden_size, hidden_size)\n",
    "        self.U_c = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        i = torch.sigmoid(self.W_i(x) + self.U_i(h_prev) + self.W_c(c_prev))\n",
    "        f = torch.sigmoid(self.W_f(x) + self.U_f(h_prev) + self.W_c(c_prev))\n",
    "        c_tilde = torch.tanh(self.W_c(x) + self.U_c(h_prev))\n",
    "        c = f * c_prev + i * c_tilde\n",
    "        o = torch.sigmoid(self.W_o(x) + self.U_o(h_prev) + self.W_c(c))\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "\n",
    "class ShallowRegressionPeepholeLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units):\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 3\n",
    "\n",
    "        # Create a list of LSTM cells\n",
    "        self.lstm_cells = nn.ModuleList([PeepholeLSTMCell(num_sensors, hidden_units)])\n",
    "\n",
    "        # # Add additional LSTM layers if needed\n",
    "        # for _ in range(1, self.num_layers):\n",
    "        #     self.lstm_cells.append(PeepholeLSTMCell(hidden_units, hidden_units))\n",
    "\n",
    "        self.linear = nn.Linear(in_features=hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(batch_size, self.hidden_units).requires_grad_()\n",
    "        h, c = h0, c0\n",
    "\n",
    "        # Forward pass through each LSTM cell\n",
    "        for lstm_cell in self.lstm_cells:\n",
    "            h, c = lstm_cell(x, h, c)\n",
    "\n",
    "        out = self.linear(h[0]).flatten()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_elements_from_batch(X, y, s):\n",
    "    cond = np.where(s)\n",
    "    return X[cond], y[cond], s[cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        for X, y, s in tepoch:\n",
    "            # X, y, s = remove_elements_from_batch(X, y, s)\n",
    "            output = model(X)\n",
    "            loss = loss_function(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "            for X, y, s in tepoch:\n",
    "                # X, y, s = remove_elements_from_batch(X, y, s)\n",
    "                output = model(X)\n",
    "                total_loss += loss_function(output, y).item()\n",
    "\n",
    "    # loss\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(101)\n",
    "batch_size = 400\n",
    "sequence_length = 400\n",
    "learning_rate = 5e-4\n",
    "num_hidden_units = 550\n",
    "\n",
    "experiment = Experiment(\n",
    "    api_key=\"leAiWyR5Ck7tkdiHIT7n6QWNa\",\n",
    "    project_name=\"fh_2_hrrr\",\n",
    "    workspace=\"shmaronshmevans\",\n",
    ")\n",
    "# # Report multiple hyperparameters using a dictionary:\n",
    "# hyper_params = {\n",
    "#     \"num_layers\": num_layers,\n",
    "#     \"learning_rate\": learning_rate,\n",
    "#     \"sequence_length\": sequence_length,\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"num_hidden_units\": num_hidden_units,\n",
    "#     \"forecast_lead\": forecast_lead,\n",
    "# }\n",
    "\n",
    "batch_size = batch_size\n",
    "sequence_length = sequence_length\n",
    "\n",
    "train_dataset = SequenceDataset(\n",
    "    df_train, target=target, features=new_features, sequence_length=sequence_length\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test, target=target, features=new_features, sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "X, y, s = next(iter(train_loader))\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "learning_rate = learning_rate\n",
    "num_hidden_units = num_hidden_units\n",
    "\n",
    "model = ShallowRegressionPeepholeLSTM(\n",
    "    num_sensors=len(new_features), hidden_units=num_hidden_units\n",
    ")\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "early_stopper = EarlyStopper(patience=10, min_delta=0)\n",
    "\n",
    "\n",
    "print(\"Untrained test\\n--------\")\n",
    "test_model(test_loader, model, loss_function)\n",
    "print()\n",
    "\n",
    "for ix_epoch in range(3):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    train_loss = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "    val_loss = test_model(test_loader, model, loss_function)\n",
    "    print()\n",
    "    experiment.log_epoch_end(ix_epoch)\n",
    "    # experiment.log_parameters(hyper_params, step = ix_epoch)\n",
    "    if early_stopper.early_stop(val_loss):\n",
    "        break\n",
    "\n",
    "# Seamlessly log your Pytorch model\n",
    "log_model(experiment, model, model_name=\"exp1\")\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    output = torch.tensor([])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _, s in data_loader:\n",
    "            y_star = model(X)\n",
    "            # print(y_star)\n",
    "            output = torch.cat((output, y_star), 0)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "ystar_col = \"Model forecast\"\n",
    "df_train[ystar_col] = predict(train_eval_loader, model).numpy()\n",
    "df_test[ystar_col] = predict(test_loader, model).numpy()\n",
    "print(df_test[ystar_col])\n",
    "\n",
    "df_out = pd.concat((df_train, df_test))[[target, ystar_col]]\n",
    "\n",
    "for c in df_out.columns:\n",
    "    vals = df_out[c].values.tolist()\n",
    "    mean = st.mean(vals)\n",
    "    std = st.pstdev(vals)\n",
    "    df_out[c] = df_out[c] * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"plotly_white\"\n",
    "plot_template = dict(\n",
    "    layout=go.Layout(\n",
    "        {\"font_size\": 18, \"xaxis_title_font_size\": 24, \"yaxis_title_font_size\": 24}\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig = px.line(df_out, labels=dict(created_at=\"Date\", value=\"Forecast Error\"))\n",
    "fig.add_vline(x=(length * 0.75), line_width=4, line_dash=\"dash\")\n",
    "fig.add_annotation(\n",
    "    xref=\"paper\", x=0.75, yref=\"paper\", y=0.8, text=\"Test set start\", showarrow=False\n",
    ")\n",
    "fig.update_layout(\n",
    "    template=plot_template, legend=dict(orientation=\"h\", y=1.02, title_text=\"\")\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Use the training data for deep explainer => can use fewer instances\n",
    "# explainer = shap.DeepExplainer(model, X_train)\n",
    "# # explain the the testing instances (can use fewer instanaces)\n",
    "# # explaining each prediction requires 2 * background dataset size runs\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "# # init the JS visualization code\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0][0], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn related imports\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# pytorch relates imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    DeepLift,\n",
    "    GradientShap,\n",
    "    NoiseTunnel,\n",
    "    FeatureAblation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).view(-1, 1).float()\n",
    "\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).view(-1, 1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# # Use the training data for deep explainer => can use fewer instances\n",
    "# explainer = shap.DeepExplainer(model, y_train)\n",
    "# # explain the the testing instances (can use fewer instanaces)\n",
    "# # explaining each prediction requires 2 * background dataset size runs\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "# # init the JS visualization code\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value[0], shap_values[0][0], features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "\n",
    "print(\"model err: \", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "ig_nt = NoiseTunnel(ig)\n",
    "dl = DeepLift(model)\n",
    "gs = GradientShap(model)\n",
    "fa = FeatureAblation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_nt_attr_test = ig_nt.attribute(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ig_attr_test_norm_sum.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.shape[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_nt_attr_test_norm_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "e = 20\n",
    "n = 10\n",
    "# prepare attributions for visualization\n",
    "\n",
    "x_axis_data = np.arange(X_test.shape[1])\n",
    "x_axis_data_labels = list(map(lambda idx: new_features[idx], x_axis_data))\n",
    "\n",
    "while e < len(x_axis_data):\n",
    "    ig_nt_attr_test_sum = ig_nt_attr_test.detach().numpy().sum(0)\n",
    "    ig_nt_attr_test_norm_sum = ig_nt_attr_test_sum / np.linalg.norm(\n",
    "        ig_nt_attr_test_sum, ord=1\n",
    "    )\n",
    "\n",
    "    lin_weight = model.linear.weight[0].detach().numpy()\n",
    "    y_axis_lin_weight = lin_weight / np.linalg.norm(lin_weight, ord=1)\n",
    "\n",
    "    width = 0.14\n",
    "    legends = [\"Int Grads w/SmoothGrad\", \"Weights\"]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    ax.set_title(\n",
    "        \"Comparing input feature importances across multiple algorithms and learned weights\"\n",
    "    )\n",
    "    ax.set_ylabel(\"Attributions\")\n",
    "\n",
    "    FONT_SIZE = 16\n",
    "    plt.rc(\"font\", size=FONT_SIZE)  # fontsize of the text sizes\n",
    "    plt.rc(\"axes\", titlesize=FONT_SIZE)  # fontsize of the axes title\n",
    "    plt.rc(\"axes\", labelsize=FONT_SIZE)  # fontsize of the x and y labels\n",
    "    plt.rc(\"legend\", fontsize=FONT_SIZE - 4)  # fontsize of the legend\n",
    "\n",
    "    print(x_axis_data.shape)\n",
    "\n",
    "    ax.bar(\n",
    "        x_axis_data[b:e] + width,\n",
    "        ig_nt_attr_test_norm_sum[b:e, n],\n",
    "        width,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "        color=\"#A90000\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        x_axis_data[b:e] + 5 * width,\n",
    "        y_axis_lin_weight[b:e],\n",
    "        width,\n",
    "        align=\"center\",\n",
    "        alpha=1.0,\n",
    "        color=\"grey\",\n",
    "    )\n",
    "    ax.autoscale_view()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    ax.set_xticks(x_axis_data[b:e] + 0.5)\n",
    "    ax.set_xticklabels(x_axis_data_labels[b:e], rotation=90)\n",
    "\n",
    "    plt.legend(legends, loc=3)\n",
    "    plt.show()\n",
    "\n",
    "    b += 20\n",
    "    e += 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of feature columns = 50\n",
    "Compute LSTM Feature Importance\n",
    "After we train (or load) each fold model, we will compute LSTM feature importance for all of our features. We do this with a for-loop of size N where N is the number of features we have. \n",
    "\n",
    "For each feature we wish to evaluate, we infer our OOF with that feature column randomly shuffled. If this feature column is important to our LSTM model, then the OOF MAE will become worse for that for-loop step. After our for-loop, we display bars equal to the size of how much MAE worsened without each feature, which is the importance of each feature.\n",
    "\n",
    "Note that computing LSTM feature importance after each fold will add about 1 minute for every 5 features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
