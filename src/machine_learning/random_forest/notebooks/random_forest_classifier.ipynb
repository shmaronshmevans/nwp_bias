{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "# instead of creating a package using setup.py or building from a docker/singularity file,\n",
    "# import the sister directory of src code to be called on in notebook.\n",
    "# This keeps the notebook free from code to only hold visualizations and is easier to test\n",
    "# It also helps keep the state of variables clean such that cells aren't run out of order with a mysterious state\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCode taken from: \\n\\nhttps://towardsdatascience.com/how-to-train-a-regression-model-using-a-random-forest-c1cf16288f6b\\n\\nhttps://medium.com/@pratyush.sinha/training-random-forest-by-back-propagation-for-fun-pytorch-part-1-a54674355aa7\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Code taken from: \n",
    "\n",
    "https://towardsdatascience.com/how-to-train-a-regression-model-using-a-random-forest-c1cf16288f6b\n",
    "\n",
    "https://medium.com/@pratyush.sinha/training-random-forest-by-back-propagation-for-fun-pytorch-part-1-a54674355aa7\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from data import create_data_for_lstm\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am I using GPUS ??? True\n",
      "Number of gpus:  1\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4b52c744d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Am I using GPUS ???\", torch.cuda.is_available())\n",
    "print(\"Number of gpus: \", torch.cuda.device_count())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print(device)\n",
    "torch.manual_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LSTM Model\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe,\n",
    "        target,\n",
    "        features,\n",
    "        stations,\n",
    "        sequence_length,\n",
    "        forecast_hr,\n",
    "        device,\n",
    "    ):\n",
    "        self.dataframe = dataframe\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.sequence_length = sequence_length\n",
    "        self.stations = stations\n",
    "        self.forecast_hr = forecast_hr\n",
    "        self.device = device\n",
    "        self.y = torch.tensor(dataframe[target].values).float().to(device)\n",
    "        self.X = torch.tensor(dataframe[features].values).float().to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i >= self.sequence_length - 1:\n",
    "            i_start = i - self.sequence_length + 1\n",
    "            x = self.X[i_start : (i + 1), :]\n",
    "            x[: self.forecast_hr, -int(len(self.stations) * 15) :] = x[\n",
    "                self.forecast_hr + 1, -int(len(self.stations) * 15) :\n",
    "            ]\n",
    "        else:\n",
    "            padding = self.X[0].repeat(self.sequence_length - i - 1, 1)\n",
    "            x = self.X[0 : (i + 1), :]\n",
    "            x = torch.cat((padding, x), 0)\n",
    "        \n",
    "        # x = x.reshape(X.shape[1]*self.sequence_length, 784)\n",
    "        return x, self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting Error for BKLN\n",
      "-- loading data from NYSM --\n",
      "-- loading data from HRRR --\n",
      "now = 2024-01-19 19:23:56.695599\n",
      "Test Set Fraction 0.20000445394619634\n",
      "Data Processed\n"
     ]
    }
   ],
   "source": [
    "station = 'BKLN'\n",
    "fh = 4\n",
    "sequence_length = 120\n",
    "batch_size = int(20e2)\n",
    "df_train, df_test, features, forecast_lead, stations, target = create_data_for_lstm.create_data_for_model(station, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(\n",
    "    df_train,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    stations=stations,\n",
    "    sequence_length=sequence_length,\n",
    "    forecast_hr=fh,\n",
    "    device=device,\n",
    ")\n",
    "test_dataset = SequenceDataset(\n",
    "    df_test,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    stations=stations,\n",
    "    sequence_length=sequence_length,\n",
    "    forecast_hr=fh,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = {\"batch_size\": batch_size, \"pin_memory\": False, \"shuffle\": True}\n",
    "test_kwargs = {\"batch_size\": batch_size, \"pin_memory\": False, \"shuffle\": False}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_selection_node(nn.Module):\n",
    "    def __init__(self, number_of_trees, batch_size, device):\n",
    "        super(feature_selection_node, self).__init__()\n",
    "        self.number_of_trees = number_of_trees\n",
    "        self.attention_mask = torch.nn.Parameter(data=torch.Tensor(number_of_trees, 1000), requires_grad=True,)\n",
    "        self.attention_mask.data.uniform_(-1.0,1.0)\n",
    "        self.batch = batch_size\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.to(self.device)\n",
    "        x = x.view(-1,(x.shape[1]*x.shape[2]))\n",
    "        attention_tmp = torch.sigmoid(self.attention_mask).to(self.device)\n",
    "        #scatter mask by only keeping top 200 vals and reset rest to 0\n",
    "        topk, idx = torch.topk(attention_tmp, k=200, dim=-1)\n",
    "        topk.to(self.device)\n",
    "        idx.to(self.device)\n",
    "        attention = torch.zeros(self.number_of_trees, 16080).to(self.device)\n",
    "        attention.scatter_(-1,idx, topk)\n",
    "        return_value=torch.zeros(self.batch,self.number_of_trees, 16080)\n",
    "        print(x.shape)\n",
    "        print(topk.shape)\n",
    "        print(idx.shape)\n",
    "        for mask_index in range(0,self.number_of_trees):\n",
    "            return_value[:,mask_index,:] = x*attention[mask_index]\n",
    "        return return_value, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decision_node(nn.Module):\n",
    "    def __init__(self, number_of_trees, max_num_of_leaf_nodes, classes, batch, device):\n",
    "        super(decision_node, self).__init__()\n",
    "        self.leaf = max_num_of_leaf_nodes\n",
    "        self.tree = number_of_trees\n",
    "        self.classes = classes\n",
    "        self.batch = batch\n",
    "        self.symbolic_path_weights = nn.Linear(16080,max_num_of_leaf_nodes, bias=True)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.contribution = torch.nn.Parameter(data=torch.Tensor(number_of_trees,max_num_of_leaf_nodes, classes), requires_grad=True)\n",
    "        self.contribution.data.uniform_(-1.0, 1.0)\n",
    "        self.device = device\n",
    "        #define trainable params here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.to(self.device)\n",
    "        # use trainable params to define compuatations here\n",
    "        class_value = torch.randn(self.batch,self.tree, self.leaf, self.classes)\n",
    "        symbolic_paths = self.hardtanh(self.symbolic_path_weights(x))\n",
    "\n",
    "        for tree_index in range(0,self.tree):\n",
    "            for decision_index in range(0, self.leaf):\n",
    "                class_value[:,tree_index, decision_index,:]=torch.mm(symbolic_paths[:,tree_index, decision_index].view(-1,1),self.contribution[tree_index, decision_index].view(1,-1))\n",
    "        class_value=self.softmax(class_value)\n",
    "        class_value = 1.0-class_value*class_value\n",
    "        class_value = class_value.sum(dim=-1)\n",
    "        return symbolic_paths, class_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(d):\n",
    "    dic={}\n",
    "\n",
    "    for item in d:\n",
    "        if item in dic.keys():\n",
    "            dic[item] = dic[item]+1\n",
    "        else:\n",
    "            dic[item]=1\n",
    "    \n",
    "    dic = {\"value\":dic.keys(), \"count\":dic.values()}\n",
    "    df = pd.DataFrame.from_dict(dic, orient='index').transpose().sort_values(['value'])\n",
    "    df['cum'] = df['count']/df['count'].sum()\n",
    "    value = df['cum'].values\n",
    "    value = torch.from_numpy(value).float()\n",
    "    value = 1-value*value\n",
    "    value = value.sum(-1)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epoch, device):\n",
    "    print(\"Hello World!\")\n",
    "    print(device)\n",
    "    mask.train()\n",
    "    decision.train()\n",
    "    flag = torch.ones(1000, 100, 200)\n",
    "    flag = flag.to(device)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        print(batch_idx)\n",
    "        optimizer.zero_grad()\n",
    "        masked_output, attention = mask(data)\n",
    "        decision_output, weights = decision(masked_output)\n",
    "        weights_numpy = weights.detach().cpu().numpy()\n",
    "        weights_numpy=np.roll(weights_numpy,1,axis=-1)\n",
    "        weights_numpy[:,:,0] = frequency(target.cpu().numpy())\n",
    "        print(\"check\")\n",
    "        weights_output=torch.from_numpy(weights_numpy).float()\n",
    "        weights_output = weights_output.to(device)\n",
    "        weights = weights.to(device)\n",
    "        decision_output = decision_output.to(device)\n",
    "        target = target.to(device)\n",
    "        print('check0')\n",
    "        print(weights_output.is_cuda)\n",
    "        print(weights.is_cuda)\n",
    "        print(flag.is_cuda)\n",
    "        print(data.is_cuda)\n",
    "        loss = torch.nn.MarginRankingLoss(margin=1e-7)(weights_output, weights,flag)\n",
    "        print(\"check1\")\n",
    "        loss.backward()\n",
    "        print(\"check2\")\n",
    "        optimizer.step()\n",
    "        print(\"check3\")\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()))\n",
    "            train_loss.append(loss.item())\n",
    "            train_counter.append((batch_idx*batch_size_train) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "log_interval = 10\n",
    "train_loss = []\n",
    "train_counter = []\n",
    "test_loss = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(epochs+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = feature_selection_node(100, batch_size, device)\n",
    "decision = decision_node(100,200,10,batch_size, device)\n",
    "params = list(mask.parameters()) + list(decision.parameters())\n",
    "optimizer = optim.SGD(params, lr=1e-3, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Hello World!\n",
      "cuda:0\n",
      "0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    print(epoch)\n",
    "    train_model(epoch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RandomForestClassifier\n",
    "#Create an object (model)\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=12, min_samples_split=13, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.iloc[:,1:].values\n",
    "Y = df_train.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier on the training data\n",
    "rf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import export_graphviz\n",
    "# import graphviz\n",
    "\n",
    "# dot_data = export_graphviz(rf.estimators_[0], feature_names=features,  \n",
    "#                            filled=True, rounded=True)  \n",
    "\n",
    "# graph = graphviz.Source(dot_data, format='png')\n",
    "# graph.render(\"tree\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.10,\n",
    "                                                    random_state=2,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "rs = RandomizedSearchCV(model, n_iter=10,\n",
    "                        param_distributions = {'max_depth': range(1, 15),\n",
    "                                               'min_samples_split': range(2, 50)},\n",
    "                        cv=5, n_jobs=-1, random_state=3,\n",
    "                        scoring='neg_mean_squared_error')\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "print(rs.best_params_)\n",
    "print(-rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{'min_samples_split': 13, 'max_depth': 12}\n",
    "0.17789285797670434'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
