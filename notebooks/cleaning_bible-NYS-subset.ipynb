{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a3926a-7728-49d0-9c0e-76b759396b8f",
   "metadata": {},
   "source": [
    "### this file is used to convert data from grib to nc. you are able to select the dates, vars, and prs levels you want and it will return training/testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f08a849-3a60-4c6b-8a37-678edfbe64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import xarray as xr\n",
    "import re\n",
    "import argparse\n",
    "\n",
    "# import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "from pathlib import Path\n",
    "import calendar\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c2f1f21-baf5-4c1b-928a-a59bfe62a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of good files and print out any bad files\n",
    "def is_non_zero_file(fpath):\n",
    "    if os.path.isfile(fpath) and os.path.getsize(fpath) > 0:\n",
    "        #         print(\"this file is good \", fpath)\n",
    "        return fpath\n",
    "    else:\n",
    "        print(\"file not found or corrupt \", fpath)\n",
    "        return\n",
    "\n",
    "\n",
    "#         raise Exception('No good file found ', fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e9261df-7c6b-4e5b-ab04-cee7f7a7c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def days2files(input_path, start_date, end_date, init_time, model, fh, prs):\n",
    "    DATES = pd.date_range(start_date, end_date)\n",
    "\n",
    "    if model == \"nam\":\n",
    "        fileList = [\n",
    "            f\"{input_path}{DATE:%Y}/{DATE:%m}/nam_218_{DATE:%Y%m%d}_{init_time}00_0{f:02d}.grb2\"\n",
    "            for DATE in DATES\n",
    "            for f in fh\n",
    "        ]\n",
    "    elif model == \"hrrr\":\n",
    "        if prs:\n",
    "            fileList = [\n",
    "                f\"{input_path}prs/{DATE:%Y}/{DATE:%m}/{DATE:%Y%m%d}_hrrr.t{init_time}z.wrfprsf{f:02d}.grib2\"\n",
    "                for DATE in DATES\n",
    "                for f in fh\n",
    "            ]\n",
    "        else:\n",
    "            fileList = [\n",
    "                f\"{input_path}/{DATE:%Y}/{DATE:%m}/{DATE:%Y%m%d}_hrrr.t{init_time}z.wrfsfcf{f:02d}.grib2\"\n",
    "                for DATE in DATES\n",
    "                for f in fh\n",
    "            ]\n",
    "    elif model == \"gfs\":\n",
    "        fileList = [\n",
    "            f\"{input_path}{DATE:%Y}/{DATE:%m}/gfs_4_{DATE:%Y%m%d}_{init_time}00_0{f:02d}.grb2\"\n",
    "            for DATE in DATES\n",
    "            for f in fh\n",
    "        ]\n",
    "\n",
    "    fileList.sort()\n",
    "\n",
    "    if len(fileList) == 0:\n",
    "        raise Exception(\"No files found\")\n",
    "\n",
    "    goodFiles = []\n",
    "\n",
    "    # check to make sure file is not empty and actually is there\n",
    "    for file in fileList:\n",
    "        if (is_non_zero_file(file)) is not None:\n",
    "            goodFiles.append(is_non_zero_file(file))\n",
    "\n",
    "    print(goodFiles)\n",
    "    return goodFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76461a9-df5c-4ada-a4e0-384d61f33398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call this preprocessing when reading in multi files to clean up and only take what we need while reading to save time and mem\n",
    "def preprocessNam(ds):\n",
    "    # get available vars\n",
    "    available_vars = [v for v in nam_vars if v in ds.keys()]\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"x\"] != 614 and ds.dims[\"y\"] != 428:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    ds = ds[available_vars]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocessNamALL(ds):\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"x\"] != 614 and ds.dims[\"y\"] != 428:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd055e6f-f3a4-4c2a-ab4a-33af22040207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call this preprocessing when reading in multi files to clean up and only take what we need while reading to save time and mem\n",
    "def preprocessHRRRPrs(ds):\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"x\"] != 1799 and ds.dims[\"y\"] != 1059:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    available_vars = [v for v in hrrr_prs_vars if v in ds.keys()]\n",
    "    available_pres = [p for p in pres if p in ds.coords[\"isobaricInhPa\"].values]\n",
    "\n",
    "    if len(available_pres) < 3:\n",
    "        return xr.Dataset(coords={\"isobaricInhPa\": (\"isobaricInhPa\", pres)})\n",
    "    # drop = [ d for d in dim if d not in ['lv_ISBL0', 'xgrid_0', 'ygrid_0'] ]\n",
    "\n",
    "    ds = ds[available_vars].sel(isobaricInhPa=available_pres)\n",
    "    #     dim = ds.dims\n",
    "    #     drop = [ d for d in dim if d not in ['isobaricInhPa', 'latitude', 'longitude'] ]\n",
    "    # ds = ds.drop(drop)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocessHRRRPrsALL(ds):\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"x\"] != 1799 and ds.dims[\"y\"] != 1059:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    available_pres = [p for p in pres if p in ds.coords[\"isobaricInhPa\"].values]\n",
    "\n",
    "    if len(available_pres) < 3:\n",
    "        return xr.Dataset(coords={\"isobaricInhPa\": (\"isobaricInhPa\", pres)})\n",
    "\n",
    "    ds = ds.sel(isobaricInhPa=available_pres)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1d08ee-db0e-4a6a-bed3-6f09c3af2f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call this preprocessing when reading in multi files to clean up and only take what we need while reading to save time and mem\n",
    "def preprocessHRRRSrf(ds):\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"x\"] != 1799 and ds.dims[\"y\"] != 1059:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    available_vars = [v for v in hrrr_sfc_vars if v in ds.keys()]\n",
    "    ds = ds[available_vars]\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocessHRRRSrfALL(ds):\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"x\"] != 1799 and ds.dims[\"y\"] != 1059:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1b5103d-195f-4e43-9aef-7fc934eceaca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessGFS(ds):\n",
    "    # get available vars\n",
    "    available_vars = [v for v in gfs_vars if v in ds.keys()]\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"latitude\"] != 361 and ds.dims[\"longitude\"] != 720:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    ds = ds[available_vars]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocessGFSALL(ds):\n",
    "    # make sure full grid is there\n",
    "    if ds.dims[\"latitude\"] != 361 and ds.dims[\"longitude\"] != 720:\n",
    "        print(\"bad dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \", ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67dba438-2eea-4f38-8bd8-6646757eb7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_variables(ds, remove_vars):\n",
    "    for var in remove_vars:\n",
    "        keys = [v for v in ds.keys()]\n",
    "        if var in keys:\n",
    "            print(f\"dropping {var} from dataset\")\n",
    "            ds = ds.drop(var)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a918cd31-69e5-44c4-8a39-f96fde24cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_in_one_file(fileList, model, prs):\n",
    "    dict_opts = [\n",
    "        {\"typeOfLevel\": \"heightAboveGround\", \"level\": 2},\n",
    "        {\"typeOfLevel\": \"heightAboveGround\", \"level\": 10},\n",
    "        {\"typeOfLevel\": \"surface\", \"stepType\": \"accum\"},\n",
    "    ]\n",
    "\n",
    "    if model == \"hrrr\":\n",
    "        if prs:\n",
    "            return xr.open_dataset(\n",
    "                fileList,\n",
    "                engine=\"cfgrib\",\n",
    "                backend_kwargs={\n",
    "                    \"indexpath\": \"\",\n",
    "                    \"filter_by_keys\": {\n",
    "                        \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            ds_save = []\n",
    "            for opt in dict_opts:\n",
    "                ds_save += [\n",
    "                    xr.open_dataset(\n",
    "                        fileList,\n",
    "                        engine=\"cfgrib\",\n",
    "                        backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": opt},\n",
    "                    )\n",
    "                ]\n",
    "            ds = xr.merge(ds_save, compat=\"override\")\n",
    "            ds = drop_variables(ds, [\"unknown\", \"acpcp\", \"sdwe\", \"ssrun\", \"bgrun\"])\n",
    "\n",
    "            return ds\n",
    "\n",
    "    elif model == \"gfs\":\n",
    "        ds_save = []\n",
    "        for opt in dict_opts:\n",
    "            ds_save += [\n",
    "                xr.open_dataset(\n",
    "                    fileList,\n",
    "                    engine=\"cfgrib\",\n",
    "                    backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": opt},\n",
    "                )\n",
    "            ]\n",
    "        ds = xr.merge(ds_save, compat=\"override\")\n",
    "        ds = drop_variables(ds, [\"unknown\", \"acpcp\", \"sdwe\", \"ssrun\", \"bgrun\"])\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e09bdfb-0e3e-4a22-8417-da9a6fd93967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(fileList, model, prs):\n",
    "    # list the datasets that you want extracted from grib file\n",
    "    dict_opts = [\n",
    "        {\"typeOfLevel\": \"heightAboveGround\", \"level\": 2},\n",
    "        {\"typeOfLevel\": \"heightAboveGround\", \"level\": 10},\n",
    "        {\"typeOfLevel\": \"surface\", \"stepType\": \"accum\"},\n",
    "        {\"typeOfLevel\": \"meanSea\"},\n",
    "        {\"typeOfLevel\": \"surface\", \"cfVarName\": \"orog\"},\n",
    "    ]\n",
    "\n",
    "    if model == \"nam\":\n",
    "        # this solution/option exists because open_mfdataset cannot handle the 'unknown' variables\n",
    "        # within the NAM grib files. Ideally would rather figure out how to use the usual open_mfdataset\n",
    "        # solution, but not able to at this time.\n",
    "        ds_opt_save = []\n",
    "        for opt in dict_opts:\n",
    "            ds_save = []\n",
    "            for file in fileList:\n",
    "                print(file)\n",
    "                ds_save += [\n",
    "                    xr.open_dataset(\n",
    "                        file,\n",
    "                        engine=\"cfgrib\",\n",
    "                        backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": opt},\n",
    "                    )\n",
    "                ]\n",
    "            ds_files = xr.combine_nested(ds_save, concat_dim=\"time\")\n",
    "            ds_opt_save += [ds_files]\n",
    "\n",
    "        ds = xr.merge(ds_opt_save, compat=\"override\")\n",
    "        ds = drop_variables(ds, [\"unknown\", \"acpcp\", \"sdwe\", \"ssrun\", \"bgrun\"])\n",
    "\n",
    "        return ds\n",
    "\n",
    "    elif model == \"hrrr\":\n",
    "        if prs:\n",
    "            return xr.open_mfdataset(\n",
    "                fileList,\n",
    "                parallel=True,\n",
    "                engine=\"cfgrib\",\n",
    "                concat_dim=\"time\",\n",
    "                combine=\"nested\",\n",
    "                backend_kwargs={\n",
    "                    \"indexpath\": \"\",\n",
    "                    \"filter_by_keys\": {\n",
    "                        \"typeOfLevel\": \"isobaricInhPa\",\n",
    "                    },\n",
    "                },\n",
    "                preprocess=preprocessHRRRPrsALL,\n",
    "            )\n",
    "        else:\n",
    "            ds_save = []\n",
    "            for opt in dict_opts:\n",
    "                ds_save += [\n",
    "                    xr.open_mfdataset(\n",
    "                        fileList,\n",
    "                        parallel=True,\n",
    "                        engine=\"cfgrib\",\n",
    "                        concat_dim=\"time\",\n",
    "                        combine=\"nested\",\n",
    "                        backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": opt},\n",
    "                        preprocess=preprocessHRRRSrfALL,\n",
    "                    )\n",
    "                ]\n",
    "            ds = xr.merge(ds_save, compat=\"override\")\n",
    "            ds = drop_variables(ds, [\"unknown\", \"acpcp\", \"sdwe\", \"ssrun\", \"bgrun\"])\n",
    "\n",
    "            return ds\n",
    "\n",
    "    elif model == \"gfs\":\n",
    "        ds_save = []\n",
    "        for opt in dict_opts:\n",
    "            ds_save += [\n",
    "                xr.open_mfdataset(\n",
    "                    fileList,\n",
    "                    parallel=True,\n",
    "                    engine=\"cfgrib\",\n",
    "                    concat_dim=\"time\",\n",
    "                    combine=\"nested\",\n",
    "                    backend_kwargs={\"indexpath\": \"\", \"filter_by_keys\": opt},\n",
    "                    preprocess=preprocessGFSALL,\n",
    "                )\n",
    "            ]\n",
    "        ds = xr.merge(ds_save, compat=\"override\")\n",
    "        ds = drop_variables(ds, [\"unknown\", \"acpcp\", \"sdwe\", \"ssrun\", \"bgrun\"])\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25ca95bf-d7ef-4e45-8432-34e96fcdab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ds_with_projection(ds):\n",
    "    # In order to slice by lat & lon values, need to transform the grid into a projection\n",
    "    # solution from https://stackoverflow.com/questions/58758480/xarray-select-nearest-lat-lon-with-multi-dimension-coordinates\n",
    "    projection = ccrs.LambertConformal(\n",
    "        central_longitude=-97.5, central_latitude=38.5, standard_parallels=[38.5]\n",
    "    )\n",
    "    transform = np.vectorize(\n",
    "        lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree())\n",
    "    )\n",
    "\n",
    "    # The grid should be aligned such that the projection x and y are the same\n",
    "    # at every y and x index respectively\n",
    "    grid_y = ds.isel(x=0)\n",
    "    grid_x = ds.isel(y=0)\n",
    "\n",
    "    _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "    proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "    # ds.sel only works on the dimensions, so we can't just add\n",
    "    # proj_x and proj_y as additional coordinate variables\n",
    "    ds[\"x\"] = proj_x\n",
    "    ds[\"y\"] = proj_y\n",
    "\n",
    "    # grab the unique latitude and longitude for NYSM sites\n",
    "    nysm_path = \"/home/aevans/nysm/archive/nysm/netcdf/proc/2019/01/\"\n",
    "    ds_nysm = xr.open_dataset(f\"{nysm_path}20190101.nc\")\n",
    "    df = ds_nysm.to_dataframe()\n",
    "\n",
    "    nysm_lats = df.lat.unique()\n",
    "    nysm_lons = df.lon.unique()\n",
    "\n",
    "    closest_to_nysm_lons_lats = [\n",
    "        transform(nysm_lons[x], nysm_lats[x]) for x in range(len(nysm_lats))\n",
    "    ]\n",
    "    closest_to_nysm_lons = [\n",
    "        closest_to_nysm_lons_lats[x][0] for x in range(len(nysm_lats))\n",
    "    ]\n",
    "    closest_to_nysm_lats = [\n",
    "        closest_to_nysm_lons_lats[x][1] for x in range(len(nysm_lats))\n",
    "    ]\n",
    "\n",
    "    xx = xr.DataArray(closest_to_nysm_lons, dims=\"z\")\n",
    "    yy = xr.DataArray(closest_to_nysm_lats, dims=\"z\")\n",
    "\n",
    "    return ds.sel(x=xx, y=yy, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adb1e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ds_with_projection_ok(ds):\n",
    "    # In order to slice by lat & lon values, need to transform the grid into a projection\n",
    "    # solution from https://stackoverflow.com/questions/58758480/xarray-select-nearest-lat-lon-with-multi-dimension-coordinates\n",
    "    projection = ccrs.LambertConformal(\n",
    "        central_longitude=-98.8, central_latitude=35.4, standard_parallels=[35.4]\n",
    "    )\n",
    "    transform = np.vectorize(\n",
    "        lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree())\n",
    "    )\n",
    "\n",
    "    # The grid should be aligned such that the projection x and y are the same\n",
    "    # at every y and x index respectively\n",
    "    grid_y = ds.isel(x=0)\n",
    "    grid_x = ds.isel(y=0)\n",
    "\n",
    "    _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "    proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "    # ds.sel only works on the dimensions, so we can't just add\n",
    "    # proj_x and proj_y as additional coordinate variables\n",
    "    ds[\"x\"] = proj_x\n",
    "    ds[\"y\"] = proj_y\n",
    "\n",
    "    # grab the unique latitude and longitude for NYSM sites\n",
    "    df = pd.read_csv(\"/home/aevans/landtype/geoinfo.csv\")\n",
    "\n",
    "    oksm_lats = df.lat.unique()\n",
    "    oksm_lons = df.lon.unique()\n",
    "\n",
    "    closest_to_oksm_lons_lats = [\n",
    "        transform(oksm_lons[x], oksm_lats[x]) for x in range(len(oksm_lats))\n",
    "    ]\n",
    "    closest_to_oksm_lons = [\n",
    "        closest_to_oksm_lons_lats[x][0] for x in range(len(oksm_lats))\n",
    "    ]\n",
    "    closest_to_oksm_lats = [\n",
    "        closest_to_oksm_lons_lats[x][1] for x in range(len(oksm_lats))\n",
    "    ]\n",
    "\n",
    "    xx = xr.DataArray(closest_to_oksm_lons, dims=\"z\")\n",
    "    yy = xr.DataArray(closest_to_oksm_lats, dims=\"z\")\n",
    "\n",
    "    return ds.sel(x=xx, y=yy, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "615d95ad-59ce-4283-a68c-5770eab41800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid_bounds(ds, model, file):\n",
    "    ds = ds.assign_coords({\"longitude\": (((ds.longitude + 180) % 360) - 180)})\n",
    "\n",
    "    if model != \"gfs\":\n",
    "        ds_grid = xr.open_dataset(\n",
    "            file,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\n",
    "                \"indexpath\": \"\",\n",
    "                \"filter_by_keys\": {\n",
    "                    \"typeOfLevel\": \"heightAboveGround\",\n",
    "                    \"level\": 2,\n",
    "                    \"cfVarName\": \"t2m\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        central_longitude = (\n",
    "            (ds_grid.t2m.attrs.get(\"GRIB_LoVInDegrees\") + 180) % 360\n",
    "        ) - 180\n",
    "        central_latitude = ds_grid.t2m.attrs.get(\"GRIB_LaDInDegrees\")\n",
    "        projection = ccrs.LambertConformal(\n",
    "            central_longitude=central_longitude,\n",
    "            central_latitude=central_latitude,\n",
    "            standard_parallels=[central_latitude],\n",
    "        )\n",
    "        transform = np.vectorize(\n",
    "            lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree())\n",
    "        )\n",
    "\n",
    "        # The grid should be aligned such that the projection x and y are the same\n",
    "        # at every y and x index respectively\n",
    "        grid_y = ds.isel(x=0)\n",
    "        grid_x = ds.isel(y=0)\n",
    "        _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "        proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "        # ds.sel only works on the dimensions, so we can't just add\n",
    "        # proj_x and proj_y as additional coordinate variables\n",
    "        ds[\"x\"] = proj_x\n",
    "        ds[\"y\"] = proj_y\n",
    "\n",
    "    # set the longitude and latitude bounds of the smaller grid\n",
    "    # that you want to extract from the model data\n",
    "\n",
    "    # set the longitude and latitude bounds of the smaller grid\n",
    "    # that you want to extract from the model data\n",
    "    long_min, long_max = -103.5, -65\n",
    "    lat_min, lat_max = 33, 47\n",
    "\n",
    "    if model != \"gfs\":\n",
    "        x_min, y_min = transform(long_min, lat_min)\n",
    "        x_max, y_max = transform(long_max, lat_max)\n",
    "\n",
    "    # use the x, y min and max values from above to make the selection from the dataset\n",
    "    # this is better than solely selecting the point locations because I will need different solutions for diff models\n",
    "    # and those solutions should be independent of this cleaning script\n",
    "\n",
    "    if model != \"gfs\":\n",
    "        ds_return = ds.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    else:\n",
    "        mask_lon = (ds.longitude >= long_min) & (ds.longitude <= long_max)\n",
    "        mask_lat = (ds.latitude >= lat_min) & (ds.latitude <= lat_max)\n",
    "        ds_return = ds.where(mask_lon & mask_lat, drop=True)\n",
    "\n",
    "    return ds_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9efc307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid_bounds_ok(ds, model, file):\n",
    "    ds = ds.assign_coords({\"longitude\": (((ds.longitude + 180) % 360) - 180)})\n",
    "\n",
    "    if model != \"gfs\":\n",
    "        ds_grid = xr.open_dataset(\n",
    "            file,\n",
    "            engine=\"cfgrib\",\n",
    "            backend_kwargs={\n",
    "                \"indexpath\": \"\",\n",
    "                \"filter_by_keys\": {\n",
    "                    \"typeOfLevel\": \"heightAboveGround\",\n",
    "                    \"level\": 2,\n",
    "                    \"cfVarName\": \"t2m\",\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "        central_longitude = (\n",
    "            (ds_grid.t2m.attrs.get(\"GRIB_LoVInDegrees\") + 180) % 360\n",
    "        ) - 180\n",
    "        central_latitude = ds_grid.t2m.attrs.get(\"GRIB_LaDInDegrees\")\n",
    "        projection = ccrs.LambertConformal(\n",
    "            central_longitude=central_longitude,\n",
    "            central_latitude=central_latitude,\n",
    "            standard_parallels=[central_latitude],\n",
    "        )\n",
    "        transform = np.vectorize(\n",
    "            lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree())\n",
    "        )\n",
    "\n",
    "        # The grid should be aligned such that the projection x and y are the same\n",
    "        # at every y and x index respectively\n",
    "        grid_y = ds.isel(x=0)\n",
    "        grid_x = ds.isel(y=0)\n",
    "        _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "        proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "        # ds.sel only works on the dimensions, so we can't just add\n",
    "        # proj_x and proj_y as additional coordinate variables\n",
    "        ds[\"x\"] = proj_x\n",
    "        ds[\"y\"] = proj_y\n",
    "\n",
    "    # set the longitude and latitude bounds of the smaller grid\n",
    "    # that you want to extract from the model data\n",
    "    long_min, long_max = -103.5, -94.4\n",
    "    lat_min, lat_max = 33, 38\n",
    "\n",
    "    if model != \"gfs\":\n",
    "        x_min, y_min = transform(long_min, lat_min)\n",
    "        x_max, y_max = transform(long_max, lat_max)\n",
    "\n",
    "    # use the x, y min and max values from above to make the selection from the dataset\n",
    "    # this is better than solely selecting the point locations because I will need different solutions for diff models\n",
    "    # and those solutions should be independent of this cleaning script\n",
    "\n",
    "    if model != \"gfs\":\n",
    "        ds_return = ds.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    else:\n",
    "        mask_lon = (ds.longitude >= long_min) & (ds.longitude <= long_max)\n",
    "        mask_lat = (ds.latitude >= lat_min) & (ds.latitude <= lat_max)\n",
    "        ds_return = ds.where(mask_lon & mask_lat, drop=True)\n",
    "\n",
    "    return ds_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78a0363a-b3b7-4dc3-8f40-8feee753fad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def main(model, year, init_time, prs=False, combined_file=False):\n",
    "def main(prs=False, combined_file=False):\n",
    "    \"\"\"\n",
    "    This is the main function that converts grib files from the GFS, NAM, and HRRR to parquet files.\n",
    "    The function loops over all months and day in a given year.\n",
    "    Within the conversion, specific variables are extracted. The datasets where these can be found\n",
    "    need to be specified within read_data(). Smaller forecast grids focused around NYS are defined\n",
    "    and saved within these parquet files.\n",
    "\n",
    "    The following parameters need to be passed into main():\n",
    "\n",
    "    model (str) - hrrr, nam, gfs\n",
    "    year (int) - the year of interest (e.g., 2020)\n",
    "    init_time (str) - initilization time for model, 00 or 12 UTC\n",
    "    prs (bool) - true if you want the pressure files, false if you only want surface\n",
    "    combined_file (bool) - this flag should be turned on if multiple forecast times exist in one grib file\n",
    "    \"\"\"\n",
    "    for year in [2018, 2019, 2020, 2021]:\n",
    "        model = \"gfs\"\n",
    "        year = year\n",
    "        init_time = \"00\"\n",
    "\n",
    "        # input_path (str) - path to base location of data\n",
    "        # output_path (str) - where to write new smaller clean files\n",
    "        if combined_file:\n",
    "            input_path = \"/home/aevans/ai2es/GFS/GFSv16_parallel\"\n",
    "            output_path = \"/home/aevans/ai2es/GFS/GFSv16_parallel/cleaned\"\n",
    "        else:\n",
    "            input_path = f\"/home/aevans/ai2es/{model.upper()}/\"\n",
    "            output_path = f\"/home/aevans/ai2es/{model.upper()}/cleaned\"\n",
    "        # choosing to start at first ~forecast~ time rather than ~init~ time because of variable list inconsistencies\n",
    "        if model == \"hrrr\":\n",
    "            fh = range(1, 19)  # forecast hours, second num exclusive\n",
    "        elif model == \"nam\":\n",
    "            fh = np.arange(1, 37, 1).tolist() + np.arange(39, 85, 3).tolist()\n",
    "        elif model == \"gfs\":\n",
    "            fh = np.arange(3, 99, 3)\n",
    "\n",
    "        # loop through months & days\n",
    "        for month in range(1, 13):  # call all months in calendar year\n",
    "            num_days = calendar.monthrange(year, month)[1]\n",
    "            for day in range(\n",
    "                1, num_days + 1\n",
    "            ):  # call all days in calendar month & respective year\n",
    "                print(\"This is your path!\")\n",
    "                print(\".    .\")\n",
    "                print(\n",
    "                    f\"{output_path}{model.upper()}/{year}/{month}/{year}{month}{day}_{model}.t{init_time}z_fhAll.parquet\"\n",
    "                )\n",
    "\n",
    "                start_date = datetime.datetime(year, month, day)\n",
    "                end_date = datetime.datetime(year, month, day)\n",
    "\n",
    "                if combined_file:\n",
    "                    fileList = f\"{input_path}{year}{str(month).zfill(2)}{str(day).zfill(2)}{init_time}_{model}.grb2\"\n",
    "                else:\n",
    "                    fileList = days2files(\n",
    "                        input_path, start_date, end_date, init_time, model, fh, prs\n",
    "                    )\n",
    "                print(fileList)\n",
    "\n",
    "                if not fileList:\n",
    "                    print(\"No files exist to read!\")\n",
    "                else:\n",
    "                    if combined_file:\n",
    "                        ds = read_data_in_one_file(fileList, model, prs)\n",
    "                        ds = define_grid_bounds(ds, model, fileList)\n",
    "                    else:\n",
    "                        ds = read_data(fileList, model, prs)\n",
    "                        ds = define_grid_bounds(ds, model, fileList[-1])\n",
    "\n",
    "                    # fill all na values with 0\n",
    "                    ds = ds.fillna(0)\n",
    "                    df = ds.to_dataframe(dim_order=None)\n",
    "\n",
    "                    if model == \"hrrr\":\n",
    "                        new_index = [\"time\", \"y\", \"x\"]\n",
    "                    elif model in [\"gfs\", \"nam\"]:\n",
    "                        new_index = [\"time\", \"latitude\", \"longitude\"]\n",
    "\n",
    "                    # drop step since val time already has it and drop other data group names\n",
    "                    # as these do not include info that is necessary to keep\n",
    "                    if model == \"gfs\" and combined_file == True:\n",
    "                        df = (\n",
    "                            df.reset_index()\n",
    "                            .drop([\"step\", \"heightAboveGround\", \"surface\"], axis=1)\n",
    "                            .set_index(new_index)\n",
    "                        )\n",
    "                    else:\n",
    "                        df = (\n",
    "                            df.reset_index()\n",
    "                            .drop(\n",
    "                                [\"step\", \"heightAboveGround\", \"surface\", \"meanSea\"],\n",
    "                                axis=1,\n",
    "                            )\n",
    "                            .set_index(new_index)\n",
    "                        )\n",
    "\n",
    "                    # save the data to parquet file\n",
    "                    sday = str(start_date.day).zfill(2)\n",
    "                    smonth = str(start_date.month).zfill(2)\n",
    "                    syear = start_date.year\n",
    "\n",
    "                    savepath = f\"{output_path}{model.upper()}/{syear}/{smonth}/\"\n",
    "                    # create this directory if it doesn't already exist\n",
    "                    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "                    if model == \"hrrr\":\n",
    "                        df.to_parquet(\n",
    "                            f\"{savepath}{syear}{smonth}{sday}_{model}.t{init_time}z_wrfsfc_fhAll.parquet\"\n",
    "                        )\n",
    "                    else:\n",
    "                        df.to_parquet(\n",
    "                            f\"{savepath}{syear}{smonth}{sday}_{model}.t{init_time}z_fhAll.parquet\"\n",
    "                        )\n",
    "\n",
    "            print(\"This is your path!\")\n",
    "            print(\".    .\")\n",
    "            print(\n",
    "                f\"{output_path}{model.upper()}/{year}/{month}/{year}{month}{day}_{model}.t{init_time}z_wrfsfc_fhAll.parquet\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b59861bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     pool = multiprocessing.Pool(16)\n",
    "#     pool.map(main, [])\n",
    "#     pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8ab04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is your path!\n",
      ".    .\n",
      "/home/aevans/ai2es/GFS/cleanedGFS/2018/1/201811_gfs.t00z_wrfsfc_fhAll.parquet\n",
      "['/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2']\n",
      "['/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2']\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main(\"nam\", 2018, \"00\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
