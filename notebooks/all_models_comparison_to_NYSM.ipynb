{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3c6d1b8-20e6-49be-bc98-7f64dd971bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "import cfgrib\n",
    "import metpy.calc as mpcalc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from metpy.units import units\n",
    "from scipy import interpolate\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f8b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time\n",
      "latitude\n",
      "longitude\n",
      "tp\n",
      "t2m\n",
      "u_total\n",
      "u_dir\n",
      "d2m\n",
      "prmsl\n",
      "orog\n",
      "adding OKSM site column\n",
      "Saving New Files For :: GFS t12z : 2018--01\n",
      "--- 90.9499204158783 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import warnings\n",
    "import cfgrib\n",
    "import metpy.calc as mpcalc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from metpy.units import units\n",
    "from scipy import interpolate\n",
    "from sklearn.neighbors import BallTree\n",
    "from multiprocessing import Process\n",
    "\n",
    "def load_nysm_data(year):\n",
    "    \"\"\"\n",
    "    Load resampled New York State Mesonet (NYSM) observational data for a given year.\n",
    "\n",
    "    The function reads two parquet files containing resampled NYSM data at 1-hour and 3-hour intervals\n",
    "    respectively. These files are created by running \"get_resampled_nysm_data.ipynb\" notebook.\n",
    "\n",
    "    Parameters:\n",
    "    year (int): The year for which NYSM data is to be loaded.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple of two pandas.DataFrame objects containing the resampled NYSM data at 1-hour and 3-hour\n",
    "    intervals respectively.\n",
    "    \"\"\"\n",
    "    # these parquet files are created by running \"get_resampled_nysm_data.ipynb\"\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    nysm_1H_obs = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "    nysm_3H_obs = pd.read_parquet(f\"{nysm_path}nysm_3H_obs_{year}.parquet\")\n",
    "    return nysm_1H_obs, nysm_3H_obs\n",
    "\n",
    "\n",
    "def load_oksm_data(year):\n",
    "    # these parquet files are created by running \"get_resampled_nysm_data.ipynb\"\n",
    "    oksm_path = \"/home/aevans/nwp_bias/data/oksm/\"\n",
    "\n",
    "    oksm_1H_obs = pd.read_csv(f\"{oksm_path}oksm_1H_obs_{year}.csv\")\n",
    "    oksm_3H_obs = pd.read_csv(f\"{oksm_path}oksm_3H_obs_{year}.csv\")\n",
    "    return oksm_1H_obs, oksm_3H_obs\n",
    "\n",
    "\n",
    "def read_data_ny(model, init, month, year):\n",
    "        \"\"\"\n",
    "    Read cleaned data files for a given model, initialization time, month, and year.\n",
    "\n",
    "    The function reads a set of cleaned data files for a given atmospheric model, initialization time,\n",
    "    month, and year. The files are stored in parquet format and contain pre-processed weather data for\n",
    "    the New York area.\n",
    "\n",
    "    Parameters:\n",
    "    model (str): The name of the atmospheric model for which data is to be read.\n",
    "    init (int): The initialization time (in hours) for which data is to be read.\n",
    "    month (str): The month for which data is to be read.\n",
    "    year (int): The year for which data is to be read.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the cleaned weather data for the specified model, initialization\n",
    "    time, month, and year.\n",
    "    \"\"\"\n",
    "    cleaned_data_path = f\"/home/aevans/ai2es/cleaned/{model}/\"\n",
    "\n",
    "    filelist = glob.glob(f\"{cleaned_data_path}{year}/{month}/*t{init}z*.parquet\")\n",
    "    filelist.sort()\n",
    "\n",
    "    li = []\n",
    "    for filename in filelist:\n",
    "        df_temp = pd.read_parquet(filename)\n",
    "        li.append(\n",
    "            df_temp.reset_index()\n",
    "        )  # reset the index in case indices are different among files\n",
    "    df = pd.concat(li)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data_ok(model, init, month, year):\n",
    "    cleaned_data_path = f\"/home/aevans/ai2es/{model}/cleaned{model}/\"\n",
    "\n",
    "    filelist = glob.glob(f\"{cleaned_data_path}{year}/{month}/*t{init}z*.parquet\")\n",
    "    filelist.sort()\n",
    "\n",
    "    li = []\n",
    "    for filename in filelist:\n",
    "        df_temp = pd.read_parquet(filename)\n",
    "        li.append(\n",
    "            df_temp.reset_index()\n",
    "        )  # reset the index in case indices are different among files\n",
    "    df = pd.concat(li)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reformat_df(df):\n",
    "    \"\"\"\n",
    "    Reformat a DataFrame of weather data to standard units and add new columns.\n",
    "\n",
    "    The function takes a DataFrame of weather data, reformats it to standard units, and adds\n",
    "    new columns for wind speed, wind direction, and lead time. The resulting DataFrame is\n",
    "    returned.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input DataFrame containing weather data. The DataFrame should\n",
    "    have columns for time, longitude, latitude, temperature (in Kelvin), dewpoint temperature\n",
    "    (in Kelvin), zonal wind component (in m/s), and meridional wind component (in m/s).\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the reformatted weather data. The DataFrame has\n",
    "    columns for time (as the index), longitude, latitude, temperature (in degrees Celsius), dewpoint\n",
    "    temperature (in degrees Celsius), wind speed (in m/s), wind direction (in degrees), and lead time\n",
    "    (in hours).\n",
    "    \"\"\"\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    df[\"longitude\"] = ((df[\"longitude\"] + 180) % 360) - 180\n",
    "    df[[\"t2m\", \"d2m\"]] = df[[\"t2m\", \"d2m\"]] - 273.15  # convert from K to deg C\n",
    "    u10 = units.Quantity(df[\"u10\"].values, \"m/s\")\n",
    "    v10 = units.Quantity(df[\"v10\"].values, \"m/s\")\n",
    "    df[\"u_total\"] = mpcalc.wind_speed(u10, v10).magnitude\n",
    "    df[\"u_dir\"] = mpcalc.wind_direction(u10, v10, convention=\"from\").magnitude\n",
    "\n",
    "    lead_time_delta = df[\"valid_time\"] - df[\"time\"]\n",
    "    df[\"lead time\"] = (24.0 * lead_time_delta.dt.days) + divmod(\n",
    "        lead_time_delta.dt.seconds, 3600\n",
    "    )[0]\n",
    "\n",
    "    df = df.set_index(\"valid_time\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# def interpolate_func_griddata(values, model_lon, model_lat, xnew, ynew):\n",
    "#     if np.mean(values) == np.nan:\n",
    "#         print(\"SOME VALS ARE NAN\")\n",
    "#     vals = interpolate.griddata(\n",
    "#         (model_lon, model_lat), values, (xnew, ynew), method=\"linear\"\n",
    "#     )\n",
    "#     if np.mean(values) == np.nan:\n",
    "#         print(\"SOME VALS ARE NAN\")\n",
    "#     return vals\n",
    "\n",
    "\n",
    "# def interpolate_model_data_to_nysm_locations_groupby(df_model, df_nysm, vars_to_interp):\n",
    "#     \"\"\"\n",
    "#     Use this function if you would like to interpolate to NYSM locations rather than use the ball tree with\n",
    "#     the existing model grid.\n",
    "#     This function interpolates the model grid to the NYSM site locations for each variable in dataframe.\n",
    "#     The new dataframe is then returned.\n",
    "\n",
    "#     This function should not be used with the HRRR grid, as it is incredibly slow.\n",
    "#     \"\"\"\n",
    "#     # New York\n",
    "#     df_nysm = df_nysm.groupby(\"station\").mean()[[\"lat\", \"lon\"]]\n",
    "#     xnew = df_nysm[\"lon\"]\n",
    "#     ynew = df_nysm[\"lat\"]\n",
    "\n",
    "#     df_model = df_model.reset_index().set_index(\"time\")\n",
    "\n",
    "#     # if vals != points in interpolation routine\n",
    "#     # called a few lines below, it's because this line is finding multiple of the same valid_times which occurs at times later in the month\n",
    "#     # grab a smaller subset of the data encompassing New York State and Oklahoma\n",
    "#     df_model_ny = df_model[\n",
    "#         (df_model.latitude >= 39.0)\n",
    "#         & (df_model.latitude <= 47.0)\n",
    "#         & (df_model.longitude <= -71.0)\n",
    "#         & (df_model.longitude >= -80.0)\n",
    "#     ]\n",
    "\n",
    "#     model_lon_lat_ny = df_model_ny[\n",
    "#         df_model_ny[\"valid_time\"] == df_model_ny[\"valid_time\"].unique().min()\n",
    "#     ]\n",
    "\n",
    "#     model_lon_ny = model_lon_lat_ny[\"longitude\"].values\n",
    "#     model_lat_ny = model_lon_lat_ny[\"latitude\"].values\n",
    "\n",
    "#     df_model = df_model_ny[df_model_ny[\"longitude\"].isin(model_lon_ny)]\n",
    "#     df_model = df_model.reset_index()\n",
    "\n",
    "#     # NEED TO FIX THIS\n",
    "#     df = pd.DataFrame()\n",
    "#     for v, var in enumerate(vars_to_interp):\n",
    "#         print(var)\n",
    "#         df[var] = df_model.groupby([\"time\", \"valid_time\"])[var].apply(\n",
    "#             interpolate_func_griddata, model_lon_ny, model_lat_ny, xnew, ynew\n",
    "#         )\n",
    "\n",
    "#     df_explode = df.apply(pd.Series.explode)\n",
    "\n",
    "#     # add in the lat & lon & station\n",
    "#     if \"latitude\" in df_explode.keys():\n",
    "#         print(\"adding NYSM site column\")\n",
    "#         nysm_sites = df_nysm.reset_index().station.unique()\n",
    "#         model_interp_lats = df_explode.latitude.unique()\n",
    "#         map_dict = {model_interp_lats[i]: nysm_sites[i] for i in range(len(nysm_sites))}\n",
    "#         df_explode[\"station\"] = df_explode[\"latitude\"].map(map_dict)\n",
    "#     return df_explode\n",
    "\n",
    "\n",
    "def interpolate_model_data_to_oksm_locations_groupby(df_model, df_oksm, vars_to_interp):\n",
    "    \"\"\"\n",
    "    Use this function if you would like to interpolate to NYSM locations rather than use the ball tree with\n",
    "    the existing model grid.\n",
    "    This function interpolates the model grid to the NYSM site locations for each variable in dataframe.\n",
    "    The new dataframe is then returned.\n",
    "\n",
    "    This function should not be used with the HRRR grid, as it is incredibly slow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Oklahoma\n",
    "    df_oksm = df_oksm.rename(columns={\"STID\": \"station\"})\n",
    "    df_oksm = df_oksm.groupby(\"station\").mean()[[\"lat\", \"lon\"]]\n",
    "    xnew_ok = df_oksm[\"lon\"]\n",
    "    ynew_ok = df_oksm[\"lat\"]\n",
    "\n",
    "    df_model = df_model.reset_index().set_index(\"time\")\n",
    "\n",
    "    # if vals != points in interpolation routine\n",
    "    # called a few lines below, it's because this line is finding multiple of the same valid_times which occurs at times later in the month\n",
    "    # grab a smaller subset of the data encompassing New York State and Oklahoma\n",
    "\n",
    "    df_model_ok = df_model[\n",
    "        (df_model.latitude >= 33.0)\n",
    "        & (df_model.latitude <= 38.0)\n",
    "        & (df_model.longitude <= -94.0)\n",
    "        & (df_model.longitude >= -104.0)\n",
    "    ]\n",
    "\n",
    "    model_lon_lat_ok = df_model_ok[\n",
    "        df_model_ok[\"valid_time\"] == df_model_ok[\"valid_time\"].unique().min()\n",
    "    ]\n",
    "\n",
    "    model_lon_ok = model_lon_lat_ok[\"longitude\"].values\n",
    "    model_lat_ok = model_lon_lat_ok[\"latitude\"].values\n",
    "\n",
    "    df_model = df_model_ok[df_model_ok[\"longitude\"].isin(model_lon_ok)]\n",
    "    df_model = df_model.reset_index()\n",
    "\n",
    "    # NEED TO FIX THIS\n",
    "    df = pd.DataFrame()\n",
    "    for v, var in enumerate(vars_to_interp):\n",
    "        print(var)\n",
    "        df[var] = df_model.groupby([\"time\", \"valid_time\"])[var].apply(\n",
    "            interpolate_func_griddata, model_lon_ok, model_lat_ok, xnew_ok, ynew_ok\n",
    "        )\n",
    "\n",
    "    df_explode = df.apply(pd.Series.explode)\n",
    "\n",
    "    # add in the lat & lon & station\n",
    "    if \"latitude\" in df_explode.keys():\n",
    "        print(\"adding OKSM site column\")\n",
    "        oksm_sites = df_oksm.reset_index().station.unique()\n",
    "        model_interp_lats = df_explode.latitude.unique()\n",
    "        map_dict = {model_interp_lats[i]: oksm_sites[i] for i in range(len(oksm_sites))}\n",
    "        df_explode[\"station\"] = df_explode[\"latitude\"].map(map_dict)\n",
    "    return df_explode\n",
    "\n",
    "\n",
    "# def get_locations_for_ball_tree(df, nysm_1H_obs):\n",
    "#     locations_a = df.reset_index()[[\"latitude\", \"longitude\"]]\n",
    "#     locations_b = (\n",
    "#         nysm_1H_obs[[\"lat\", \"lon\"]]\n",
    "#         .dropna()\n",
    "#         .drop_duplicates()\n",
    "#         .reset_index()\n",
    "#     )\n",
    "\n",
    "#         # ball tree to find nysm site locations\n",
    "#     # locations_a ==> build the tree\n",
    "#     # locations_b ==> query the tree\n",
    "#     # Creates new columns converting coordinate degrees to radians.\n",
    "#     for column in locations_a[[\"latitude\", \"longitude\"]]:\n",
    "#         rad = np.deg2rad(locations_a[column].values)\n",
    "#         locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "#     for column in locations_b[[\"lat\", \"lon\"]]:\n",
    "#         rad = np.deg2rad(locations_b[column].values)\n",
    "#         locations_b[f\"{column}_rad\"] = rad\n",
    "\n",
    "#     return locations_a, locations_b\n",
    "\n",
    "\n",
    "def get_locations_for_ball_tree_ok(df, oksm_1H_obs):\n",
    "    locations_a = df.reset_index()[[\"latitude\", \"longitude\"]]\n",
    "    locations_b = oksm_1H_obs[[\"lat\", \"lon\"]].dropna().drop_duplicates().reset_index()\n",
    "\n",
    "    # ball tree to find nysm site locations\n",
    "    # locations_a ==> build the tree\n",
    "    # locations_b ==> query the tree\n",
    "    # Creates new columns converting coordinate degrees to radians.\n",
    "    for column in locations_a[[\"latitude\", \"longitude\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    for column in locations_b[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_b[column].values)\n",
    "        locations_b[f\"{column}_rad\"] = rad\n",
    "\n",
    "    return locations_a, locations_b\n",
    "\n",
    "\n",
    "# def get_ball_tree_indices_ny(model_data, nysm_1H_obs):\n",
    "#     locations_a, locations_b = get_locations_for_ball_tree(model_data, nysm_1H_obs)\n",
    "#     # Takes the first group's latitude and longitude values to construct the ball tree.\n",
    "\n",
    "#     ball = BallTree(\n",
    "#         locations_a[[\"latitude_rad\", \"longitude_rad\"]].values, metric=\"haversine\"\n",
    "#     )\n",
    "#     # k: The number of neighbors to return from tree\n",
    "#     k = 1\n",
    "#     # Executes a query with the second group. This will also return two arrays.\n",
    "#     distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "#     # get indices in a format where we can query the df\n",
    "#     indices_list = [indices[x][0] for x in range(len(indices))]\n",
    "#     return indices_list\n",
    "\n",
    "\n",
    "def get_ball_tree_indices_ok(model_data, oksm_1H_obs):\n",
    "    locations_a, locations_b = get_locations_for_ball_tree_ok(model_data, oksm_1H_obs)\n",
    "    # Takes the first group's latitude and longitude values to construct the ball tree.\n",
    "\n",
    "    ball = BallTree(\n",
    "        locations_a[[\"latitude_rad\", \"longitude_rad\"]].values, metric=\"haversine\"\n",
    "    )\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = 1\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "    # get indices in a format where we can query the df\n",
    "    indices_list = [indices[x][0] for x in range(len(indices))]\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "# def df_with_nysm_locations(df, df_nysm, indices_list):\n",
    "#     df_closest_locs = df.iloc[indices_list][[\"latitude\", \"longitude\"]].reset_index()\n",
    "#     df_nysm_station_locs = df_nysm.groupby(\"station\")[[\"lat\", \"lon\"]].mean()\n",
    "\n",
    "#     for x in range(len(df_nysm_station_locs.index)):\n",
    "#         df_dummy = df[\n",
    "#             (df.latitude == df_closest_locs.latitude[x])\n",
    "#             & (df.longitude == df_closest_locs.longitude[x])\n",
    "#         ]\n",
    "#         df_dummy = df_dummy.reset_index()\n",
    "#         df_dummy[\"station\"] = df_nysm_station_locs.index[x]\n",
    "#         if x == 0:\n",
    "#             df_save = df_dummy\n",
    "#         else:\n",
    "#             df_save = pd.concat([df_save, df_dummy])\n",
    "#     print(\"complete\")\n",
    "#     return df_save.set_index([\"station\", \"valid_time\"])\n",
    "\n",
    "\n",
    "def df_with_oksm_locations(df, df_oksm, indices_list):\n",
    "       \"\"\"\n",
    "    Assigns the nearest location from `df` to each station in `df_oksm` and returns a new DataFrame with station IDs\n",
    "    and timestamps as the new index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas DataFrame\n",
    "        A DataFrame containing weather data with columns \"latitude\" and \"longitude\".\n",
    "    df_oksm : pandas DataFrame\n",
    "        A DataFrame containing weather station data with columns \"station\", \"lat\", and \"lon\".\n",
    "    indices_list : list of integers\n",
    "        A list of indices corresponding to the nearest locations in `df` to each station in `df_oksm`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas DataFrame\n",
    "        A new DataFrame with station IDs and timestamps as the new index, with weather data from `df` assigned to the\n",
    "        nearest location for each station in `df_oksm`.\n",
    "\n",
    "    \"\"\"\n",
    "    df_closest_locs = df.iloc[indices_list][[\"latitude\", \"longitude\"]].reset_index()\n",
    "    df_oksm_station_locs = df_oksm.groupby(\"station\")[[\"lat\", \"lon\"]].mean()\n",
    "\n",
    "    for x in range(len(df_oksm_station_locs.index)):\n",
    "        df_dummy = df[\n",
    "            (df.latitude == df_closest_locs.latitude[x])\n",
    "            & (df.longitude == df_closest_locs.longitude[x])\n",
    "        ]\n",
    "        df_dummy = df_dummy.reset_index()\n",
    "        df_dummy[\"station\"] = df_oksm_station_locs.index[x]\n",
    "        if x == 0:\n",
    "            df_save = df_dummy\n",
    "        else:\n",
    "            df_save = pd.concat([df_save, df_dummy])\n",
    "    print(\"complete\")\n",
    "    return df_save.set_index([\"station\", \"valid_time\"])\n",
    "\n",
    "\n",
    "def redefine_precip_intervals_NAM(data, dt):\n",
    "        \"\"\"\n",
    "    Redefines the precipitation intervals in the input dataframe for North American Mesoscale (NAM) model data.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): A pandas DataFrame with columns 'valid_time', 'time', 'station', and 'tp' representing the forecast valid time, initial time, station identifier, and precipitation amount in mm, respectively.\n",
    "        dt (int): An integer representing the time interval in hours for the forecast valid time (1 for 1H and 3 for 3H).\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A pandas DataFrame with the same columns as the input dataframe, but with the precipitation amount redefined based on the valid time intervals. The precipitation amount is defined for intervals with a valid time of 00 and 12 for dt=1 and 03, 06, 09, 12, 15, 18, 21, and 00 for dt=3. The values for the other intervals are computed as the difference between the precipitation amount for the current interval and the previous one. The precipitation amounts are clipped at a minimum of 0.\n",
    "\n",
    "    Example:\n",
    "        >>> redefine_precip_intervals_NAM(df, 3)\n",
    "    \"\"\"\n",
    "    # dt is 1 for 1H and 3 for 3H\n",
    "    tp_data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])[[\"tp\"]]\n",
    "    # get valid times 00, 06, 12, & 18\n",
    "    tp_data[\"new tp keep\"] = tp_data[\n",
    "        (tp_data.index.get_level_values(level=0).hour == 12 + dt)\n",
    "        | (tp_data.index.get_level_values(level=0).hour == 0 + dt)\n",
    "    ]\n",
    "    tp_data[\"tp to diff\"] = tp_data[\n",
    "        (tp_data.index.get_level_values(level=0).hour != 12 + dt)\n",
    "        | (tp_data.index.get_level_values(level=0).hour != 0 + dt)\n",
    "    ][\"tp\"]\n",
    "    dummy = (\n",
    "        tp_data.reset_index()\n",
    "        .set_index([\"station\", \"time\", \"valid_time\"])\n",
    "        .sort_index(level=1)\n",
    "        .shift(periods=1)\n",
    "    )\n",
    "    tp_data[\"tp shifted\"] = dummy.reset_index().set_index(\n",
    "        [\"valid_time\", \"time\", \"station\"]\n",
    "    )[\"tp\"]\n",
    "    tp_data[\"tp diff\"] = tp_data[\"tp to diff\"] - tp_data[\"tp shifted\"]\n",
    "    tp_data[\"new_tp\"] = tp_data[\"new tp keep\"].combine_first(tp_data[\"tp diff\"])\n",
    "    tp_data = tp_data.drop(\n",
    "        columns=[\"new tp keep\", \"tp to diff\", \"tp shifted\", \"tp diff\"]\n",
    "    )\n",
    "\n",
    "    # merge in with original dataframe\n",
    "    data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])\n",
    "    data[\"new_tp\"] = tp_data[\"new_tp\"].clip(lower=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def redefine_precip_intervals_GFS(data):\n",
    "    tp_data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])[\n",
    "        [\"tp\", \"lead time\"]\n",
    "    ]\n",
    "    # get valid times 00, 06, 12, & 18\n",
    "    tp_data[\"new tp 1\"] = tp_data.loc[\n",
    "        (tp_data.index.get_level_values(level=0).hour % 6 == 0)\n",
    "    ][\"tp\"]\n",
    "    dummy = (\n",
    "        tp_data.reset_index()\n",
    "        .set_index([\"station\", \"time\", \"valid_time\"])\n",
    "        .sort_index(level=1)\n",
    "        .shift(periods=1)\n",
    "    )\n",
    "    tp_data[\"tp shifted\"] = dummy.reset_index().set_index(\n",
    "        [\"valid_time\", \"time\", \"station\"]\n",
    "    )[\"tp\"]\n",
    "    tp_data[\"tp diff\"] = tp_data[\"new tp 1\"] - tp_data[\"tp shifted\"]\n",
    "    tp_data[\"new tp 2\"] = tp_data.loc[\n",
    "        (tp_data.index.get_level_values(level=0).hour % 6 != 0)\n",
    "    ][\"tp\"]\n",
    "    tp_data[\"new_tp\"] = tp_data[\"new tp 2\"].combine_first(tp_data[\"tp diff\"])\n",
    "    tp_data = tp_data.drop(columns=[\"tp shifted\", \"tp diff\", \"new tp 1\", \"new tp 2\"])\n",
    "\n",
    "    # merge in with original dataframe\n",
    "    data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])\n",
    "    data[\"new_tp\"] = tp_data[\"new_tp\"].clip(lower=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def redefine_precip_intervals_HRRR(data):\n",
    "    # dt is 1 for 1H and 3 for 3H\n",
    "    tp_data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])[\n",
    "        [\"tp\", \"lead time\"]\n",
    "    ]\n",
    "    # should use lead time here instead of valid_time (to be more generalizable)\n",
    "    tp_data[\"new tp keep\"] = tp_data[tp_data[\"lead time\"] == 1][\"tp\"]\n",
    "    tp_data[\"tp to diff\"] = tp_data[(tp_data[\"lead time\"] != 1)][\"tp\"]\n",
    "    dummy = (\n",
    "        tp_data.reset_index()\n",
    "        .set_index([\"station\", \"time\", \"valid_time\"])\n",
    "        .sort_index(level=1)\n",
    "        .shift(periods=1)\n",
    "    )\n",
    "    tp_data[\"tp shifted\"] = dummy.reset_index().set_index(\n",
    "        [\"valid_time\", \"time\", \"station\"]\n",
    "    )[\"tp\"]\n",
    "    tp_data[\"tp diff\"] = tp_data[\"tp\"].diff()\n",
    "    tp_data[\"new_tp\"] = tp_data[\"new tp keep\"].combine_first(tp_data[\"tp diff\"])\n",
    "    tp_data = tp_data.drop(\n",
    "        columns=[\"new tp keep\", \"tp to diff\", \"tp shifted\", \"tp diff\", \"lead time\"]\n",
    "    )\n",
    "\n",
    "    # merge in with original dataframe\n",
    "    data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])\n",
    "    # replacing negative values with 0...the negative values are occurring during the forecast period which is unexpected behavior\n",
    "    # as the precipitation forecast should accumulate throughout forecast period\n",
    "    data[\"new_tp\"] = tp_data[\"new_tp\"].clip(lower=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def drop_unwanted_time_diffs(df_model_both_sites, t_int):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame with uneven time intervals that may cause issues in precipitation forecasts.\n",
    "    \n",
    "    Args:\n",
    "    df_model_both_sites (pandas DataFrame): The DataFrame containing the model data to be cleaned.\n",
    "    t_int (int): The time interval in hours between consecutive model time steps.\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: A cleaned DataFrame with even time intervals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get rid of uneven time intervals that mess up precipitation forecasts\n",
    "\n",
    "    # t_int == 3 for GFS and NAM > f36\n",
    "    # t_int == 1 for NAM <= f36 & HRRR\n",
    "    df_model_both_sites[\"lead time diff\"] = df_model_both_sites.groupby(\n",
    "        [\"station\", \"time\"]\n",
    "    )[\"lead time\"].diff()\n",
    "    # following line fixes the issue where the lead time difference is nan for f01 and f39 because of the diff - we don't want to drop these later in the func\n",
    "    df_model_both_sites = df_model_both_sites.fillna(value={\"lead time diff\": t_int})\n",
    "\n",
    "    df_model_both_sites = df_model_both_sites.drop(\n",
    "        df_model_both_sites[\n",
    "            (df_model_both_sites[\"lead time diff\"] > t_int)\n",
    "            | (df_model_both_sites[\"lead time diff\"].isnull())\n",
    "        ].index\n",
    "    )\n",
    "    df_model_both_sites = df_model_both_sites.drop(columns=[\"lead time diff\"])\n",
    "    return df_model_both_sites\n",
    "\n",
    "\n",
    "def mask_out_water(model, df_model):\n",
    "    \"\"\"\n",
    "    Given a model name and a DataFrame with latitude and longitude information,\n",
    "    return a new DataFrame with only the grid cells over land.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : str\n",
    "        The name of the model to be used, one of ['GFS', 'NAM', 'HRRR'].\n",
    "    df_model : pd.DataFrame\n",
    "        The DataFrame containing the latitudes and longitudes to be masked.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with the same columns as `df_model`, but only containing\n",
    "        grid cells over land.\n",
    "    \"\"\"\n",
    "    df_model = df_model.reset_index()\n",
    "    # read in respective data\n",
    "    # these files are hard coded since we only need land surface information that was not extracted from original files\n",
    "    # within the data cleaning script\n",
    "    indir = f\"/home/aevans/ai2es/{model.upper()}/2018/01/\"\n",
    "    if model.upper() == \"GFS\":\n",
    "        filename = \"gfs_4_20180101_0000_003.grb2\"\n",
    "        ind = 42\n",
    "        var = \"landn\"\n",
    "    elif model.upper() == \"NAM\":\n",
    "        filename = \"nam_218_20180101_0000_003.grb2\"\n",
    "        ind = 26\n",
    "        var = \"lsm\"\n",
    "    elif model.upper() == \"HRRR\":\n",
    "        filename = \"20180101_hrrr.t00z.wrfsfcf03.grib2\"\n",
    "        ind = 34\n",
    "        var = \"lsm\"\n",
    "    ds = cfgrib.open_datasets(f\"{indir}{filename}\")\n",
    "\n",
    "    ds_tointerp = ds[ind]  # extract the data array that contains land surface class\n",
    "    ds_tointerp = ds_tointerp.assign_coords(\n",
    "        {\"longitude\": (((ds_tointerp.longitude + 180) % 360) - 180)}\n",
    "    )\n",
    "    if model.upper() == \"GFS\":\n",
    "        ds_tointerp = ds_tointerp.sortby(\"longitude\")\n",
    "    df_tointerp = ds_tointerp.to_dataframe(dim_order=None).reset_index()\n",
    "\n",
    "    # will need to use this dataframe & merge with data (that needs to be interpolated)\n",
    "    # based on lat/lon values\n",
    "    df_model_merge = df_model.merge(\n",
    "        df_tointerp[[\"latitude\", \"longitude\", var]], on=[\"latitude\", \"longitude\"]\n",
    "    )\n",
    "    df_model_merge = df_model_merge[\n",
    "        df_model_merge[var] == 1\n",
    "    ]  # only return grid cells over land\n",
    "\n",
    "    return df_model_merge\n",
    "\n",
    "\n",
    "def main(month, year, model, init, mask_water=True):\n",
    "    start_time = time.time()\n",
    "    \"\"\"\n",
    "    This function loads in the parquet data cleaned from the grib files and interpolates (GFS, NAM) or finds the nearest\n",
    "    grid neighbor (HRRR) for each specified variable to each NYSM site location across NYS. It also calculates the\n",
    "    precipitation accumulation over 1-h (HRRR, NAM when forecast hour <= 36) and 3-h (GFS, NAM when forecast hour > 36)\n",
    "    increments. These data are saved as parquet files.\n",
    "\n",
    "    The following parameters need to be passed into main():\n",
    "\n",
    "    month (str) - integer corresponding to calendar month (e.g. '01' is January, '02' is Februrary, etc.)\n",
    "    year (str) - the year of interest (e.g., '2020')\n",
    "    model (str) - hrrr, nam, gfs\n",
    "    init(str) - initilization time for model, '00' or '12' UTC\n",
    "    mask_water (bool) - true to mask out grid cells over water before interpolation/nearest-neighbor,\n",
    "                        false to leave all grid cells available for interpolation/nearest-neighbor\n",
    "    \"\"\"\n",
    "    if model == \"HRRR\":\n",
    "        pres = \"mslma\"\n",
    "    else:\n",
    "        pres = \"prmsl\"\n",
    "\n",
    "    nysm_1H_obs, nysm_3H_obs = load_nysm_data(year)\n",
    "    oksm_1H_obs, oksm_3H_obs = load_oksm_data(year)\n",
    "    df_model_ny = read_data_ny(model, init, month, year)\n",
    "    df_model_ok = read_data_ok(model, init, month, year)\n",
    "\n",
    "    oksm_1H_obs = oksm_1H_obs.rename(columns={\"STID\": \"station\"})\n",
    "    oksm_3H_obs = oksm_3H_obs.rename(columns={\"STID\": \"station\"})\n",
    "\n",
    "    # drop some info that got carried over from xarray data array\n",
    "    keep_vars = [\n",
    "        \"valid_time\",\n",
    "        \"time\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"t2m\",\n",
    "        \"sh2\",\n",
    "        \"d2m\",\n",
    "        \"r2\",\n",
    "        \"u10\",\n",
    "        \"v10\",\n",
    "        \"tp\",\n",
    "        pres,\n",
    "        \"orog\",\n",
    "    ]\n",
    "\n",
    "    # if \"x\" in df_model_ny.keys():\n",
    "    #     df_model_ny = df_model_ny.drop(\n",
    "    #         columns=[\"x\", \"y\"]\n",
    "    #     )  # drop x & y if they're columns since reindex will fail with them in original index\n",
    "    if \"x\" in df_model_ok.keys():\n",
    "        df_model_ok = df_model_ok.drop(\n",
    "            columns=[\"x\", \"y\"]\n",
    "        )  # drop x & y if they're columns since reindex will fail with them in original index\n",
    "\n",
    "    df_model_ny = df_model_ny.reset_index()[keep_vars]\n",
    "    df_model_ny = reformat_df(df_model_ny)\n",
    "    df_model_ok = df_model_ok.reset_index()[keep_vars]\n",
    "    df_model_ok = reformat_df(df_model_ok)\n",
    "\n",
    "    if mask_water:\n",
    "        # before interpolation or nearest neighbor methods, mask out any grid cells over water\n",
    "        df_model_ny = mask_out_water(model, df_model_ny)\n",
    "        df_model_ok = mask_out_water(model, df_model_ok)\n",
    "\n",
    "    #use this for gfs and nam if you want to interpolate\n",
    "    #if model in [\"GFS\", \"NAM\"]:\n",
    "        #vars_to_interp = [\n",
    "        #     \"lead time\",\n",
    "        #     \"latitude\",\n",
    "        #     \"longitude\",\n",
    "        #     \"tp\",\n",
    "        #     \"t2m\",\n",
    "        #     \"u_total\",\n",
    "        #     \"u_dir\",\n",
    "        #     \"d2m\",\n",
    "        #     pres,\n",
    "        #     \"orog\",\n",
    "        # ]\n",
    "        # df_model_nysm_sites = interpolate_model_data_to_nysm_locations_groupby(\n",
    "        #    df_model_ny, nysm_1H_obs, vars_to_interp\n",
    "        # )\n",
    "        # df_model_oksm_sites = interpolate_model_data_to_oksm_locations_groupby(\n",
    "        #     df_model_ok, oksm_1H_obs, vars_to_interp\n",
    "        # )\n",
    "\n",
    "        indices_list_ny = get_ball_tree_indices_ny(df_model_ny, nysm_1H_obs)\n",
    "        indices_list_ok = get_ball_tree_indices_ok(df_model_ok, oksm_1H_obs)\n",
    "        df_model_nysm_sites = df_with_nysm_locations(\n",
    "        df_model_ny, nysm_1H_obs, indices_list_ny\n",
    "        )\n",
    "        df_model_oksm_sites = df_with_oksm_locations(\n",
    "            df_model_ok, oksm_1H_obs, indices_list_ok\n",
    "        )\n",
    "\n",
    "    # to avoid future issues, convert lead time to float, round, and then convert to integer\n",
    "    # without rounding first, the conversion to int will round to the floor, leading to incorrect lead times\n",
    "    # df_model_nysm_sites[\"lead time\"] = (\n",
    "    #     df_model_nysm_sites[\"lead time\"].astype(float).round(0).astype(int)\n",
    "    # )\n",
    "    df_model_oksm_sites[\"lead time\"] = (\n",
    "        df_model_oksm_sites[\"lead time\"].astype(float).round(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # now get precip forecasts in smallest intervals (e.g., 1-h and 3-h) possible\n",
    "\n",
    "    if model == \"NAM\":\n",
    "        model_data_1H_ny = df_model_nysm_sites[df_model_nysm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ny= df_model_nysm_sites[df_model_nysm_sites[\"lead time\"] > 36]\n",
    "        model_data_1H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] > 36]\n",
    "\n",
    "        # NY\n",
    "        df_model_sites_1H_ny = redefine_precip_intervals_NAM(model_data_1H_ny, 1)\n",
    "        df_model_sites_1H_ny = drop_unwanted_time_diffs(df_model_sites_1H_ny, 1.0)\n",
    "        df_model_sites_3H_ny = redefine_precip_intervals_NAM(model_data_3H_ny, 3)\n",
    "        df_model_sites_3H_ny = drop_unwanted_time_diffs(df_model_sites_3H_ny, 3.0)\n",
    "        df_model_sites_1H_ny = redefine_precip_intervals_NAM(model_data_1H_ny, 1)\n",
    "        df_model_sites_1H_ny = drop_unwanted_time_diffs(df_model_sites_1H_ny, 1.0)\n",
    "        df_model_sites_3H_ny = redefine_precip_intervals_NAM(model_data_3H_ny, 3)\n",
    "        df_model_sites_3H_ny = drop_unwanted_time_diffs(df_model_sites_3H_ny, 3.0)\n",
    "\n",
    "        # OK\n",
    "        model_data_1H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] > 36]\n",
    "        model_data_1H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] > 36]\n",
    "        df_model_sites_1H_ok = redefine_precip_intervals_NAM(model_data_1H_ok, 1)\n",
    "        df_model_sites_1H_ok = drop_unwanted_time_diffs(df_model_sites_1H_ok, 1.0)\n",
    "        df_model_sites_3H_ok = redefine_precip_intervals_NAM(model_data_3H_ok, 3)\n",
    "        df_model_sites_3H_ok = drop_unwanted_time_diffs(df_model_sites_3H_ok, 3.0)\n",
    "\n",
    "        df_model_nysm_sites = pd.concat([df_model_sites_1H_ny, df_model_sites_3H_ny])\n",
    "        df_model_oksm_sites = pd.concat([df_model_sites_1H_ok, df_model_sites_3H_ok])\n",
    "    elif model == \"GFS\":\n",
    "        df_model_nysm_sites = redefine_precip_intervals_GFS(df_model_nysm_sites)\n",
    "        df_model_nysm_sites = drop_unwanted_time_diffs(df_model_nysm_sites, 3.0)\n",
    "        df_model_oksm_sites = redefine_precip_intervals_GFS(df_model_oksm_sites)\n",
    "        df_model_oksm_sites = drop_unwanted_time_diffs(df_model_oksm_sites, 3.0)\n",
    "    elif model == \"HRRR\":\n",
    "        df_model_nysm_sites = redefine_precip_intervals_HRRR(df_model_nysm_sites)\n",
    "        df_model_nysm_sites = drop_unwanted_time_diffs(df_model_nysm_sites, 1.0)\n",
    "        df_model_oksm_sites = redefine_precip_intervals_HRRR(df_model_oksm_sites)\n",
    "        df_model_oksm_sites = drop_unwanted_time_diffs(df_model_oksm_sites, 1.0)\n",
    "        print(df_model_oksm_sites)\n",
    "\n",
    "    savedir = f\"/home/aevans/ai2es/processed_data/{model}/\"\n",
    "    # savedir = f'/home/lgaudet/model-data/GFS/GFSv16_parallel/interp/'\n",
    "    if mask_water:\n",
    "        # df_model_nysm_sites.to_parquet(\n",
    "        #     f\"{savedir}ny/{model}_{init}z_{month}-{year}_interp_to_nysm_sites_mask_water.parquet\"\n",
    "        # )\n",
    "        df_model_oksm_sites.to_parquet(\n",
    "            f\"{savedir}ok/{model}_{init}z_{month}-{year}_interp_to_oksm_sites_mask_water.parquet\"\n",
    "        )\n",
    "    else:\n",
    "        # df_model_nysm_sites.to_parquet(\n",
    "        #     f\"{savedir}ny/{model}_{init}z_{month}-{year}_interp_to_nysm_sites.parquet\"\n",
    "        # )\n",
    "        df_model_oksm_sites.to_parquet(\n",
    "            f\"{savedir}ok/{model}_{init}z_{month}-{year}_interp_to_oksm_sites.parquet\"\n",
    "        )\n",
    "\n",
    "    timer9 = time.time() - start_time\n",
    "\n",
    "    print(f\"Saving New Files For :: {model} t{init}z : {year}--{month}\")\n",
    "    print(\"--- %s seconds ---\" % (timer9))\n",
    "\n",
    "\n",
    "# ignore warnings\n",
    "# notebook throws warnings about Dtype\n",
    "# this line filters them out of output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Multiprocessing Notes\n",
    "\"\"\"\n",
    "Multiprocessing Pool\n",
    "RUN with 75 gb \n",
    "4 tasks per cpu \n",
    "\"\"\"\n",
    "\n",
    "init = \"12\"\n",
    "\n",
    "\n",
    "main(str(1).zfill(2), 2018, \"GFS\", init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ef0e22c-71b5-43c9-a0c8-1fe3d4da3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nysm_data(year):\n",
    "    # these parquet files are created by running \"get_resampled_nysm_data.ipynb\"\n",
    "    nysm_path = \"/home/aevans/nwp_bias/data/nysm/\"\n",
    "\n",
    "    nysm_1H_obs = pd.read_parquet(f\"{nysm_path}nysm_1H_obs_{year}.parquet\")\n",
    "    nysm_3H_obs = pd.read_parquet(f\"{nysm_path}nysm_3H_obs_{year}.parquet\")\n",
    "    return nysm_1H_obs, nysm_3H_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed67c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oksm_data(year):\n",
    "    # these parquet files are created by running \"get_resampled_nysm_data.ipynb\"\n",
    "    oksm_path = \"/home/aevans/nwp_bias/data/oksm/\"\n",
    "\n",
    "    oksm_1H_obs = pd.read_csv(f\"{oksm_path}oksm_1H_obs_{year}.csv\")\n",
    "    oksm_3H_obs = pd.read_csv(f\"{oksm_path}oksm_3H_obs_{year}.csv\")\n",
    "    return oksm_1H_obs, oksm_3H_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed2271da-8499-4a50-8d15-c5a56ba7a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_ny(model, init, month, year):\n",
    "    cleaned_data_path = f\"/home/aevans/ai2es/cleaned/{model}/\"\n",
    "\n",
    "    filelist = glob.glob(f\"{cleaned_data_path}{year}/{month}/*t{init}z*.parquet\")\n",
    "    filelist.sort()\n",
    "\n",
    "    li = []\n",
    "    for filename in filelist:\n",
    "        df_temp = pd.read_parquet(filename)\n",
    "        li.append(\n",
    "            df_temp.reset_index()\n",
    "        )  # reset the index in case indices are different among files\n",
    "    df = pd.concat(li)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data_ok(model, init, month, year):\n",
    "    cleaned_data_path = f\"/home/aevans/ai2es/{model}/cleaned{model}/\"\n",
    "\n",
    "    filelist = glob.glob(f\"{cleaned_data_path}{year}/{month}/*t{init}z*.parquet\")\n",
    "    filelist.sort()\n",
    "\n",
    "    li = []\n",
    "    for filename in filelist:\n",
    "        df_temp = pd.read_parquet(filename)\n",
    "        li.append(\n",
    "            df_temp.reset_index()\n",
    "        )  # reset the index in case indices are different among files\n",
    "    df = pd.concat(li)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reformat_df(df):\n",
    "    df = df.reset_index()\n",
    "    df[\"longitude\"] = ((df[\"longitude\"] + 180) % 360) - 180\n",
    "    df[[\"t2m\", \"d2m\"]] = df[[\"t2m\", \"d2m\"]] - 273.15  # convert from K to deg C\n",
    "    u10 = units.Quantity(df[\"u10\"].values, \"m/s\")\n",
    "    v10 = units.Quantity(df[\"v10\"].values, \"m/s\")\n",
    "    df[\"u_total\"] = mpcalc.wind_speed(u10, v10).magnitude\n",
    "    df[\"u_dir\"] = mpcalc.wind_direction(u10, v10, convention=\"from\").magnitude\n",
    "\n",
    "    lead_time_delta = df[\"valid_time\"] - df[\"time\"]\n",
    "    df[\"lead time\"] = (24.0 * lead_time_delta.dt.days) + divmod(\n",
    "        lead_time_delta.dt.seconds, 3600\n",
    "    )[0]\n",
    "\n",
    "    df = df.set_index(\"valid_time\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c22d339-d6be-4a97-a646-6d6b8ab5d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_func_griddata(values, model_lon, model_lat, xnew, ynew):\n",
    "    if np.mean(values) == np.nan:\n",
    "        print(\"SOME VALS ARE NAN\")\n",
    "    vals = interpolate.griddata(\n",
    "        (model_lon, model_lat), values, (xnew, ynew), method=\"linear\"\n",
    "    )\n",
    "    if np.mean(values) == np.nan:\n",
    "        print(\"SOME VALS ARE NAN\")\n",
    "    return vals\n",
    "\n",
    "\n",
    "def interpolate_model_data_to_nysm_locations_groupby(df_model, df_nysm, vars_to_interp):\n",
    "    \"\"\"\n",
    "    Use this function if you would like to interpolate to NYSM locations rather than use the ball tree with\n",
    "    the existing model grid.\n",
    "    This function interpolates the model grid to the NYSM site locations for each variable in dataframe.\n",
    "    The new dataframe is then returned.\n",
    "\n",
    "    This function should not be used with the HRRR grid, as it is incredibly slow.\n",
    "    \"\"\"\n",
    "    # New York\n",
    "    df_nysm = df_nysm.groupby(\"station\").mean()[[\"lat\", \"lon\"]]\n",
    "    xnew = df_nysm[\"lon\"]\n",
    "    ynew = df_nysm[\"lat\"]\n",
    "\n",
    "    df_model = df_model.reset_index().set_index(\"time\")\n",
    "\n",
    "    # if vals != points in interpolation routine\n",
    "    # called a few lines below, it's because this line is finding multiple of the same valid_times which occurs at times later in the month\n",
    "    # grab a smaller subset of the data encompassing New York State and Oklahoma\n",
    "    df_model_ny = df_model[\n",
    "        (df_model.latitude >= 39.0)\n",
    "        & (df_model.latitude <= 47.0)\n",
    "        & (df_model.longitude <= -71.0)\n",
    "        & (df_model.longitude >= -80.0)\n",
    "    ]\n",
    "\n",
    "    model_lon_lat_ny = df_model_ny[\n",
    "        df_model_ny[\"valid_time\"] == df_model_ny[\"valid_time\"].unique().min()\n",
    "    ]\n",
    "\n",
    "    model_lon_ny = model_lon_lat_ny[\"longitude\"].values\n",
    "    model_lat_ny = model_lon_lat_ny[\"latitude\"].values\n",
    "\n",
    "    df_model = df_model_ny[df_model_ny[\"longitude\"].isin(model_lon_ny)]\n",
    "    df_model = df_model.reset_index()\n",
    "\n",
    "    # NEED TO FIX THIS\n",
    "    df = pd.DataFrame()\n",
    "    for v, var in enumerate(vars_to_interp):\n",
    "        print(var)\n",
    "        df[var] = df_model.groupby([\"time\", \"valid_time\"])[var].apply(\n",
    "            interpolate_func_griddata, model_lon_ny, model_lat_ny, xnew, ynew\n",
    "        )\n",
    "\n",
    "    df_explode = df.apply(pd.Series.explode)\n",
    "\n",
    "    # add in the lat & lon & station\n",
    "    if \"latitude\" in df_explode.keys():\n",
    "        print(\"adding NYSM site column\")\n",
    "        nysm_sites = df_nysm.reset_index().station.unique()\n",
    "        model_interp_lats = df_explode.latitude.unique()\n",
    "        map_dict = {model_interp_lats[i]: nysm_sites[i] for i in range(len(nysm_sites))}\n",
    "        df_explode[\"station\"] = df_explode[\"latitude\"].map(map_dict)\n",
    "    return df_explode\n",
    "\n",
    "\n",
    "def interpolate_model_data_to_oksm_locations_groupby(df_model, df_oksm, vars_to_interp):\n",
    "    \"\"\"\n",
    "    Use this function if you would like to interpolate to NYSM locations rather than use the ball tree with\n",
    "    the existing model grid.\n",
    "    This function interpolates the model grid to the NYSM site locations for each variable in dataframe.\n",
    "    The new dataframe is then returned.\n",
    "\n",
    "    This function should not be used with the HRRR grid, as it is incredibly slow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Oklahoma\n",
    "    df_oksm = df_oksm.rename(columns={\"STID\": \"station\"})\n",
    "    df_oksm = df_oksm.groupby(\"station\").mean()[[\"lat\", \"lon\"]]\n",
    "    xnew_ok = df_oksm[\"lon\"]\n",
    "    ynew_ok = df_oksm[\"lat\"]\n",
    "\n",
    "    df_model = df_model.reset_index().set_index(\"time\")\n",
    "\n",
    "    # if vals != points in interpolation routine\n",
    "    # called a few lines below, it's because this line is finding multiple of the same valid_times which occurs at times later in the month\n",
    "    # grab a smaller subset of the data encompassing New York State and Oklahoma\n",
    "\n",
    "    df_model_ok = df_model[\n",
    "        (df_model.latitude >= 33.0)\n",
    "        & (df_model.latitude <= 38.0)\n",
    "        & (df_model.longitude <= -94.0)\n",
    "        & (df_model.longitude >= -104.0)\n",
    "    ]\n",
    "\n",
    "    model_lon_lat_ok = df_model_ok[\n",
    "        df_model_ok[\"valid_time\"] == df_model_ok[\"valid_time\"].unique().min()\n",
    "    ]\n",
    "\n",
    "    model_lon_ok = model_lon_lat_ok[\"longitude\"].values\n",
    "    model_lat_ok = model_lon_lat_ok[\"latitude\"].values\n",
    "\n",
    "    df_model = df_model_ok[df_model_ok[\"longitude\"].isin(model_lon_ok)]\n",
    "    df_model = df_model.reset_index()\n",
    "\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for v, var in enumerate(vars_to_interp):\n",
    "        print(var)\n",
    "        df[var] = df_model.groupby([\"time\", \"valid_time\"])[var].apply(\n",
    "            interpolate_func_griddata, model_lon_ok, model_lat_ok, xnew_ok, ynew_ok\n",
    "        )\n",
    "\n",
    "    df_explode = df.apply(pd.Series.explode)\n",
    "\n",
    "    # add in the lat & lon & station\n",
    "    if \"latitude\" in df_explode.keys():\n",
    "        print(\"adding OKSM site column\")\n",
    "        oksm_sites = df_oksm.reset_index().station.unique()\n",
    "        model_interp_lats = df_explode.latitude.unique()\n",
    "        map_dict = {model_interp_lats[i]: oksm_sites[i] for i in range(len(oksm_sites))}\n",
    "        df_explode[\"station\"] = df_explode[\"latitude\"].map(map_dict)\n",
    "    return df_explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664b4e23-717c-4895-9bdc-ac50dd1aee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_locations_for_ball_tree(df, nysm_1H_obs):\n",
    "    locations_a = df.reset_index()[[\"latitude\", \"longitude\"]]\n",
    "    locations_b = nysm_1H_obs[[\"lat\", \"lon\"]].dropna().drop_duplicates().reset_index()\n",
    "\n",
    "    # ball tree to find nysm site locations\n",
    "    # locations_a ==> build the tree\n",
    "    # locations_b ==> query the tree\n",
    "    # Creates new columns converting coordinate degrees to radians.\n",
    "    for column in locations_a[[\"latitude\", \"longitude\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    for column in locations_b[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_b[column].values)\n",
    "        locations_b[f\"{column}_rad\"] = rad\n",
    "\n",
    "    return locations_a, locations_b\n",
    "\n",
    "\n",
    "def get_locations_for_ball_tree_ok(df, oksm_1H_obs):\n",
    "    locations_a = df.reset_index()[[\"latitude\", \"longitude\"]]\n",
    "    locations_b = oksm_1H_obs[[\"lat\", \"lon\"]].dropna().drop_duplicates().reset_index()\n",
    "\n",
    "    # ball tree to find nysm site locations\n",
    "    # locations_a ==> build the tree\n",
    "    # locations_b ==> query the tree\n",
    "    # Creates new columns converting coordinate degrees to radians.\n",
    "    for column in locations_a[[\"latitude\", \"longitude\"]]:\n",
    "        rad = np.deg2rad(locations_a[column].values)\n",
    "        locations_a[f\"{column}_rad\"] = rad\n",
    "\n",
    "    for column in locations_b[[\"lat\", \"lon\"]]:\n",
    "        rad = np.deg2rad(locations_b[column].values)\n",
    "        locations_b[f\"{column}_rad\"] = rad\n",
    "\n",
    "    print(\"locations_a\", locations_a)\n",
    "    print(\"locations_b\", locations_b)\n",
    "    return locations_a, locations_b\n",
    "\n",
    "\n",
    "def get_ball_tree_indices_ny(model_data, nysm_1H_obs):\n",
    "    all_1H_obs = nysm_1H_obs\n",
    "    locations_a, locations_b = get_locations_for_ball_tree(model_data, all_1H_obs)\n",
    "    # Takes the first group's latitude and longitude values to construct the ball tree.\n",
    "\n",
    "    ball = BallTree(\n",
    "        locations_a[[\"latitude_rad\", \"longitude_rad\"]].values, metric=\"haversine\"\n",
    "    )\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = 1\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "    # get indices in a format where we can query the df\n",
    "    indices_list = [indices[x][0] for x in range(len(indices))]\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "def get_ball_tree_indices_ok(model_data, oksm_1H_obs):\n",
    "    all_1H_obs = oksm_1H_obs\n",
    "    locations_a, locations_b = get_locations_for_ball_tree(model_data, all_1H_obs)\n",
    "    # Takes the first group's latitude and longitude values to construct the ball tree.\n",
    "\n",
    "    ball = BallTree(\n",
    "        locations_a[[\"latitude_rad\", \"longitude_rad\"]].values, metric=\"haversine\"\n",
    "    )\n",
    "    # k: The number of neighbors to return from tree\n",
    "    k = 1\n",
    "    # Executes a query with the second group. This will also return two arrays.\n",
    "    distances, indices = ball.query(locations_b[[\"lat_rad\", \"lon_rad\"]].values, k=k)\n",
    "    # get indices in a format where we can query the df\n",
    "    indices_list = [indices[x][0] for x in range(len(indices))]\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "def df_with_nysm_locations(df, df_nysm, indices_list):\n",
    "    df_closest_locs = df.iloc[indices_list][[\"latitude\", \"longitude\"]].reset_index()\n",
    "    df_nysm_station_locs = df_nysm.groupby(\"station\")[[\"lat\", \"lon\"]].mean()\n",
    "\n",
    "    for x in range(len(df_nysm_station_locs.index)):\n",
    "        df_dummy = df[\n",
    "            (df.latitude == df_closest_locs.latitude[x])\n",
    "            & (df.longitude == df_closest_locs.longitude[x])\n",
    "        ]\n",
    "        df_dummy = df_dummy.reset_index()\n",
    "        df_dummy[\"station\"] = df_nysm_station_locs.index[x]\n",
    "        if x == 0:\n",
    "            df_save = df_dummy\n",
    "        else:\n",
    "            df_save = pd.concat([df_save, df_dummy])\n",
    "    print(\"complete\")\n",
    "    return df_save.set_index([\"station\", \"valid_time\"])\n",
    "\n",
    "\n",
    "def df_with_oksm_locations(df, df_oksm, indices_list):\n",
    "    df_closest_locs = df.iloc[indices_list][[\"latitude\", \"longitude\"]].reset_index()\n",
    "    df_oksm_station_locs = df_oksm.groupby(\"station\")[[\"lat\", \"lon\"]].mean()\n",
    "\n",
    "    for x in range(len(df_oksm_station_locs.index)):\n",
    "        df_dummy = df[\n",
    "            (df.latitude == df_closest_locs.latitude[x])\n",
    "            & (df.longitude == df_closest_locs.longitude[x])\n",
    "        ]\n",
    "        df_dummy = df_dummy.reset_index()\n",
    "        df_dummy[\"station\"] = df_oksm_station_locs.index[x]\n",
    "        if x == 0:\n",
    "            df_save = df_dummy\n",
    "        else:\n",
    "            df_save = pd.concat([df_save, df_dummy])\n",
    "    print(\"complete\")\n",
    "    return df_save.set_index([\"station\", \"valid_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c54172-1807-40b9-be35-50ab9b5329b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_precip_intervals_NAM(data, dt):\n",
    "    # dt is 1 for 1H and 3 for 3H\n",
    "    tp_data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])[[\"tp\"]]\n",
    "    # get valid times 00, 06, 12, & 18\n",
    "    tp_data[\"new tp keep\"] = tp_data[\n",
    "        (tp_data.index.get_level_values(level=0).hour == 12 + dt)\n",
    "        | (tp_data.index.get_level_values(level=0).hour == 0 + dt)\n",
    "    ]\n",
    "    tp_data[\"tp to diff\"] = tp_data[\n",
    "        (tp_data.index.get_level_values(level=0).hour != 12 + dt)\n",
    "        | (tp_data.index.get_level_values(level=0).hour != 0 + dt)\n",
    "    ][\"tp\"]\n",
    "    dummy = (\n",
    "        tp_data.reset_index()\n",
    "        .set_index([\"station\", \"time\", \"valid_time\"])\n",
    "        .sort_index(level=1)\n",
    "        .shift(periods=1)\n",
    "    )\n",
    "    tp_data[\"tp shifted\"] = dummy.reset_index().set_index(\n",
    "        [\"valid_time\", \"time\", \"station\"]\n",
    "    )[\"tp\"]\n",
    "    tp_data[\"tp diff\"] = tp_data[\"tp to diff\"] - tp_data[\"tp shifted\"]\n",
    "    tp_data[\"new_tp\"] = tp_data[\"new tp keep\"].combine_first(tp_data[\"tp diff\"])\n",
    "    tp_data = tp_data.drop(\n",
    "        columns=[\"new tp keep\", \"tp to diff\", \"tp shifted\", \"tp diff\"]\n",
    "    )\n",
    "\n",
    "    # merge in with original dataframe\n",
    "    data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])\n",
    "    data[\"new_tp\"] = tp_data[\"new_tp\"].clip(lower=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def redefine_precip_intervals_GFS(data):\n",
    "    tp_data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])[\n",
    "        [\"tp\", \"lead time\"]\n",
    "    ]\n",
    "    # get valid times 00, 06, 12, & 18\n",
    "    tp_data[\"new tp 1\"] = tp_data.loc[\n",
    "        (tp_data.index.get_level_values(level=0).hour % 6 == 0)\n",
    "    ][\"tp\"]\n",
    "    dummy = (\n",
    "        tp_data.reset_index()\n",
    "        .set_index([\"station\", \"time\", \"valid_time\"])\n",
    "        .sort_index(level=1)\n",
    "        .shift(periods=1)\n",
    "    )\n",
    "    tp_data[\"tp shifted\"] = dummy.reset_index().set_index(\n",
    "        [\"valid_time\", \"time\", \"station\"]\n",
    "    )[\"tp\"]\n",
    "    tp_data[\"tp diff\"] = tp_data[\"new tp 1\"] - tp_data[\"tp shifted\"]\n",
    "    tp_data[\"new tp 2\"] = tp_data.loc[\n",
    "        (tp_data.index.get_level_values(level=0).hour % 6 != 0)\n",
    "    ][\"tp\"]\n",
    "    tp_data[\"new_tp\"] = tp_data[\"new tp 2\"].combine_first(tp_data[\"tp diff\"])\n",
    "    tp_data = tp_data.drop(columns=[\"tp shifted\", \"tp diff\", \"new tp 1\", \"new tp 2\"])\n",
    "\n",
    "    # merge in with original dataframe\n",
    "    data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])\n",
    "    data[\"new_tp\"] = tp_data[\"new_tp\"].clip(lower=0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def redefine_precip_intervals_HRRR(data):\n",
    "    # dt is 1 for 1H and 3 for 3H\n",
    "    tp_data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])[\n",
    "        [\"tp\", \"lead time\"]\n",
    "    ]\n",
    "    # should use lead time here instead of valid_time (to be more generalizable)\n",
    "    tp_data[\"new tp keep\"] = tp_data[tp_data[\"lead time\"] == 1][\"tp\"]\n",
    "    tp_data[\"tp to diff\"] = tp_data[(tp_data[\"lead time\"] != 1)][\"tp\"]\n",
    "    dummy = (\n",
    "        tp_data.reset_index()\n",
    "        .set_index([\"station\", \"time\", \"valid_time\"])\n",
    "        .sort_index(level=1)\n",
    "        .shift(periods=1)\n",
    "    )\n",
    "    tp_data[\"tp shifted\"] = dummy.reset_index().set_index(\n",
    "        [\"valid_time\", \"time\", \"station\"]\n",
    "    )[\"tp\"]\n",
    "    tp_data[\"tp diff\"] = tp_data[\"tp\"].diff()\n",
    "    tp_data[\"new_tp\"] = tp_data[\"new tp keep\"].combine_first(tp_data[\"tp diff\"])\n",
    "    tp_data = tp_data.drop(\n",
    "        columns=[\"new tp keep\", \"tp to diff\", \"tp shifted\", \"tp diff\", \"lead time\"]\n",
    "    )\n",
    "\n",
    "    # merge in with original dataframe\n",
    "    data = data.reset_index().set_index([\"valid_time\", \"time\", \"station\"])\n",
    "    # replacing negative values with 0...the negative values are occurring during the forecast period which is unexpected behavior\n",
    "    # as the precipitation forecast should accumulate throughout forecast period\n",
    "    data[\"new_tp\"] = tp_data[\"new_tp\"].clip(lower=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5990d72-b6de-496f-b2c5-a1d92b817095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unwanted_time_diffs(df_model_both_sites, t_int):\n",
    "    # get rid of uneven time intervals that mess up precipitation forecasts\n",
    "\n",
    "    # t_int == 3 for GFS and NAM > f36\n",
    "    # t_int == 1 for NAM <= f36 & HRRR\n",
    "    df_model_both_sites[\"lead time diff\"] = df_model_both_sites.groupby(\n",
    "        [\"station\", \"time\"]\n",
    "    )[\"lead time\"].diff()\n",
    "    # following line fixes the issue where the lead time difference is nan for f01 and f39 because of the diff - we don't want to drop these later in the func\n",
    "    df_model_both_sites = df_model_both_sites.fillna(value={\"lead time diff\": t_int})\n",
    "\n",
    "    df_model_both_sites = df_model_both_sites.drop(\n",
    "        df_model_both_sites[\n",
    "            (df_model_both_sites[\"lead time diff\"] > t_int)\n",
    "            | (df_model_both_sites[\"lead time diff\"].isnull())\n",
    "        ].index\n",
    "    )\n",
    "    df_model_both_sites = df_model_both_sites.drop(columns=[\"lead time diff\"])\n",
    "    return df_model_both_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c22ebc26-8fab-4518-b5fd-1961d4ad3456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_out_water(model, df_model):\n",
    "    df_model = df_model.reset_index()\n",
    "    # read in respective data\n",
    "    # these files are hard coded since we only need land surface information that was not extracted from original files\n",
    "    # within the data cleaning script\n",
    "    indir = f\"/home/aevans/ai2es/{model.upper()}/2018/01/\"\n",
    "    if model.upper() == \"GFS\":\n",
    "        filename = \"gfs_4_20180101_0000_003.grb2\"\n",
    "        ind = 42\n",
    "        var = \"landn\"\n",
    "    elif model.upper() == \"NAM\":\n",
    "        filename = \"nam_218_20180101_0000_003.grb2\"\n",
    "        ind = 26\n",
    "        var = \"lsm\"\n",
    "    elif model.upper() == \"HRRR\":\n",
    "        filename = \"20180101_hrrr.t00z.wrfsfcf03.grib2\"\n",
    "        ind = 34\n",
    "        var = \"lsm\"\n",
    "    ds = cfgrib.open_datasets(f\"{indir}{filename}\")\n",
    "\n",
    "    ds_tointerp = ds[ind]  # extract the data array that contains land surface class\n",
    "    ds_tointerp = ds_tointerp.assign_coords(\n",
    "        {\"longitude\": (((ds_tointerp.longitude + 180) % 360) - 180)}\n",
    "    )\n",
    "    if model.upper() == \"GFS\":\n",
    "        ds_tointerp = ds_tointerp.sortby(\"longitude\")\n",
    "    df_tointerp = ds_tointerp.to_dataframe(dim_order=None).reset_index()\n",
    "\n",
    "    # will need to use this dataframe & merge with data (that needs to be interpolated)\n",
    "    # based on lat/lon values\n",
    "    df_model_merge = df_model.merge(\n",
    "        df_tointerp[[\"latitude\", \"longitude\", var]], on=[\"latitude\", \"longitude\"]\n",
    "    )\n",
    "    df_model_merge = df_model_merge[\n",
    "        df_model_merge[var] == 1\n",
    "    ]  # only return grid cells over land\n",
    "\n",
    "    return df_model_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7833154e-0400-45c3-911e-a695441d9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(month, year, model, init, mask_water=True):\n",
    "    \"\"\"\n",
    "    This function loads in the parquet data cleaned from the grib files and interpolates (GFS, NAM) or finds the nearest\n",
    "    grid neighbor (HRRR) for each specified variable to each NYSM site location across NYS. It also calculates the\n",
    "    precipitation accumulation over 1-h (HRRR, NAM when forecast hour <= 36) and 3-h (GFS, NAM when forecast hour > 36)\n",
    "    increments. These data are saved as parquet files.\n",
    "\n",
    "    The following parameters need to be passed into main():\n",
    "\n",
    "    month (str) - integer corresponding to calendar month (e.g. '01' is January, '02' is Februrary, etc.)\n",
    "    year (str) - the year of interest (e.g., '2020')\n",
    "    model (str) - hrrr, nam, gfs\n",
    "    init(str) - initilization time for model, '00' or '12' UTC\n",
    "    mask_water (bool) - true to mask out grid cells over water before interpolation/nearest-neighbor,\n",
    "                        false to leave all grid cells available for interpolation/nearest-neighbor\n",
    "    \"\"\"\n",
    "    if model == \"HRRR\":\n",
    "        pres = \"mslma\"\n",
    "    else:\n",
    "        pres = \"prmsl\"\n",
    "\n",
    "    nysm_1H_obs, nysm_3H_obs = load_nysm_data(year)\n",
    "    oksm_1H_obs, oksm_3H_obs = load_oksm_data(year)\n",
    "    df_model_ny = read_data_ny(model, init, month, year)\n",
    "    df_model_ok = read_data_ok(model, init, month, year)\n",
    "\n",
    "    # drop some info that got carried over from xarray data array\n",
    "    keep_vars = [\n",
    "        \"valid_time\",\n",
    "        \"time\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"t2m\",\n",
    "        \"sh2\",\n",
    "        \"d2m\",\n",
    "        \"r2\",\n",
    "        \"u10\",\n",
    "        \"v10\",\n",
    "        \"tp\",\n",
    "        pres,\n",
    "        \"orog\",\n",
    "    ]\n",
    "\n",
    "    if \"x\" in df_model_ny.keys():\n",
    "        df_model_ny = df_model_ny.drop(\n",
    "            columns=[\"x\", \"y\"]\n",
    "        )  # drop x & y if they're columns since reindex will fail with them in original index\n",
    "    if \"x\" in df_model_ok.keys():\n",
    "        df_model_ok = df_model_ok.drop(\n",
    "            columns=[\"x\", \"y\"]\n",
    "        )  # drop x & y if they're columns since reindex will fail with them in original index\n",
    "\n",
    "    df_model_ny = df_model_ny.reset_index()[keep_vars]\n",
    "    df_model_ny = reformat_df(df_model_ny)\n",
    "    df_model_ok = df_model_ok.reset_index()[keep_vars]\n",
    "    df_model_ok = reformat_df(df_model_ok)\n",
    "\n",
    "    if mask_water:\n",
    "        # before interpolation or nearest neighbor methods, mask out any grid cells over water\n",
    "        df_model_ny = mask_out_water(model, df_model_ny)\n",
    "        df_model_ok = mask_out_water(model, df_model_ok)\n",
    "\n",
    "    if model in [\"GFS\", \"NAM\"]:\n",
    "        vars_to_interp = [\n",
    "            \"lead time\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"tp\",\n",
    "            \"t2m\",\n",
    "            \"u_total\",\n",
    "            \"u_dir\",\n",
    "            \"d2m\",\n",
    "            pres,\n",
    "            \"orog\",\n",
    "        ]\n",
    "        df_model_nysm_sites = interpolate_model_data_to_nysm_locations_groupby(\n",
    "            df_model_ny, nysm_1H_obs, vars_to_interp\n",
    "        )\n",
    "        df_model_oksm_sites = interpolate_model_data_to_oksm_locations_groupby(\n",
    "            df_model_ok, oksm_1H_obs, vars_to_interp\n",
    "        )\n",
    "    elif model == \"HRRR\":\n",
    "        indices_list_ny = get_ball_tree_indices_ny(df_model_ny, nysm_1H_obs)\n",
    "        indices_list_ok = get_ball_tree_indices_ok(df_model_ok, oksm_1H_obs)\n",
    "        df_model_nysm_sites = df_with_nysm_locations(\n",
    "            df_model_ny, nysm_1H_obs, indices_list_ny\n",
    "        )\n",
    "        df_model_oksm_sites = df_with_oksm_locations(\n",
    "            df_model_ok, oksm_1H_obs, indices_list_ok\n",
    "        )\n",
    "\n",
    "    # to avoid future issues, convert lead time to float, round, and then convert to integer\n",
    "    # without rounding first, the conversion to int will round to the floor, leading to incorrect lead times\n",
    "    df_model_nysm_sites[\"lead time\"] = (\n",
    "        df_model_nysm_sites[\"lead time\"].astype(float).round(0).astype(int)\n",
    "    )\n",
    "    df_model_oksm_sites[\"lead time\"] = (\n",
    "        df_model_oksm_sites[\"lead time\"].astype(float).round(0).astype(int)\n",
    "    )\n",
    "\n",
    "    # now get precip forecasts in smallest intervals (e.g., 1-h and 3-h) possible\n",
    "    if model == \"NAM\":\n",
    "        model_data_1H_ny = df_model_nysm_sites[df_model_nysm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ny = df_model_nysm_sites[df_model_nysm_sites[\"lead time\"] > 36]\n",
    "        model_data_1H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] > 36]\n",
    "\n",
    "        # NY\n",
    "        df_model_sites_1H_ny = redefine_precip_intervals_NAM(model_data_1H_ny, 1)\n",
    "        df_model_sites_1H_ny = drop_unwanted_time_diffs(df_model_sites_1H_ny, 1.0)\n",
    "        df_model_sites_3H_ny = redefine_precip_intervals_NAM(model_data_3H_ny, 3)\n",
    "        df_model_sites_3H_ny = drop_unwanted_time_diffs(df_model_sites_3H_ny, 3.0)\n",
    "        df_model_sites_1H_ny = redefine_precip_intervals_NAM(model_data_1H_ny, 1)\n",
    "        df_model_sites_1H_ny = drop_unwanted_time_diffs(df_model_sites_1H_ny, 1.0)\n",
    "        df_model_sites_3H_ny = redefine_precip_intervals_NAM(model_data_3H_ny, 3)\n",
    "        df_model_sites_3H_ny = drop_unwanted_time_diffs(df_model_sites_3H_ny, 3.0)\n",
    "\n",
    "        # OK\n",
    "        model_data_1H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] > 36]\n",
    "        model_data_1H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] <= 36]\n",
    "        model_data_3H_ok = df_model_oksm_sites[df_model_oksm_sites[\"lead time\"] > 36]\n",
    "        df_model_sites_1H_ok = redefine_precip_intervals_NAM(model_data_1H_ok, 1)\n",
    "        df_model_sites_1H_ok = drop_unwanted_time_diffs(df_model_sites_1H_ok, 1.0)\n",
    "        df_model_sites_3H_ok = redefine_precip_intervals_NAM(model_data_3H_ok, 3)\n",
    "        df_model_sites_3H_ok = drop_unwanted_time_diffs(df_model_sites_3H_ok, 3.0)\n",
    "\n",
    "        df_model_nysm_sites = pd.concat([df_model_sites_1H_ny, df_model_sites_3H_ny])\n",
    "        df_model_oksm_sites = pd.concat([df_model_sites_1H_ok, df_model_sites_3H_ok])\n",
    "    elif model == \"GFS\":\n",
    "        df_model_nysm_sites = redefine_precip_intervals_GFS(df_model_nysm_sites)\n",
    "        df_model_nysm_sites = drop_unwanted_time_diffs(df_model_nysm_sites, 3.0)\n",
    "        df_model_oksm_sites = redefine_precip_intervals_GFS(df_model_oksm_sites)\n",
    "        df_model_oksm_sites = drop_unwanted_time_diffs(df_model_oksm_sites, 3.0)\n",
    "    elif model == \"HRRR\":\n",
    "        df_model_nysm_sites = redefine_precip_intervals_HRRR(df_model_nysm_sites)\n",
    "        df_model_nysm_sites = drop_unwanted_time_diffs(df_model_nysm_sites, 1.0)\n",
    "        df_model_oksm_sites = redefine_precip_intervals_HRRR(df_model_oksm_sites)\n",
    "        df_model_oksm_sites = drop_unwanted_time_diffs(df_model_oksm_sites, 1.0)\n",
    "\n",
    "    savedir = f\"/home/aevans/ai2es/processed_data/{model}/\"\n",
    "    # savedir = f'/home/lgaudet/model-data/GFS/GFSv16_parallel/interp/'\n",
    "    if mask_water:\n",
    "        df_model_nysm_sites.to_parquet(\n",
    "            f\"{savedir}ny/{model}_{init}z_{month}-{year}_interp_to_nysm_sites_mask_water.parquet\"\n",
    "        )\n",
    "        df_model_oksm_sites.to_parquet(\n",
    "            f\"{savedir}ok/{model}_{init}z_{month}-{year}_interp_to_oksm_sites_mask_water.parquet\"\n",
    "        )\n",
    "    else:\n",
    "        df_model_nysm_sites.to_parquet(\n",
    "            f\"{savedir}ny/{model}_{init}z_{month}-{year}_interp_to_nysm_sites.parquet\"\n",
    "        )\n",
    "        df_model_oksm_sites.to_parquet(\n",
    "            f\"{savedir}ok/{model}_{init}z_{month}-{year}_interp_to_oksm_sites.parquet\"\n",
    "        )\n",
    "\n",
    "    print(\"Saving New Files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ef21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ccb2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "[main(str(month).zfill(2), 2018, \"HRRR\", \"00\") for month in range(1, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f0b015-d574-4d5b-bf9e-f733f354af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inits = [\"00\", \"12\"]\n",
    "# ranger = np.arange(2018, 2022)\n",
    "# models = [\"GFS\", \"HRRR\", \"NAM\"]\n",
    "\n",
    "\n",
    "# for year in ranger:\n",
    "#     for init in inits:\n",
    "#         for model in models:\n",
    "#             # pool cpus\n",
    "#             pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "#             # run processes\n",
    "#             pool.starmap(\n",
    "#                 main,\n",
    "#                 [(str(month).zfill(2), year, model, init) for month in range(1, 13)],\n",
    "#             )\n",
    "#             pool.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
